# ========
# TaskMetadata:
# Specifies the task we are going to perform on the graph.
taskMetadata:
  nodeAnchorBasedLinkPredictionTaskMetadata:
    # Specifying that we will perform node anchor based link prediction on edge of type: paper -> cites -> paper
    supervisionEdgeTypes:
    - dstNodeType: paper
      relation: cites
      srcNodeType: paper
graphMetadata:
  # We have 3 nodes types in the MAG240M Dataset: paper, author, and institution. We also have 3
  # edge types: paper -> cites -> paper, author -> writes -> paper, and institution -> affiliated -> author
  edgeTypes:
  - dstNodeType: paper
    relation: cites
    srcNodeType: paper
  - dstNodeType: paper
    relation: writes
    srcNodeType: author
  - dstNodeType: author
    relation: affiliated
    srcNodeType: institution
  nodeTypes:
  - paper
  - author
  - institution
# ========
# SharedConfig:
# Specifies some extra metadata about the graph structure management of orchestration.
sharedConfig:
  isGraphDirected: True
  shouldSkipAutomaticTempAssetCleanup: true # Should we skip cleaning up the temporary assets after the run is complete?
# ========
# DatasetConfig:
# Specifies information about the dataset, such as how to access it and how to process it
datasetConfig:
  dataPreprocessorConfig:
    dataPreprocessorConfigClsPath: examples.MAG240M.preprocessor_config_heterogeneous.Mag240DataPreprocessorConfig
    # our implementation takes no runtime arguments; if provided these are passed to the constructor off dataPreprocessorConfigClsPath
    # dataPreprocessorArgs:
# ========
# TrainerConfig:
# Specifies the training configuration. This includes the trainer command and the arguments to pass to it.
trainerConfig:
  trainerArgs:
    # Example argument to trainer
    log_every_n_batch: "1000"
    # The MAG240M dataset does not have specified labeled edges so we provide this field to indicate what
    # percentage of edges we should select as self-supervised labeled edges. Doing this randomly sets 5% as "labels".
    # Note that the current GiGL implementation does not remove these selected edges from the global set of edges, which may
    # have a slight negative impact on training specifically with self-supervised learning. This will improved on in the future.
    ssl_positive_label_percentage: "0.05"
    num_neighbors: "[10, 10]" # Performs a [10, 10] fanout for each edge type in the graph
    main_batch_size: "256" # Reduce batch size if Cuda OOM
    random_batch_size: "256"
    num_max_train_batches: "20000" # Increase this number to train for longer
    hid_dim: "256"
    out_dim: "256"
    val_every_n_batch: "1000" # Decrease this number to do more frequent validation
    learning_rate: "0.005"
  command: python -m gigl.examples.link_prediction.heterogeneous_training
# ========
# InferencerConfig:
# specifies the inference configuration. This includes the command and the arguments to pass to it
inferencerConfig:
  inferencerArgs:
    # Example argument to inferencer
    log_every_n_batch: "50"
    num_neighbors: "[10, 10]" # Performs a [10, 10] fanout for each edge type in the graph
    hid_dim: "256"
    out_dim: "256"
  inferenceBatchSize: 256 # Reduce batch size if Cuda OOM
  command: python -m gigl.examples.link_prediction.heterogeneous_inference
# ========
# FeatureFlags:
# any additional flags which we should specify for the training + inference job. We currently use this to
# specify whether GLT should be used as the backend for in-memory subgraph sampling
featureFlags:
  should_run_glt_backend: 'True'

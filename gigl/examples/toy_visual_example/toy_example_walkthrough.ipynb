{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Example - Tabularized GiGL\n",
    "\n",
    "Latest version of this notebook can be found on [GiGL/examples/toy_visual_example/toy_example_walkthrough.ipynb](https://github.com/Snapchat/GiGL/blob/main/gigl/examples/toy_visual_example/toy_example_walkthrough.ipynb)\n",
    "\n",
    "\n",
    "This notebook provides a walkthrough of preprocessing, subgraph sampling, and split generation components with a small toy graph for GiGL's Tabularized setting for training/inference. It will help you understand how each of these components prepare tabularized subgraphs.\n",
    "\n",
    "PRO TIP: you can ctrl/cmd + A, right click, then \"enable scrolling for outputs\"\n",
    "\n",
    "\n",
    "## Overview Of Components\n",
    "This notebook demonstrates the process of a simple, human-digestible graph being passed through all the pipeline components in GiGL in preparation for training to help understand how each of the components work.\n",
    "\n",
    "The pipeline consists of the following components:\n",
    "\n",
    "\n",
    "![](../../docs/assets/images/framework/framework_tabular.png)\n",
    "\n",
    "\n",
    "#### üì• What You Need to Provide to GiGL\n",
    "To run your pipeline with GiGL, the user is expected to supply the following:\n",
    "\n",
    "1. **üìä Raw Data**\n",
    "    - Input graph data (nodes, edges, features, labels, etc.)\n",
    "2. **üõ†Ô∏è Preprocessing Function**\n",
    "    - A function to specify how to transform raw data into the expected format\n",
    "3. **‚öôÔ∏è Configurations**\n",
    "    - Resource Config: Specify compute and memory needs for each pipeline component\n",
    "    - Task Config: Define modeling tasks like link prediction, node classification, etc., including sampling, GNN architecture, loss, metrics, etc.\n",
    "3. **üîÅ Training & Inference Loops**\n",
    "    - User defined code that define how models are trained, validated, and used for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Notebook\n",
    "\n",
    "Enabling some jupyter reload magic, reducing log clutter and changing working dir to repo root to make path resolution saner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Silence TF logspam\n",
    "\n",
    "from gigl.common.utils.jupyter_magics import change_working_dir_to_gigl_root\n",
    "change_working_dir_to_gigl_root()\n",
    "\n",
    "NOTEBOOK_DIR = pathlib.Path(\"./examples/toy_visual_example\").as_posix() # We should be in root dir because of cell # 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Graph\n",
    "\n",
    "We use the input graph defined in [examples/toy_visual_example/graph_config.yaml](./graph_config.yaml). \n",
    "\n",
    "You are welcome to change this file to a custom graph of your own choosing.\n",
    "\n",
    "Also, this is for demo purposes only; at industry-scale you probably start with a graph in some distributed datastorage layer like BigQuery\n",
    "\n",
    "### Visualizing the Input Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from gigl.common.utils.jupyter_magics import GraphVisualizer, GraphVisualizerLayoutMode\n",
    "from gigl.src.mocking.toy_asset_mocker import load_toy_graph\n",
    "\n",
    "original_graph_heterodata: HeteroData = load_toy_graph(\n",
    "    graph_config_path=\"gigl/examples/toy_visual_example/graph_config.yaml\"\n",
    ") # If you want to update the graph, you will need to re-mock - See README.md\n",
    "\n",
    "\n",
    "# Visualize the graph\n",
    "GraphVisualizer.visualize_graph(original_graph_heterodata, layout_mode=GraphVisualizerLayoutMode.HOMOGENEOUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Setting up Configs: The Hidden Boss Level of Machine Learning\n",
    "\n",
    "The first step is to set up your task and resource configurations - arguably the most overlooked but essential part of running large-scale ML workflows.\n",
    "\n",
    "**Task Config** <br>\n",
    "This defines the task specific settings that control how different components behave based on your ML objective.\n",
    "- üìò Refer to the [Task Config Guide](https://snapchat.github.io/GiGL/docs/user_guide/config_guides/task_config_guide.html) for more details.\n",
    "- üìù For this notebook, we‚Äôve pre-provided [template_task_config.yaml](./template_task_config.yaml)\n",
    "\n",
    "\n",
    "**Resource Config** <br>\n",
    "This outlines compute resource allocation and environment settings across GiGL components - in other words, the knobs you'll tweak for compute resources - defining how fast GiGL can burn through your cloud credits.\n",
    "- üìò Refer to the [Resrouce Config Guide](https://snapchat.github.io/GiGL/docs/user_guide/config_guides/resource_config_guide.html)\n",
    "- üìù For this notebook, a starter config is available: [resource_config.yaml](./resource_config.yaml).\n",
    "- ‚ö†Ô∏è You'll need to update the default values under `shared_resource_config.common_compute_config` to reflect your cloud environment. i.e. setting your project, service account, etc.\n",
    "    - Let's do this now!\n",
    "\n",
    "*[Helpful link](https://snapchat.github.io/GiGL/docs/user_guide/getting_started/quick_start.html#config-setup) to setup configs for your experiments - once you graduate from this notebook.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß∞ Resource Config Setup\n",
    "**If you're attending the GiGL tutorial at KDD'25**, <br>\n",
    "We have done some tutorial magic and already setup the GCP project for you w/ help off the Google Labs for Sales Team. Just run the cells below to bootstrap your resource config.\n",
    "\n",
    "**If you're using this notebook outside of the tutorial**, <br>\n",
    "Follow the [Quick Start Guide](https://snapchat.github.io/GiGL/docs/user_guide/getting_started/quick_start.html), and in the cell below switch the default values with how you setup your GCP Project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "from gigl.common import GcsUri\n",
    "\n",
    "# RESOURCE_CONFIG vars - overwrite if running outside of KDD'25 tutorial\n",
    "PROJECT = subprocess.check_output(['gcloud', 'config', 'get-value', 'project']).decode(\"utf-8\").strip()\n",
    "REGION = \"us-central1\"\n",
    "SA_EMAIL = f\"gigl-dev@{PROJECT}.iam.gserviceaccount.com\"\n",
    "DOCKER_REGISTRY_PATH = f\"us-central1-docker.pkg.dev/{PROJECT}/gigl-base-images\"\n",
    "TEMP_ASSETS_BQ_DATASET_NAME = \"gigl_temp_assets\"\n",
    "EMBEDDING_BQ_DATASET_NAME = \"gigl_embeddings\"\n",
    "TEMP_ASSETS_BUCKET = f\"gs://gigl_temp_assets_{PROJECT}\"\n",
    "PERM_ASSETS_BUCKET = f\"gs://gigl_perm_assets_{PROJECT}\"\n",
    "\n",
    "# Use the template resource config as a starting point\n",
    "TEMPLATE_RESOURCE_CONFIG_PATH = \"gigl/examples/toy_visual_example/resource_config.yaml\"\n",
    "# This is the output path where we will store your unique resource config\n",
    "RESOURCE_CONFIG_PATH = GcsUri(f\"gs://gigl_perm_assets_{PROJECT}/tabularized_resource_config.yaml\")\n",
    "\n",
    "\n",
    "# Setting up env variables for the convenience\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"SA_EMAIL\"] = SA_EMAIL\n",
    "os.environ[\"DOCKER_REGISTRY_PATH\"] = DOCKER_REGISTRY_PATH\n",
    "os.environ[\"TEMP_ASSETS_BQ_DATASET_NAME\"] = TEMP_ASSETS_BQ_DATASET_NAME\n",
    "os.environ[\"EMBEDDING_BQ_DATASET_NAME\"] = EMBEDDING_BQ_DATASET_NAME\n",
    "os.environ[\"TEMP_ASSETS_BUCKET\"] = TEMP_ASSETS_BUCKET\n",
    "os.environ[\"PERM_ASSETS_BUCKET\"] = PERM_ASSETS_BUCKET\n",
    "os.environ[\"TEMPLATE_RESOURCE_CONFIG_PATH\"] = TEMPLATE_RESOURCE_CONFIG_PATH\n",
    "os.environ[\"RESOURCE_CONFIG_PATH\"] = RESOURCE_CONFIG_PATH.uri\n",
    "print(f\"\"\"\n",
    "Project: {PROJECT}\n",
    "Region: {REGION}\n",
    "Service Account: {SA_EMAIL}\n",
    "Docker Registry Path: {DOCKER_REGISTRY_PATH}\n",
    "Temp Assets BQ Dataset Name: {TEMP_ASSETS_BQ_DATASET_NAME}\n",
    "Embedding BQ Dataset Name: {EMBEDDING_BQ_DATASET_NAME}\n",
    "Temp Assets Bucket: {TEMP_ASSETS_BUCKET}\n",
    "Perm Assets Bucket: {PERM_ASSETS_BUCKET}\n",
    "Template Resource Config Path: {TEMPLATE_RESOURCE_CONFIG_PATH}\n",
    "Resource Config Path: {RESOURCE_CONFIG_PATH.uri}\n",
    "\"\"\")\n",
    "# Run the bootstrap script to generate the resource config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python  <(curl -s https://raw.githubusercontent.com/Snapchat/GiGL/refs/heads/main/scripts/bootstrap_resource_config.py) \\\n",
    "  --project=\"$PROJECT\" \\\n",
    "  --region=\"$REGION\" \\\n",
    "  --gcp_service_account_email=\"$SA_EMAIL\" \\\n",
    "  --docker_artifact_registry_path=\"$DOCKER_REGISTRY_PATH\" \\\n",
    "  --temp_assets_bq_dataset_name=\"$TEMP_ASSETS_BQ_DATASET_NAME\" \\\n",
    "  --embedding_bq_dataset_name=\"$EMBEDDING_BQ_DATASET_NAME\" \\\n",
    "  --temp_assets_bucket=\"$TEMP_ASSETS_BUCKET\" \\\n",
    "  --perm_assets_bucket=\"$PERM_ASSETS_BUCKET\" \\\n",
    "  --template_resource_config_uri=\"$TEMPLATE_RESOURCE_CONFIG_PATH\" \\\n",
    "  --output_resource_config_path=\"$RESOURCE_CONFIG_PATH\" \\\n",
    "  --force_shell_config_update=True\n",
    "\n",
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the resource config we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cat $RESOURCE_CONFIG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß∞ Task Config Setup + other needed vars\n",
    "\n",
    "Take a quick look at the [template_task_config](./template_task_config.yaml) we will be using.\n",
    "\n",
    "We setup task config, and other vars below/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import getpass\n",
    "from gigl.common import LocalUri\n",
    "from gigl.common.constants import (\n",
    "    DEFAULT_GIGL_RELEASE_SRC_IMAGE_DATAFLOW_CPU,\n",
    "    DEFAULT_GIGL_RELEASE_SRC_IMAGE_CUDA,\n",
    "    DEFAULT_GIGL_RELEASE_SRC_IMAGE_CPU\n",
    ")\n",
    "from gigl.common import Uri, UriFactory\n",
    "\n",
    "JOB_NAME = f\"{getpass.getuser()}_gigl_toy_example_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "# Input template task config\n",
    "TEMPLATE_TASK_CONFIG_PATH = LocalUri(f\"{NOTEBOOK_DIR}/template_task_config.yaml\")\n",
    "# Where we will store the location to the frozen task config - more details later\n",
    "FROZEN_TASK_CONFIG_POINTER_FILE_PATH: Uri = UriFactory.create_uri(f\"/tmp/GiGL/{JOB_NAME}/frozen_task_config.yaml\")\n",
    "pathlib.Path(FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# We will be using latest released versions of our docker images\n",
    "# These contain all GiGL source code + necessary deps i.e. cuda drivers, dataflow dependencies, etc.\n",
    "# For convenience, we provide `gigl.orchestration.img_builder` if you want to build your own images\n",
    "# with your custom code.\n",
    "DATAFLOW_RUNTIME_IMG = DEFAULT_GIGL_RELEASE_SRC_IMAGE_DATAFLOW_CPU\n",
    "CUDA_SRC_IMG = DEFAULT_GIGL_RELEASE_SRC_IMAGE_CUDA\n",
    "CPU_SRC_IMG = DEFAULT_GIGL_RELEASE_SRC_IMAGE_CPU\n",
    "\n",
    "os.environ[\"JOB_NAME\"] = JOB_NAME\n",
    "os.environ[\"TEMPLATE_TASK_CONFIG_PATH\"] = TEMPLATE_TASK_CONFIG_PATH.uri\n",
    "os.environ[\"FROZEN_TASK_CONFIG_POINTER_FILE_PATH\"] = FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri\n",
    "os.environ[\"DATAFLOW_RUNTIME_IMG\"] = DATAFLOW_RUNTIME_IMG\n",
    "os.environ[\"CUDA_SRC_IMG\"] = CUDA_SRC_IMG\n",
    "os.environ[\"CPU_SRC_IMG\"] = CPU_SRC_IMG\n",
    "\n",
    "print(f\"\"\"\n",
    "Job Name: {JOB_NAME}\n",
    "Template Task Config Path: {TEMPLATE_TASK_CONFIG_PATH.uri}\n",
    "Frozen Task Config Pointer File Path: {FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri}\n",
    "Using the following Dataflow, CUDA, and CPU docker images:\n",
    "- {DATAFLOW_RUNTIME_IMG}\n",
    "- {CUDA_SRC_IMG}\n",
    "- {CPU_SRC_IMG}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Validating the Configs\n",
    "\n",
    "You can validate both your resource and task configs using the provided tools. While the checks aren‚Äôt exhaustive, they catch the most common issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gigl.src.validation_check.config_validator import kfp_validation_checks\n",
    "\n",
    "validator = kfp_validation_checks(\n",
    "    job_name=JOB_NAME,\n",
    "    task_config_uri=TEMPLATE_TASK_CONFIG_PATH,\n",
    "    resource_config_uri=RESOURCE_CONFIG_PATH,\n",
    "    start_at=\"config_populator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üß© Config Populator\n",
    "The Config Populator takes a template GbmlConfig and produces a frozen GbmlConfig, where all job-related metadata paths in sharedConfig are fully populated.\n",
    "\n",
    "These populated fields are primarily GCS paths used for communication between components‚Äîserving as intermediaries for reading and writing data.\n",
    "For example:\n",
    "- `sharedConfig.trainedModelMetadata` is set to a GCS URI that tells the Trainer where to write the trained model, and tells the Inferencer where to read it from.\n",
    "\n",
    "üìò For full details, see the [Config Populator Guide](../../docs/user_guide/overview/components/config_populator.md).\n",
    "______________________________________________________________________\n",
    "**After executing the command below:**\n",
    "- A frozen config will be created.\n",
    "- It will be uploaded to the `perm_assets_bucket` specified in your resource config.\n",
    "- The resulting GCS path to the frozen config will be saved to the file defined by the `FROZEN_TASK_CONFIG_POINTER_FILE_PATH` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m \\\n",
    "    gigl.src.config_populator.config_populator \\\n",
    "    --job_name=\"$JOB_NAME\" \\\n",
    "    --template_uri=\"$TEMPLATE_TASK_CONFIG_PATH\" \\\n",
    "    --resource_config_uri=\"$RESOURCE_CONFIG_PATH\" \\\n",
    "    --output_file_path_frozen_gbml_config_uri=\"$FROZEN_TASK_CONFIG_POINTER_FILE_PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Visualizing the diff between Template and the generated Frozen Config\n",
    "At this stage, we have a frozen task config, whose path is specified by the `FROZEN_TASK_CONFIG_PATH` environment variable. The following cell will visualize the difference between:\n",
    "- The original `template_task_config`, and\n",
    "- The `frozen_task_config generated` by the `config_populator`.\n",
    "\n",
    "üìù Note: The code in the various cells below, is solely for visualization/explanation purpses and not needed for actually working with GiGL\n",
    "______________________________________________________________________\n",
    "#### üîë Key Differences\n",
    "1. Addition of sharedConfig: <br>\n",
    "This section contains all intermediary and final output paths required by the GiGL components (e.g., model artifacts, logs, metadata).\n",
    "2. **Storage saving trick #1** - Node/Edge Type Representation Compression:\n",
    "   - Introduces `condensedEdgeTypeMap`: Maps each edge type to a unique int, where:\n",
    "       - `EdgeType: Tuple[srcNodeType: str, relation: str, dstNodeType: str]`\n",
    "       - `EdgeType` ‚Üí mapped to ‚Üí `int`\n",
    "   - Introduces `condensedNodeTypeMap`: Maps each node type to a unique int:\n",
    "      - `NodeType: str`\n",
    "      - `NodeType` ‚Üí mapped to ‚Üí `int`\n",
    "   - These mappings are used to reduce storage overhead during graph processing and I/O.\n",
    "   - ‚ÑπÔ∏è You can provide your own condensed*Map fields, if not provided, they will be generated automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gigl.common.utils.jupyter_magics import show_task_config_colored_unified_diff\n",
    "\n",
    "FROZEN_TASK_CONFIG_PATH: Uri\n",
    "with open(FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri, 'r') as file:\n",
    "    FROZEN_TASK_CONFIG_PATH = UriFactory.create_uri(file.read().strip())\n",
    "print(f\"FROZEN_TASK_CONFIG_PATH: {FROZEN_TASK_CONFIG_PATH}\")\n",
    "\n",
    "show_task_config_colored_unified_diff(\n",
    "    f1_uri=FROZEN_TASK_CONFIG_PATH,\n",
    "    f2_uri=TEMPLATE_TASK_CONFIG_PATH,\n",
    "    f1_name='frozen_task_config.yaml',\n",
    "    f2_name='template_task_config.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### üì¶ Loading the Frozen Configs\n",
    "\n",
    "![](./assets/config_boss.png)\n",
    "\n",
    "\n",
    "**The Config Boss has been tackled - for now.**<br>\n",
    "We‚Äôll now load the frozen task and resource config files into an object so they can be referenced in the following steps.\n",
    "\n",
    "\n",
    "Pro-tip: If I've lost you in the configs, just run all the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gigl.env.pipelines_config import get_resource_config\n",
    "from gigl.src.common.types.pb_wrappers.gbml_config import GbmlConfigPbWrapper\n",
    "from gigl.src.common.types.pb_wrappers.gigl_resource_config import GiglResourceConfigWrapper\n",
    "\n",
    "frozen_task_config = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(\n",
    "    gbml_config_uri=FROZEN_TASK_CONFIG_PATH\n",
    ")\n",
    "resource_config: GiglResourceConfigWrapper = get_resource_config(\n",
    "    resource_config_uri=RESOURCE_CONFIG_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessor\n",
    "\n",
    "Once we have the configs, and our data, the first step is to preprocess the data.\n",
    "\n",
    "![](../../docs/assets/images/framework/framework_tabular_pre.png)\n",
    "\n",
    "Our Data Preprocessor leverages TensorFlow Transform (TFT) - which is widely used in the industry for preprocessing and feature engineering @ scale. It enables users to define data preprocessing pipelines for industry-scale data using tensorflow constructs, which can then be executed efficiently using large-scale data processing frameworks like Apache Beam.\n",
    "\n",
    "For folks that are unfamiliar, you essentially write a function like below (with some boilerplate), and TFT will automatically create the Apache Beam computation graph so you can process TB's of data.\n",
    "\n",
    "![](./assets/tft_example.png)\n",
    "\n",
    "‚ÑπÔ∏è For this example we use [toy_data_preprocessor_config.py](./toy_data_preprocessor_config.py). You will note that in this case, we are not doing anything special (i.e., no feature engineering), just reading from BQ and passing through the features. \n",
    "\n",
    "### Inputs into Data Preprocessor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "print(\"Frozen Config DataPreprocessor Information:\")\n",
    "print(\"- Data Preprocessor Config: Specifies what class to use for datapreprocessing and any arguments that might be passed in at runtime to that class\")\n",
    "print(textwrap.indent(str(frozen_task_config.dataset_config.data_preprocessor_config), '\\t'))\n",
    "print(\"- Preprocessed Metadata Uri: Specifies path to the preprocessed metadata file that will be generated by this component and used by subsequent components to understand and find the data that was preprocessed\")\n",
    "print(textwrap.indent(str(frozen_task_config.shared_config.preprocessed_metadata_uri), '\\t'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Data Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m gigl.src.data_preprocessor.data_preprocessor \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$FROZEN_TASK_CONFIG_PATH \\\n",
    "--resource_config_uri=$RESOURCE_CONFIG_PATH \\\n",
    "--custom_worker_image_uri=$DATAFLOW_RUNTIME_IMG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the DataPreprocessor doing?\n",
    "\n",
    "1. **Node and Edge Enumeration**\n",
    "* Applies **Storage saving trick #1** - condensed edge/node type representations\n",
    "* Also, applies **Storage saving trick #2** - Node ID Enumeration\n",
    "  * Step to internally map all the node ids to integers to mitigate space overhead. Other components operate on these enumerated identifiers to reduce storage footprint, memory overhead, and network traffic.\n",
    "    * Before: \n",
    "        ```\n",
    "        NodeType: str | int | ...\n",
    "        NodeId: str | int | ...\n",
    "        EdgeType: [\n",
    "            src_node_type: NodeType,\n",
    "            relation: str,\n",
    "            dst_node_type: NodeType\n",
    "        ]\n",
    "        Edge[\n",
    "            edge_type: EdgeType\n",
    "            src_node: NodeId,\n",
    "            dst_node: NodeId\n",
    "        ]\n",
    "        ```\n",
    "    * After saving trick #1 and #2: \n",
    "        ```\n",
    "        Edge[\n",
    "            condensed_edge_type: int\n",
    "            src_node_id: int,\n",
    "            dst_node_id: int\n",
    "        ] # 32*3 bits - wire can be even more efficient w/ batching\n",
    "        ```\n",
    "2. **Apply user-defined TFT transformations**\n",
    "* For each node and edge type, spins up a TFT Dataflow job which:\n",
    "  - Analyzes: computes statistics from full data\n",
    "  - Transforms: applies transformations using those statistics\n",
    "  - Output: Transformed data to GCS\n",
    "\n",
    "3. **Output a Preprocessed Metadata Config**\n",
    "  - This config helps downstream components understand the data schema, types, and storage locations\n",
    "  - In most cases, you can treat it as a black box üòÆ‚Äçüí®.\n",
    "  - But, let‚Äôs take a quick look so you have a clearer sense of what‚Äôs inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frozen_task_config = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(\n",
    "    gbml_config_uri=FROZEN_TASK_CONFIG_PATH\n",
    ")\n",
    "preprocessed_metadata_pb = frozen_task_config.preprocessed_metadata_pb_wrapper.preprocessed_metadata_pb\n",
    "print(preprocessed_metadata_pb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets also take a look at that what kind of files are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Condensed Node Type Mapping:\")\n",
    "print(textwrap.indent(str(frozen_task_config.graph_metadata.condensed_node_type_map), '\\t'))\n",
    "print(\"Condensed Edge Type Mapping:\")\n",
    "print(textwrap.indent(str(frozen_task_config.graph_metadata.condensed_edge_type_map), '\\t'))\n",
    "\n",
    "preprocessed_nodes = preprocessed_metadata_pb.condensed_node_type_to_preprocessed_metadata[0].tfrecord_uri_prefix\n",
    "preprocessed_edges = preprocessed_metadata_pb.condensed_edge_type_to_preprocessed_metadata[0].main_edge_info.tfrecord_uri_prefix\n",
    "print(f\"Preprocessed Nodes are stored in: {preprocessed_nodes}\")\n",
    "print(f\"Preprocessed Edges are stored in: {preprocessed_edges}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not a lot of data so we will have likely just generated one file for each of the preprocessed nodes and edges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $preprocessed_nodes && gsutil ls $preprocessed_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraph Sampler (SGS)\n",
    "![](../../docs/assets/images/framework/framework_tabular_sgs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m gigl.src.subgraph_sampler.subgraph_sampler \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$FROZEN_TASK_CONFIG_PATH \\\n",
    "--resource_config_uri=$RESOURCE_CONFIG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéØ Purpose of the SGS\n",
    "\n",
    "**Trainer memory saving trick:**\n",
    "SGS processes node and edge data from the Data Preprocessor to generate localized training and inference samples of **k-hop localized subgraphs**. This design allows each of the samples to be stored independently, enabling downstream components to work efficiently with just relevant node batches‚Äîno need to load the entire graph into memory.\n",
    "\n",
    "#### üìÅ Output Structure\n",
    "Upon completion, for this specific use case of Anchor-Based Link Prediction, SGS produces two directories of subgraph samples:\n",
    "\n",
    "1. Node-Anchor-Based Link Prediction Training Samples\n",
    "- Generated a specified number of samples\n",
    "- Contains:\n",
    "    - Root node and its sampled neighborhood w/\n",
    "    - Positive edges, and their respective node's sampled neighborhoods\n",
    "\n",
    "2. Rooted Neighborhood Samples\n",
    "- Generated for each node in the graph\n",
    "- Contains sampled node neighborhoods\n",
    "- Used for both inference and as negative training samples\n",
    "    - At, training time these are sampled randomly for each training batch\n",
    "    - At the gigantic scale we operate in, these are almost certainly not true edges.\n",
    "\n",
    "#### üì∏ Example Visualizations\n",
    "Each run of the sampler produces different subgraphs due to randomized sampling.\n",
    "Below are example snapshots:\n",
    "##### Training Sample (2 hop subgraph)\n",
    "- Root Node: `9`\n",
    "- Positive Edge: `9 --> 7`\n",
    "- Node `8` is node `9`'s two hop; node `2` is node `7`'s two hop\n",
    "\n",
    "<img src=\"./assets/link_pred_sample_node_9.png\" alt=\"Link Pred Sample\" width=\"100%\" />\n",
    "\n",
    "##### Random negative\n",
    "- Root Node: `1`\n",
    "- The training data loader may choose to randomly sample any rooted node neighborhood to act as a \"negative sample\" - in this case it chose `1`\n",
    "- When doing inference for root node `1` we will also use this subgraph\n",
    "\n",
    "<img src=\"./assets/rooted_neighborhood_node_1.png\" alt=\"Random Negative Sample\" width=\"100%\" />\n",
    "\n",
    "\n",
    "#### ‚öôÔ∏è Scalable Sample Generation with Spark\n",
    "The SGS extract-transform-load (ETL) pipeline leverages Apache Spark to perform large-scale joins required for subgraph sampling. Considerable engineering effort went into optimizing this pipeline, including:\n",
    "- Fine-tuning partitioning strategies\n",
    "- Managing data shuffling, caching, and spill behavior\n",
    "- Reducing unnecessary CPU overhead\n",
    "\n",
    "This enables efficient subgraph construction without requiring a shared in-memory graph state, mirroring the data-parallel paradigm commonly used in other large-scale ML workflows.\n",
    "\n",
    "While frameworks like GraphLearn, ByteGNN, DeepGNN, and GraphStorm rely on real-time sampling through graph backends and partition-aware execution (inspired by works from Chiang et al., Karypis, and Kumar), approaches like AGL, TF-GNN, and MultiSAGE favor tabularization‚Äîtransforming graph structure into flat tables for scalable processing.\n",
    "\n",
    "\n",
    "##### üñºÔ∏è Visualizing Samples\n",
    "We will see where the SGS data is generated below, followed by rendering a few samples and random negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattened Graph Metadata\n",
    "flattened_graph_metadata = frozen_task_config.shared_config.flattened_graph_metadata\n",
    "print(flattened_graph_metadata)\n",
    "\n",
    "from snapchat.research.gbml import training_samples_schema_pb2\n",
    "from gigl.common.utils.jupyter_magics import GraphVisualizer, GraphVisualizerLayoutMode, PbVisualizer, PbVisualizerFromOutput\n",
    "\n",
    "# We will sample node with this id and visualize it. You will see positive edge marked in red and the root node with a black border.\n",
    "SAMPLE_NODE_ID = 9\n",
    "# We will sample random negative nodes with these ids and visualize them. You will see the root node with a black border.\n",
    "SAMPLE_RANDOM_NEGATIVE_NODE_IDS = [1, 3]\n",
    "\n",
    "print(f\"The original global graph:\")\n",
    "GraphVisualizer.visualize_graph(original_graph_heterodata, layout_mode=GraphVisualizerLayoutMode.HOMOGENEOUS)\n",
    "pb_visualizer = PbVisualizer(frozen_task_config)\n",
    "print (FROZEN_TASK_CONFIG_PATH)\n",
    "\n",
    "print(f\"Node anchor prediction sample for node {SAMPLE_NODE_ID}:\")\n",
    "sample = pb_visualizer.find_node_pb(\n",
    "    unenumerated_node_id=SAMPLE_NODE_ID,\n",
    "    unenumerated_node_type=\"user\",\n",
    "    pb_type=training_samples_schema_pb2.NodeAnchorBasedLinkPredictionSample,\n",
    "    from_output=PbVisualizerFromOutput.SGS\n",
    ")\n",
    "print(f\"Node anchor prediction sample for node {SAMPLE_NODE_ID}:\")\n",
    "pb_visualizer.plot_pb(sample, layout_mode=GraphVisualizerLayoutMode.HOMOGENEOUS)\n",
    "\n",
    "for random_negative_node_id in SAMPLE_RANDOM_NEGATIVE_NODE_IDS:\n",
    "    random_negative_sample = pb_visualizer.find_node_pb(\n",
    "        unenumerated_node_id=random_negative_node_id,\n",
    "        unenumerated_node_type=\"user\",\n",
    "        pb_type=training_samples_schema_pb2.RootedNodeNeighborhood,\n",
    "        from_output=PbVisualizerFromOutput.SGS\n",
    "    )\n",
    "    print(f\"Random negative sample for node {random_negative_node_id}:\")\n",
    "    pb_visualizer.plot_pb(\n",
    "        random_negative_sample,\n",
    "        layout_mode=GraphVisualizerLayoutMode.HOMOGENEOUS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Generator\n",
    "\n",
    "Generating Train/Test/Val Splits\n",
    "\n",
    "![](../../docs/assets/images/framework/framework_tabular_splitgen.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frozen_task_config.dataset_config.split_generator_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîÄ Split Strategy: Transductive\n",
    "In this example, we're using the transductive split strategy, as specified in the frozen_config.\n",
    "For transductive, at training time, it uses training message edges to predict training supervision edges. At validation time, the training message edges and training supervision edges are used to predict the validation edges and then all 3 are used to predict test edges.\n",
    "\n",
    "<img src=\"./assets/transductive.png\" alt=\"Transductive Split\" width=\"500px\" />\n",
    "\n",
    "\n",
    "#### üì¶ Role of the Split Generator\n",
    "The Split Generator reads localized subgraph samples produced by the SGS and applies the selected SplitStrategy to divide the data into: train/val/test splits\n",
    "\n",
    "\n",
    "#### üß© Supported Split Strategies (GiGL Platform)\n",
    "Several standard split routines are implemented and available via SplitStrategy and Assigner classes, including:\n",
    "- Inductive Node Classification\n",
    "- Transductive Node Classification\n",
    "- Transductive Link Prediction\n",
    "\n",
    "#### üìö Further Reading\n",
    "To understand the theory behind these strategies, see:\n",
    "1. [CS224W Lecture Slides ](https://web.stanford.edu/class/cs224w/slides/05-GNN3.pdf)\n",
    "2. [Graph Link Prediction](https://zqfang.github.io/2021-08-12-graph-linkpredict/) (relevant for explaining transductive vs inductive).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m gigl.src.split_generator.split_generator \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$FROZEN_TASK_CONFIG_PATH \\\n",
    "--resource_config_uri=$RESOURCE_CONFIG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon completion, there will be 3 folders for train,test, and val. Each of them contains the protos for the positive and negaitve samples. The path for these folders is specified in the following location in the frozen_config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metadata = frozen_task_config.shared_config.dataset_metadata\n",
    "print(dataset_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üñºÔ∏è Visualizing Samples\n",
    "We will visualize the train, validation, and test samples for a few nodes. These subgraphs are generated based on the configured task setup.\n",
    "\n",
    "> ‚ö†Ô∏è Important Note on Validation & Test Sets:\n",
    "> Due to the way edges are randomly bucketed into train, val, and test sets‚Äîindependent of their status as supervision (i.e., labeled) edges, not all validation or test samples will contain positive labels.\n",
    "\n",
    "This behavior is intentional and acceptable at scale. When working with large datasets and large batch sizes, it's statistically likely that each batch will contain some supervision edges. However, individual batches may occasionally lack them, especially in val/test.\n",
    "\n",
    "______________________________________________________________________\n",
    "\n",
    "##### üß† Implications for Training\n",
    "This design introduces some considerations:\n",
    "- ‚úÖ Training samples are typically unaffected due to the high frequency of labeled edges.\n",
    "- ‚ö†Ô∏è Although, at large scale it is not frequent, validation and test Loops need to account for batches without any supervision edges.\n",
    "- üõë Early Stopping Logic should be robust to these fluctuations e.g., don‚Äôt trigger early stopping based on a single underperforming batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.common.utils.jupyter_magics import  GraphVisualizerLayoutMode\n",
    "\n",
    "\n",
    "for node_id in range(min(original_graph_heterodata.num_nodes, 2)):\n",
    "    print(f\"Node anchor prediction sample for node {node_id}:\")\n",
    "    sample_train = pb_visualizer.find_node_pb(\n",
    "        from_output=PbVisualizerFromOutput.SPLIT_TRAIN,\n",
    "        unenumerated_node_id=node_id,\n",
    "        unenumerated_node_type=\"user\",\n",
    "        pb_type=training_samples_schema_pb2.NodeAnchorBasedLinkPredictionSample\n",
    "    )\n",
    "    sample_val = pb_visualizer.find_node_pb(\n",
    "        from_output=PbVisualizerFromOutput.SPLIT_VAL,\n",
    "        unenumerated_node_id=node_id,\n",
    "        unenumerated_node_type=\"user\",\n",
    "        pb_type=training_samples_schema_pb2.NodeAnchorBasedLinkPredictionSample\n",
    "    )\n",
    "    sample_test = pb_visualizer.find_node_pb(\n",
    "        from_output=PbVisualizerFromOutput.SPLIT_TEST,\n",
    "        unenumerated_node_id=node_id,\n",
    "        unenumerated_node_type=\"user\",\n",
    "        pb_type=training_samples_schema_pb2.NodeAnchorBasedLinkPredictionSample\n",
    "    )\n",
    "    if sample_train:\n",
    "        print(f\"Train sample for node {node_id}: \")\n",
    "        pb_visualizer.plot_pb(\n",
    "            sample_train,\n",
    "            layout_mode=GraphVisualizerLayoutMode.HOMOGENEOUS,\n",
    "        )\n",
    "    else:\n",
    "        print(f\"No train sample found for node {node_id}.\")\n",
    "    if sample_val:\n",
    "        print(f\"Validation sample for node {node_id}:\")\n",
    "        pb_visualizer.plot_pb(\n",
    "            sample_val,\n",
    "            layout_mode=GraphVisualizerLayoutMode.HOMOGENEOUS,\n",
    "        )\n",
    "    else:\n",
    "        print(f\"No validation sample found for node {node_id}.\")\n",
    "    if sample_test:\n",
    "        print(f\"Test sample for node {node_id}:\")\n",
    "        pb_visualizer.plot_pb(\n",
    "            sample_test,\n",
    "            layout_mode=GraphVisualizerLayoutMode.HOMOGENEOUS,\n",
    "        )\n",
    "    else:\n",
    "        print(f\"No test sample found for node {node_id}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have our graph data samples ready to be processed by the trainer and inferencer components. These components will extract representations/embeddings by learning contextual information for the specified task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Inference\n",
    "\n",
    "![](../../docs/assets/images/framework/framework_tabular_train_infer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m gigl.src.training.trainer \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$FROZEN_TASK_CONFIG_PATH \\\n",
    "--resource_config_uri=$RESOURCE_CONFIG_PATH \\\n",
    "--cpu_docker_uri=$CPU_SRC_IMG \\\n",
    "--cuda_docker_uri=$CUDA_SRC_IMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model gets saved to the path specified in the frozen task config.\n",
    "print(f\"Trained model saved to: {frozen_task_config.shared_config.trained_model_metadata.trained_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m gigl.src.inference.inferencer \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$FROZEN_TASK_CONFIG_PATH \\\n",
    "--resource_config_uri=$RESOURCE_CONFIG_PATH \\\n",
    "--cpu_docker_uri=$CPU_SRC_IMG \\\n",
    "--cuda_docker_uri=$CUDA_SRC_IMG \\\n",
    "--custom_worker_image_uri=$DATAFLOW_RUNTIME_IMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at inference results\n",
    "\n",
    "from gigl.src.common.utils.bq import BqUtils\n",
    "\n",
    "bq_emb_out_table = frozen_task_config.shared_config.inference_metadata.node_type_to_inferencer_output_info_map[frozen_task_config.graph_metadata_pb_wrapper.homogeneous_node_type].embeddings_path\n",
    "print(f\"Embeddings should be successfully stored in the following location: {bq_emb_out_table}\")\n",
    "\n",
    "bq_utils = BqUtils(project=resource_config.project)\n",
    "query = f\"SELECT * FROM {bq_emb_out_table} LIMIT 5\"\n",
    "result = list(bq_utils.run_query(query=query, labels={}))\n",
    "\n",
    "print(f\"Query result: \")\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex AI Pipelines\n",
    "\n",
    "You can also run GiGL pipelines on [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction). Doing so provides a number of advantages, such as:\n",
    "\n",
    "1. Decoupling orchestration from dev machines - when you close your laptop the pipeline keeps running\n",
    "2. Orchestrate new runs from GCP console. You can start/stop runs on the console\n",
    "3. Data isolation - production Service Accounts and Projects can access sensitive data\n",
    "\n",
    "GiGL has a [`KfpOrchestrator`](https://snapchat.github.io/GiGL/docs/api/gigl/orchestration/kubeflow/kfp_orchestrator/index.html#gigl.orchestration.kubeflow.kfp_orchestrator.KfpOrchestrator) client which will let you launch GiGL pipelines on Vertex AI.\n",
    "\n",
    "We need to call two methods on the client to start a new pipeline run:\n",
    "\n",
    "1. [`compile`](https://snapchat.github.io/GiGL/_modules/gigl/orchestration/kubeflow/kfp_orchestrator.html#KfpOrchestrator.compile), which generates a KFP Pipeline spec that Vertex AI can consume to orchestrate the pipeline. Under the hood, we write the pipeline yaml to some local file, but you can configure this with the `dst_compiled_pipeline_path` argument.\n",
    "2. [`run`](https://snapchat.github.io/GiGL/_modules/gigl/orchestration/kubeflow/kfp_orchestrator.html#KfpOrchestrator.run) which uploads the pipeline spec to Vertex AI and starts a new pipeline job. Note that `run` requires an applied_task_identifier (pipeline job name), which must be unique in every Project and Region.\n",
    "\n",
    "Note that `compile` requires docker images to be passed in, this is because the docker images that are used by each component are baked into the pipeline definition. For this tutorial, we will be using the default images, however if you want to customize the buisness logic here,  you will need to provide your own docker images. GiGL has a handy [`script`](https://github.com/Snapchat/GiGL/blob/main/scripts/build_and_push_docker_image.py) will push the docker images for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also run the entire GiGL pipeline on Vertex AI\n",
    "# With the KfpOrchestrator [1]\n",
    "# Note that we use TEMPLATE_TASK_CONFIG_PATH here so we can have another \"dataset\" e.g. SharedConfig.\n",
    "# [1]: https://snapchat.github.io/GiGL/docs/api/gigl/orchestration/kubeflow/kfp_orchestrator/index.html#gigl.orchestration.kubeflow.kfp_orchestrator.KfpOrchestrator\n",
    "from gigl.orchestration.kubeflow.kfp_orchestrator import KfpOrchestrator\n",
    "\n",
    "orchestrator = KfpOrchestrator()\n",
    "# First, compile the KFP pipeline definition\n",
    "orchestrator.compile(\n",
    "    cuda_container_image=CUDA_SRC_IMG,\n",
    "    cpu_container_image=CPU_SRC_IMG,\n",
    "    dataflow_container_image=DATAFLOW_RUNTIME_IMG,\n",
    ")\n",
    "# Then, run it.\n",
    "orchestrator.run(\n",
    "    applied_task_identifier=JOB_NAME,\n",
    "    task_config_uri=TEMPLATE_TASK_CONFIG_PATH,\n",
    "    resource_config_uri=RESOURCE_CONFIG_PATH,\n",
    ")\n",
    "\n",
    "# You'll eventually see a link to the KFP pipeline in the logs.\n",
    "# Something like: \n",
    "# https://console.cloud.google.com/vertex-ai/pipelines/locations/us-central1/runs/<job_id>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gigl (Local)",
   "language": "python",
   "name": "micromamba-env-gigl-gigl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../..\n",
    "# We need to change the working directory to the root of GiGL repo so we can import the necessary modules/scripts used below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b35af",
   "metadata": {},
   "source": [
    "# Setting up GCP Project and configs\n",
    "Assuming you have a GCP project setup:\n",
    "\n",
    "1. Open up `configs/example_resource_config.yaml` and fill all relevant fields under `common_compute_config`:\n",
    "  - project\n",
    "  - region\n",
    "  - temp_assets_bucket\n",
    "  - temp_regional_assets_bucket\n",
    "  - perm_assets_bucket\n",
    "  - temp_assets_bq_dataset_name\n",
    "  - embedding_bq_dataset_name\n",
    "  - gcp_service_account_email\n",
    "\n",
    "2. Ensure your service account has relevant perms (A non-exaustive list):\n",
    "  - roles/bigquery.user\n",
    "  - roles/cloudprofiler.user\n",
    "  - roles/compute.admin\n",
    "  - roles/dataflow.admin\n",
    "  - roles/dataflow.worker\n",
    "  - roles/dataproc.editor\n",
    "  - roles/logging.logWriter\n",
    "  - roles/monitoring.metricWriter\n",
    "  - roles/notebooks.legacyViewer\n",
    "  - roles/aiplatform.user\n",
    "  - roles/dataproc.worker\n",
    "  - roles/storage.objectAdmin : on relevant buckets\n",
    "  - roles/artifactregistry.reader\n",
    "  - roles/artifactregistry.writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import getpass\n",
    "\n",
    "from gigl.common import LocalUri\n",
    "from gigl.env.pipelines_config import get_resource_config\n",
    "from gigl.src.common.types.pb_wrappers.gigl_resource_config import GiglResourceConfigWrapper\n",
    "from gigl.src.common.types.pb_wrappers.gbml_config import GbmlConfigPbWrapper\n",
    "\n",
    "curr_datetime = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Firstly, let's give your job a name and ensure that the resource and task configs exist and can be loaded\n",
    "JOB_NAME = f\"{getpass.getuser()}_cora_{curr_datetime}\"\n",
    "TEMPLATE_TASK_CONFIG_URI = LocalUri(\"examples/distributed/configs/e2e_cora_udl_glt_task_config.yaml\")\n",
    "RESOURCE_CONFIG_URI = LocalUri(\"examples/distributed/configs/example_resource_config.yaml\")\n",
    "\n",
    "TEMPLATE_TASK_CONFIG: GbmlConfigPbWrapper = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(gbml_config_uri=TEMPLATE_TASK_CONFIG_URI)\n",
    "RESOURCE_CONFIG: GiglResourceConfigWrapper = get_resource_config(resource_config_uri=RESOURCE_CONFIG_URI)\n",
    "PROJECT = RESOURCE_CONFIG.project\n",
    "\n",
    "\n",
    "print(f\"Succesfully found task config and resource config. Script will help execute job: {JOB_NAME} on project: {PROJECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1892357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets run some basic checks to validate correctness of the task and resource config\n",
    "from gigl.src.validation_check.config_validator import kfp_validation_checks\n",
    "\n",
    "kfp_validation_checks(\n",
    "    job_name=JOB_NAME,\n",
    "    task_config_uri=TEMPLATE_TASK_CONFIG_URI,\n",
    "    resource_config_uri=RESOURCE_CONFIG_URI,\n",
    "    # config_populator is the first step in the pipeline; where we will populat the template task config specified above and generate a frozen config\n",
    "    start_at=\"config_populator\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385941bd",
   "metadata": {},
   "source": [
    "## Compiling Src Docker images\n",
    "\n",
    "You will need to build and push docker images with your custom code so that individual GiGL components can leverage your code.\n",
    "For this experiment we will consider the MAG240M specs and code to be \"custom code\", and we will guide you how to build a docker image with the code.\n",
    "\n",
    "We will make use of `scripts/build_and_push_docker_image.py` for this.\n",
    "\n",
    "Make note that this builds `containers/Dockerfile.src` and `containers/Dockerfile.dataflow.src`; which have instructions to `COPY` the `examples` folder - which contains all the source code for MAG240M, and it has all the GiGL src code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806de7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.build_and_push_docker_image import build_and_push_cpu_image, build_and_push_cuda_image, build_and_push_dataflow_image\n",
    "\n",
    "DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG = f\"gcr.io/{PROJECT}/gigl_dataflow_runtime:{curr_datetime}\"\n",
    "DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG = f\"gcr.io/{PROJECT}/gigl_cuda:{curr_datetime}\"\n",
    "DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG = f\"gcr.io/{PROJECT}/gigl_cpu:{curr_datetime}\"\n",
    "\n",
    "build_and_push_dataflow_image(\n",
    "    image_name=DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG,\n",
    ")\n",
    "build_and_push_cuda_image(\n",
    "    image_name=DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG,\n",
    ")\n",
    "build_and_push_cpu_image(\n",
    "    image_name=DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG,\n",
    ")\n",
    "\n",
    "print(f\"\"\"We built and pushed the following docker images:\n",
    "- {DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG}\n",
    "- {DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG}\n",
    "- {DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9729a",
   "metadata": {},
   "source": [
    "## We will instantiate local runner to help orchestrate the test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4fdfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.orchestration.local.runner import Runner, PipelineConfig\n",
    "\n",
    "\n",
    "runner = Runner()\n",
    "pipeline_config = PipelineConfig(\n",
    "    applied_task_identifier=JOB_NAME,\n",
    "    task_config_uri=TEMPLATE_TASK_CONFIG_URI,\n",
    "    resource_config_uri=RESOURCE_CONFIG_URI,\n",
    "    custom_cuda_docker_uri=DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG,\n",
    "    custom_cpu_docker_uri=DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG,\n",
    "    dataflow_docker_uri=DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e69e2f",
   "metadata": {},
   "source": [
    "### First we will run config populator\n",
    "The config populator takes in a template `GbmlConfig` and outputs a frozen `GbmlConfig` by populating all job related metadata paths in `sharedConfig`. These are mostly GCS paths which the following components read and write from, and use as an intermediary data communication medium. For example, the field `sharedConfig.trainedModelMetadata` is populated with a GCS URI, which indicates to the Trainer to write the trained model to this path, and to the Inferencer to read the model from this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b433f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.src.common.utils.file_loader import FileLoader\n",
    "frozen_config_uri = runner.run_config_populator(pipeline_config=pipeline_config)\n",
    "frozen_config = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(gbml_config_uri=frozen_config_uri)\n",
    "file_loader = FileLoader()\n",
    "\n",
    "print(f\"Config Populator has successfully generated the following frozen config from the template ({TEMPLATE_TASK_CONFIG_URI}) :\")\n",
    "print(frozen_config.gbml_config_pb)\n",
    "\n",
    "pipeline_config.task_config_uri = frozen_config_uri # We need to update the task config uri to the new frozen config uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa990b5",
   "metadata": {},
   "source": [
    "# Next we run the preprocessor\n",
    "The Data Preprocessor reads node, edge and respective feature data from a data source, and produces preprocessed / transformed versions of all this data, for subsequent components to use.  It uses Tensorflow Transform to achieve data transformation in a distributed fashion, and allows for transformations like categorical encoding, scaling, normalization, casting and more.\n",
    "\n",
    "In this case we are using preprocessing spec defined in `python/gigl/src/mocking/mocking_assets/passthrough_preprocessor_config_for_mocked_assets.py` - take a look for more details.\n",
    "\n",
    "You will note that the preprocessor will create a few BQ jobs to prepare the node and edge tables, subsequently it will kick off TFT (dataflow) jobs to do the actual preprocessing. The preprocessor will: (1) create a preprocessing spec and dump it to path specified in frozen config `sharedConfig.preprocessedMetadataUri`. (2) Respective Dataflow jobs will dump the preprocessed assets as `.tfrecord` files to the paths specified inside the preprocessing spec `preprocessedMetadataUri`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915d7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARN: There is an issue when trying to run dataflow jobs from inside a jupyter kernel; thus we cannot use the line \n",
    "# below to run the preprocessor as you would normally in a python script.\n",
    "# runner.run_data_preprocessor(pipeline_config=pipeline_config) \n",
    "\n",
    "# Instead, we will run the preprocessor from the command line.\n",
    "# Note: You can actually do this with every component; we just make use of the runner to make it easier to run the components.\n",
    "!python -m gigl.src.data_preprocessor.data_preprocessor \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$frozen_config_uri \\\n",
    "--resource_config_uri=$RESOURCE_CONFIG_URI \\\n",
    "--custom_worker_image_uri=$DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(mkolodner): Add trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6276b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARN: There is an issue when trying to run dataflow jobs from inside a jupyter kernel; thus we cannot use the line \n",
    "# below to run the inferencer as you would normally in a python script.\n",
    "# runner.run_inferencer(pipeline_config=pipeline_config) \n",
    "\n",
    "# Instead, we will run the inferencer from the command line.\n",
    "# Note: You can actually do this with every component; we just make use of the runner to make it easier to run the components.\n",
    "!python -m gigl.src.inference.inferencer \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$frozen_config_uri \\\n",
    "--resource_config_uri=$RESOURCE_CONFIG_URI \\\n",
    "--custom_worker_image_uri=$DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG \\\n",
    "--cpu_docker_uri=$DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG \\\n",
    "--cuda_docker_uri=$DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcceaac9",
   "metadata": {},
   "source": [
    "### Inference\n",
    "The Inferencer component is responsible for running inference of a trained model on samples generated by the Subgraph Sampler component.  At a high level, it works by applying a trained model in an embarrassingly parallel and distributed fashion across these samples, and persisting the output embeddings and/or predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29d52b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at inference results\n",
    "from gigl.src.common.utils.bq import BqUtils\n",
    "from gigl.src.inference.lib.assets import InferenceAssets\n",
    "\n",
    "\n",
    "bq_emb_out_table = InferenceAssets.get_enumerated_embedding_table_path(gbml_config_pb_wrapper=frozen_config, node_type=\"paper\")\n",
    "print(f\"Embeddings should be successfully stored in the following location: {bq_emb_out_table}\")\n",
    "\n",
    "bq_utils = BqUtils(project=PROJECT)\n",
    "query = f\"SELECT * FROM {bq_emb_out_table} LIMIT 5\"\n",
    "result = list(bq_utils.run_query(query=query, labels={}))\n",
    "\n",
    "print(f\"Query result: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bagl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

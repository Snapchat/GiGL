{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Example - Tabularized GiGL\n",
    "\n",
    "Latest version of this notebook can be found on [github](https://github.com/Snapchat/GiGL/blob/main/examples/toy_visual_example/toy_example_walkthrough.ipynb)\n",
    "\n",
    "\n",
    "This notebook provides a walkthrough of preprocessing, subgraph sampling, and split generation components with a small toy graph for GiGL's Tabularized setting for training/inference. It will help you understand how each of these components prepare tabularized subgraphs.\n",
    "\n",
    "\n",
    "## Overview Of Components\n",
    "This notebook demonstrates the process of a simple, human-digestible graph being passed through all the pipeline components in GiGL in preparation for training to help understand how each of the components work.\n",
    "\n",
    "The pipeline consists of the following components:\n",
    "\n",
    "- **Config Populator**: Takes a template config and creates a frozen workflow config that dictates all inputs/outputs and business parameters that are read and used by each subsequent component.\n",
    "    - Input: `template_config.yaml`\n",
    "    - Output: `frozen_gbml_config.yaml`\n",
    "&nbsp;\n",
    "\n",
    "- **Data Preprocessor**: Transforms necessary node and edge feature assets as needed as a precursor step in most ML tasks according to the user-provided data preprocessor config class.\n",
    "    - Input: `frozen_gbml_config.yaml`, which includes the user-defined preprocessor class for custom logic, and custom arguments can be passed under dataPreprocessorArgs.\n",
    "    - Output: PreprocessedMetadata Proto, which includes inferred GraphMetadata and preprocessed graph data Tfrecords after applying the user-defined preprocessing function.\n",
    "&nbsp;\n",
    "\n",
    "- **Subgraph Sampler**: Samples k-hop subgraphs for each node according to user-provided arguments.\n",
    "    - Input: `frozen_gbml_config.yaml`, `resource_config.yaml`\n",
    "    - Output: Subgraph Samples (tfrecord format based on predefined schema in protos) are stored in the URI defined in the flattenedGraphMetadata field.\n",
    "&nbsp;\n",
    "\n",
    "- **Split Generator**: Splits subgraph sampler outputs into train/test/val sets according to the user-provided split strategy class.\n",
    "    - Input: `frozen_gbml_config.yaml`, which includes an instance of SplitStrategy and an instance of Assigner.\n",
    "    - Output: TFRecord samples.\n",
    "&nbsp;\n",
    "\n",
    "- **Trainer**: The trainer component reads the output of the split generator and trains a model on the training set, stops based on the validation set, and evaluates on the test set.\n",
    "    - Input: `frozen_gbml_config.yaml`\n",
    "    - Output: state_dict stored in trainedModelUri.\n",
    "&nbsp;\n",
    "\n",
    "- **Inferencer**: Runs inference of a trained model on samples generated by the Subgraph Sampler.\n",
    "    - Input: `frozen_gbml_config.yaml`\n",
    "    - Output: Embeddings and/or prediction assets.\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Silence TF logspam\n",
    "\n",
    "from gigl.common.utils.jupyter_magics import change_working_dir_to_gigl_root\n",
    "change_working_dir_to_gigl_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Graph\n",
    "\n",
    "We use the input graph defined in [examples/toy_visual_example/graph_config.yaml](./graph_config.yaml). \n",
    "You are welcome to change this file to a custom graph of your own choosing.\n",
    "\n",
    "### Visualizing the Input Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from gigl.common.utils.jupyter_magics import GraphVisualizer\n",
    "from gigl.src.mocking.toy_asset_mocker import load_toy_graph\n",
    "\n",
    "\n",
    "original_graph_heterodata: HeteroData = load_toy_graph(graph_config_path=\"examples/toy_visual_example/graph_config.yaml\")\n",
    "# Visualize the graph\n",
    "GraphVisualizer.visualize_graph(original_graph_heterodata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Configs\n",
    "\n",
    "The first thing we need to do is create the resource and task configs. \n",
    "\n",
    "- **Task Config**: Specifies task-related configurations, guiding the behavior of components according to the needs of your machine learning task. See [Task Config Guide](../../docs/user_guide/config_guides/task_config_guide.md). For this task, we have already provided a task config: [task_config.yaml](./task_config.yaml).\n",
    "\n",
    "- **Resource Config**: Details the resource allocation and environmental settings across all GiGL components. This encompasses shared resources for all components, as well as component-specific settings. See [Resource Config Guide](../../docs/user_guide/config_guides/resource_config_guide.md). For this task, we provide a resource [resource_config.yaml](./resource_config.yaml). The provided default values in `shared_resource_config.common_compute_config` will need to be changed.\n",
    "\n",
    "  - **Instructions to configure the resource config to work**:\n",
    "    If you have not already, please follow the [Quick Start Guide](../../docs/user_guide/getting_started/quick_start.md) to set up your cloud environment and create a default test resource config. You can then copy the relevant `shared_resource_config.common_compute_config` to [resource_config.yaml](./resource_config.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import getpass\n",
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "\n",
    "from gigl.common import Uri, UriFactory\n",
    "notebook_dir = pathlib.Path(\"./examples/toy_visual_example\").as_posix() # We should be in root dir because of cell # 1\n",
    "# Add the root gigl dir to the Python path so `example` folder can be imported as a module.\n",
    "\n",
    "# You are welcome to customize these  to point to your own configuration files.\n",
    "JOB_NAME = f\"{getpass.getuser()}_gigl_toy_example_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "TEMPLATE_TASK_CONFIG_PATH: Uri = UriFactory.create_uri(f\"{notebook_dir}/template_task_config.yaml\")\n",
    "FROZEN_TASK_CONFIG_POINTER_FILE_PATH: Uri = UriFactory.create_uri(f\"/tmp/GiGL/{JOB_NAME}/frozen_task_config.yaml\")\n",
    "pathlib.Path(FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri).parent.mkdir(parents=True, exist_ok=True)\n",
    "# Ensure you change the resource config path to point to your own resource configuration\n",
    "# i.e. what was exported to $GIGL_TEST_DEFAULT_RESOURCE_CONFIG as part of the quick start guide.\n",
    "RESOURCE_CONFIG_PATH: Uri = UriFactory.create_uri(os.environ.get(\"GIGL_TEST_DEFAULT_RESOURCE_CONFIG\", f\"{notebook_dir}/resource_config.yaml\"))\n",
    "\n",
    "# Export string format of the uris so we can reference them in cells that execute bash commands below.\n",
    "os.environ[\"TEMPLATE_TASK_CONFIG_PATH\"] = TEMPLATE_TASK_CONFIG_PATH.uri\n",
    "os.environ[\"FROZEN_TASK_CONFIG_POINTER_FILE_PATH\"] = FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri\n",
    "os.environ[\"RESOURCE_CONFIG_PATH\"] = RESOURCE_CONFIG_PATH.uri\n",
    "\n",
    "print(f\"JOB_NAME: {JOB_NAME}\")\n",
    "print(f\"TEMPLATE_TASK_CONFIG_PATH: {TEMPLATE_TASK_CONFIG_PATH.uri}\")\n",
    "print(f\"FROZEN_TASK_CONFIG_POINTER_FILE_PATH: {FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri}\")\n",
    "print(f\"RESOURCE_CONFIG_PATH: {RESOURCE_CONFIG_PATH.uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on the Use of Mocked Assets\n",
    "\n",
    "This step is already done for you. We provide instructions below for posterity, in case the mocked data input [\"graph_config.yaml\"](./graph_config.yaml) is updated.\n",
    "\n",
    "You can choose to update `MOCK_DATA_GCS_BUCKET` and `MOCK_DATA_BQ_DATASET_NAME` in `python/gigl/src/mocking/lib/constants.py` to upload to resources you own.\n",
    "\n",
    "We run the following command to upload the relevant mocks to GCS and BQ:\n",
    "```bash\n",
    "python -m gigl.src.mocking.dataset_asset_mocking_suite \\\n",
    "--select mock_toy_graph_homogeneous_node_anchor_based_link_prediction_dataset \\\n",
    "--resource_config_uri=examples/toy_visual_example/resource_config.yaml\n",
    "```\n",
    "\n",
    "Subsequently, we can update the paths in [task_config.yaml](./task_config.yaml).\n",
    "\n",
    "## Validating the Configs\n",
    "\n",
    "We provide the ability to validate your resource and task configs. Although the validation is not exhaustive, it does help assert that the more common issues are not present before expensive compute is scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.src.validation_check.config_validator import kfp_validation_checks\n",
    "\n",
    "validator = kfp_validation_checks(\n",
    "    job_name=JOB_NAME,\n",
    "    task_config_uri=TEMPLATE_TASK_CONFIG_PATH,\n",
    "    resource_config_uri=RESOURCE_CONFIG_PATH,\n",
    "    start_at=\"config_populator\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Populator\n",
    "\n",
    "Takes in a template `GbmlConfig` and outputs a frozen `GbmlConfig` by populating all job-related metadata paths in `sharedConfig`. These are mostly GCS paths that the following components read and write from, and use as an intermediary data communication medium. For example, the field `sharedConfig.trainedModelMetadata` is populated with a GCS URI, which indicates to the Trainer to write the trained model to this path, and to the Inferencer to read the model from this path. See the full [Config Populator Guide](../../docs/user_guide/overview/components/config_populator.md).\n",
    "\n",
    "After running the command below, we will have created a frozen config and uploaded it to the `perm_assets_bucket` provided in the `resource config`. The path to that file will be stored in the file at `FROZEN_TASK_CONFIG_POINTER_FILE_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# We suppress the output of this cell to avoid cluttering the notebook with logs. Remove %%%capture if you want to see the output.\n",
    "\n",
    "!python -m \\\n",
    "    gigl.src.config_populator.config_populator \\\n",
    "    --job_name=\"$JOB_NAME\" \\\n",
    "    --template_uri=\"$TEMPLATE_TASK_CONFIG_PATH\" \\\n",
    "    --resource_config_uri=\"$RESOURCE_CONFIG_PATH\" \\\n",
    "    --output_file_path_frozen_gbml_config_uri=\"$FROZEN_TASK_CONFIG_POINTER_FILE_PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The command above will write the frozen task config path to the file specified by `FROZEN_TASK_CONFIG_POINTER_FILE_PATH`.\n",
    "# Lets see where it was generated\n",
    "FROZEN_TASK_CONFIG_PATH: Uri\n",
    "with open(FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri, 'r') as file:\n",
    "    FROZEN_TASK_CONFIG_PATH = UriFactory.create_uri(file.read().strip())\n",
    "print(f\"FROZEN_TASK_CONFIG_PATH: {FROZEN_TASK_CONFIG_PATH}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Diff Between Template and Frozen Config\n",
    "\n",
    "We now have a frozen task config, with the path specified by `FROZEN_TASK_CONFIG_PATH`. We visualize the diff between the `frozen_task_config` generated by the `config_populator` and the original `template_task_config`. All the code below is just to do that and has nothing to do with GiGL.\n",
    "\n",
    "Specifically, note that:\n",
    "1. The component added `sharedConfig` to the YAML, which contains all the intermediary and final output paths for each component.\n",
    "2. It also added a `condensedEdgeTypeMap` and a `condensedNodeTypeMap`, which map all provided edge types and node types to `int` to save storage space:\n",
    "   - `EdgeType: Tuple[srcNodeType: str, relation: str, dstNodeType: str)] -> int`, and \n",
    "   - `NodeType: str -> int`\n",
    "   - Note: You may also provide your own condensedMaps; they will be generated for you if not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.common.utils.jupyter_magics import show_task_config_colored_unified_diff\n",
    "\n",
    "show_task_config_colored_unified_diff(\n",
    "    f1_uri=FROZEN_TASK_CONFIG_PATH,\n",
    "    f2_uri=TEMPLATE_TASK_CONFIG_PATH,\n",
    "    f1_name='frozen_task_config.yaml',\n",
    "    f2_name='template_task_config.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will load the frozen task and resource configs file into an object so we can reference it in the next cells\n",
    "from gigl.env.pipelines_config import get_resource_config\n",
    "from gigl.src.common.types.pb_wrappers.gbml_config import GbmlConfigPbWrapper\n",
    "from gigl.src.common.types.pb_wrappers.gigl_resource_config import GiglResourceConfigWrapper\n",
    "\n",
    "frozen_task_config = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(\n",
    "    gbml_config_uri=FROZEN_TASK_CONFIG_PATH\n",
    ")\n",
    "resource_config: GiglResourceConfigWrapper = get_resource_config(\n",
    "    resource_config_uri=RESOURCE_CONFIG_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Scala Jars\n",
    "\n",
    "GiGL uses Spark/Scala to run data processing pipelines for the [subgraph sampler](../../docs/user_guide/overview/components/subgraph_sampler.md) and [split generator](../../docs/user_guide/overview/components/split_generator.md) components.\n",
    "\n",
    "As such, we need to build and packages jars so that we can run the pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.scala_packager import ScalaPackager\n",
    "sgs_spark31, sgs_spark35, split_gen  = ScalaPackager().package_all()\n",
    "\n",
    "print(f\"Subgraph Sampler Spark 3.1 jar: {sgs_spark31.uri}\")\n",
    "print(f\"Subgraph Sampler Spark 3.5 jar: {sgs_spark35.uri}\")\n",
    "print(f\"Split Generator Spark 3.5 jar: {split_gen.uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling Source Docker Images\n",
    "\n",
    "You will need to build and push Docker images with your custom code so that individual GiGL components can leverage your code. For this experiment, we will consider the toy_visual_example specs and relevant code to be \"custom code,\" and we will guide you on how to build a Docker image with the code.\n",
    "\n",
    "We will make use of `scripts/build_and_push_docker_image.py` for this.\n",
    "\n",
    "Note that this builds `containers/Dockerfile.src` and `containers/Dockerfile.dataflow.src`, which have instructions to `COPY` the `examples` folder - which contains all the source code for this example, and it has all the GiGL source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# We suppress the output of this cell to avoid cluttering the notebook with logs. Remove %%%capture if you want to see the output.\n",
    "\n",
    "from scripts.build_and_push_docker_image import build_and_push_cpu_image, build_and_push_cuda_image, build_and_push_dataflow_image\n",
    "import datetime\n",
    "\n",
    "project = resource_config.project\n",
    "curr_datetime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Change to whereever you want to store the docker images.\n",
    "DOCKER_ARTIFACT_REGISTRY = f\"us-central1-docker.pkg.dev/{project}/gigl-base-images\"\n",
    "\n",
    "DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG = f\"{DOCKER_ARTIFACT_REGISTRY}/gigl_dataflow_runtime:{curr_datetime}\"\n",
    "DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG = f\"{DOCKER_ARTIFACT_REGISTRY}/gigl_cuda:{curr_datetime}\"\n",
    "DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG = f\"{DOCKER_ARTIFACT_REGISTRY}/gigl_cpu:{curr_datetime}\"\n",
    "os.environ[\"DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG\"] = DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG\n",
    "os.environ[\"DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG\"] = DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG\n",
    "os.environ[\"DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG\"] = DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG\n",
    "\n",
    "build_and_push_dataflow_image(\n",
    "    image_name=DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG,\n",
    ")\n",
    "build_and_push_cuda_image(\n",
    "    image_name=DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG,\n",
    ")\n",
    "build_and_push_cpu_image(\n",
    "    image_name=DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG,\n",
    ")\n",
    "\n",
    "print(f\"\"\"We built and pushed the following docker images:\n",
    "- {DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG}\n",
    "- {DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG}\n",
    "- {DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessor\n",
    "\n",
    "Once we have a `frozen_task_config`, the first step is to preprocess the data.\n",
    "\n",
    "The Data Preprocessor component uses [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started) to achieve data transformation in a distributed fashion. \n",
    "\n",
    "- Any custom preprocessing is to be defined in the preprocessor class, specified in the task config by `datasetConfig.dataPreprocessorConfig.dataPreprocessorConfigClsPath`.\n",
    "- This class must inherit from {py:class}`gigl.src.data_preprocessor.lib.data_preprocessor_config.DataPreprocessorConfig`.\n",
    "\n",
    "In your preprocessor spec, you must implement the following 3 functions as defined by the base class `DataPreprocessorConfig`:\n",
    "  - `prepare_for_pipeline`: Preparing datasets for ingestion and transformation.\n",
    "  - `get_nodes_preprocessing_spec`: Defining transformation imperatives for different node types.\n",
    "  - `get_edges_preprocessing_spec`: Defining transformation imperatives for different edge types.\n",
    "\n",
    "Please take a look at [toy_data_preprocessor_config.py](./toy_data_preprocessor_config.py) to see how these are defined. You will note that in this case, we are not doing anything special (i.e., no feature engineering), just reading from BQ and passing through the features. We could, if we wanted, define our own [preprocessing function](https://www.tensorflow.org/tfx/transform/get_started#preprocessing_function_example), and replace it with `build_passthrough_transform_preprocessing_fn()` defined in the code.\n",
    "\n",
    "### Input Parameters and Output Paths for Data Preprocessor\n",
    "Let's take a quick look at what these are from our frozen config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Frozen Config DataPreprocessor Information:\")\n",
    "\n",
    "print(\"- Data Preprocessor Config: Specifies what class to use for datapreprocessing and any arguments that might be passed in at runtime to that class\")\n",
    "print(textwrap.indent(str(frozen_task_config.dataset_config.data_preprocessor_config), '\\t'))\n",
    "print(\"- Preprocessed Metadata Uri: Specifies path to the preprocessed metadata file that will be generated by this component and used by subsequent components to understand and find the data that was preprocessed\")\n",
    "print(textwrap.indent(str(frozen_task_config.shared_config.preprocessed_metadata_uri), '\\t'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Data Preprocessor and visualizing the Preprocessed Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!python -m gigl.src.data_preprocessor.data_preprocessor \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$FROZEN_TASK_CONFIG_PATH \\\n",
    "--resource_config_uri=$RESOURCE_CONFIG_PATH \\\n",
    "--custom_worker_image_uri=$DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upon completion of job, we will see the preprocessed metadata be populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_task_config = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(\n",
    "    gbml_config_uri=FROZEN_TASK_CONFIG_PATH\n",
    ")\n",
    "preprocessed_metadata_pb = frozen_task_config.preprocessed_metadata_pb_wrapper.preprocessed_metadata_pb\n",
    "print(preprocessed_metadata_pb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not have to worry about these details in code as it is all handled by the data preprocessor component and subsequent data loaders\n",
    "But, for the sake of understanding, we will investigate the condensed_node_type = 0 and condensed_edge_type = 0\n",
    "If you remember the from the frozen config the mappings were as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Condensed Node Type Mapping:\")\n",
    "print(textwrap.indent(str(frozen_task_config.graph_metadata.condensed_node_type_map), '\\t'))\n",
    "print(\"Condensed Edge Type Mapping:\")\n",
    "print(textwrap.indent(str(frozen_task_config.graph_metadata.condensed_edge_type_map), '\\t'))\n",
    "\n",
    "preprocessed_nodes = preprocessed_metadata_pb.condensed_node_type_to_preprocessed_metadata[0].tfrecord_uri_prefix\n",
    "preprocessed_edges = preprocessed_metadata_pb.condensed_edge_type_to_preprocessed_metadata[0].main_edge_info.tfrecord_uri_prefix\n",
    "print(f\"Preprocessed Nodes are stored in: {preprocessed_nodes}\")\n",
    "print(f\"Preprocessed Edges are stored in: {preprocessed_edges}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not a lot of data so we will have likely just generated one file for each of the preprocessed nodes and edges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $preprocessed_nodes && gsutil ls $preprocessed_edges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraph Sampler\n",
    "\n",
    "The Subgraph Sampler receives node and edge data from the Data Preprocessor and generates k-hop localized subgraphs for each node in the graph. The purpose is to store the neighborhood of each node independently, thereby reducing the memory footprint for downstream components, as they need not load the entire graph into memory but only batches of these node neighborhoods. \n",
    "\n",
    "To run the subgraph sampler, we use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!python -m gigl.src.subgraph_sampler.subgraph_sampler \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$FROZEN_TASK_CONFIG_PATH \\\n",
    "--resource_config_uri=$RESOURCE_CONFIG_PATH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon completion, there will be two different directories of subgraph samples. One is the main node anchor-based link prediction samples, and the other is random negative rooted neighborhood samples, which are stored in the locations specified in the frozen_config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_graph_metadata = frozen_task_config.shared_config.flattened_graph_metadata\n",
    "print(flattened_graph_metadata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main k-hop node_anchor_based_link_prediction_samples include root nodes' neighborhoods, positive nodes' neighborhoods, and positive edges. These samples will be used for training.\n",
    "\n",
    "The k-hop random_negative_rooted_neighborhood_samples (which include root nodes' neighborhoods) serve a dual purpose: they will be used for the inferencer and as random negative samples for training.\n",
    "\n",
    "The random negatives are used for the model to learn non-existent (negative) edges since it could overfit on just positive samples. This means it would fail to generalize well to unseen data. The negative edges are just edges chosen at random. At a large scale, this would most probably be a negative edge. \n",
    "\n",
    "Below, we visualize the Root Node Neighborhood of 5, the Root Node Neighborhood of its pos_edge's destination node (1), and the resulting sample for root node 5. \n",
    "\n",
    "Since the subgraph sampler is sampling randomly here, you will get different subgraphs every time you run this.\n",
    "\n",
    "For the purposes of example, we also provide some screenshots of what these graphs might look like:\n",
    "\n",
    "When training, you may see a sample for node 9 as follows. Specifically, note that edge `9 --> 7` is classified as a positive edge, where SGS has sampled a 2-hop subgraph with incoming edges to both nodes 9 and 7.\n",
    "\n",
    "<img src=\"./assets/link_pred_sample_node_9.png\" alt=\"Link Pred Sample\" width=\"50%\" />\n",
    "\n",
    "Secondly, we may choose to randomly sample a rooted node neighborhood to act as a \"negative sample\" (i.e., in this case, we sample node 1 and can have `9 --> 1` edge be a \"negative sample\").\n",
    "\n",
    "<img src=\"./assets/rooted_neighborhood_node_1.png\" alt=\"Random Negative Sample\" width=\"50%\" />\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snapchat.research.gbml import training_samples_schema_pb2\n",
    "\n",
    "# We will sample node with this id and visualize it. You will see positive edge marked in red and the root node with a black border.\n",
    "SAMPLE_NODE_ID = 9\n",
    "# We will sample random negative nodes with these ids and visualize them. You will see the root node with a black border.\n",
    "SAMPLE_RANDOM_NEGATIVE_NODE_IDS = [1, 3]\n",
    "\n",
    "\n",
    "from gigl.common.utils.jupyter_magics import GraphVisualizer, find_node_pb\n",
    "\n",
    "\n",
    "print(f\"The original global graph:\")\n",
    "GraphVisualizer.visualize_graph(original_graph_heterodata)\n",
    "\n",
    "\n",
    "sample = find_node_pb(\n",
    "    tfrecord_uri_prefix=flattened_graph_metadata.node_anchor_based_link_prediction_output.tfrecord_uri_prefix,\n",
    "    node_id=SAMPLE_NODE_ID,\n",
    "    pb_type=training_samples_schema_pb2.NodeAnchorBasedLinkPredictionSample\n",
    ")\n",
    "print(f\"Node anchor prediction sample for node {SAMPLE_NODE_ID}:\")\n",
    "GraphVisualizer.plot_graph(sample)\n",
    "for random_negative_node_id in SAMPLE_RANDOM_NEGATIVE_NODE_IDS:\n",
    "    random_negative_sample = find_node_pb(\n",
    "        tfrecord_uri_prefix=flattened_graph_metadata.node_anchor_based_link_prediction_output.node_type_to_random_negative_tfrecord_uri_prefix[\"user\"],\n",
    "        node_id=random_negative_node_id,\n",
    "        pb_type=training_samples_schema_pb2.RootedNodeNeighborhood\n",
    "    )\n",
    "    print(f\"Random negative sample for node {random_negative_node_id}:\")\n",
    "    GraphVisualizer.plot_graph(random_negative_sample)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Generator\n",
    "\n",
    "The Split Generator reads localized subgraph samples produced by the Subgraph Sampler and executes the user-specified split strategy logic to split the data into training, validation, and test sets. Several standard configurations of SplitStrategy and corresponding Assigner classes are already implemented at a GiGL platform level: transductive node classification, inductive node classification, and transductive link prediction split routines. For more information on split strategies in Graph Machine Learning, check out these resources:\n",
    "\n",
    "1. [CS224W Lecture Slides](http://web.stanford.edu/class/cs224w/slides/07-theory.pdf)\n",
    "2. [Graph Link Prediction](https://zqfang.github.io/2021-08-12-graph-linkpredict/) (relevant for explaining transductive vs inductive).\n",
    "\n",
    "In this example, we are using the transductive strategy as specified in our frozen_config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frozen_task_config.dataset_config.split_generator_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For transductive, at training time, it uses training message edges to predict training supervision edges. At validation time, the training message edges and training supervision edges are used to predict the validation edges and then all 3 are used to predict test edges. Below is the command to run split generator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!python -m gigl.src.split_generator.split_generator \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$FROZEN_TASK_CONFIG_PATH \\\n",
    "--resource_config_uri=$RESOURCE_CONFIG_PATH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon completion, there will be 3 folders for train,test, and val. Each of them contains the protos for the positive and negaitve samples. The path for these folders is specified in the following location in the frozen_config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metadata = frozen_task_config.shared_config.dataset_metadata\n",
    "print(dataset_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can visualize the train,test, and val samples for all nodes.\n",
    "Note, for this specific task config setup, not all val amd test samples will have positive labels. This is because edges are randomly assigned into \"train\", \"val\", or \"test\" buckets independent of whether or not they are supervision edges. Thus, although at scale this setting is okay i.e. with large data and a large batch size each batch will have some supervision edges, it can be a case that certain batches dont have any supervision edges. Thus, your val/test loops and early stopping logic may need to be carefully designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node_id in range(original_graph_heterodata.num_nodes):\n",
    "    print(f\"Node anchor prediction sample for node {node_id}:\")\n",
    "    sample_train = find_node_pb(\n",
    "        tfrecord_uri_prefix=dataset_metadata.node_anchor_based_link_prediction_dataset.train_main_data_uri,\n",
    "        node_id=node_id,\n",
    "        pb_type=training_samples_schema_pb2.NodeAnchorBasedLinkPredictionSample\n",
    "    )\n",
    "    sample_val = find_node_pb(\n",
    "        tfrecord_uri_prefix=dataset_metadata.node_anchor_based_link_prediction_dataset.val_main_data_uri,\n",
    "        node_id=node_id,\n",
    "        pb_type=training_samples_schema_pb2.NodeAnchorBasedLinkPredictionSample\n",
    "    )\n",
    "    sample_test = find_node_pb(\n",
    "        tfrecord_uri_prefix=dataset_metadata.node_anchor_based_link_prediction_dataset.test_main_data_uri,\n",
    "        node_id=node_id,\n",
    "        pb_type=training_samples_schema_pb2.NodeAnchorBasedLinkPredictionSample\n",
    "    )\n",
    "    if sample_train:\n",
    "        print(f\"Train sample for node {node_id}: \")\n",
    "        GraphVisualizer.plot_graph(sample_train)\n",
    "    else:\n",
    "        print(f\"No train sample found for node {node_id}.\")\n",
    "    if sample_val:\n",
    "        print(f\"Validation sample for node {node_id}:\")\n",
    "        GraphVisualizer.plot_graph(sample_val)\n",
    "    else:\n",
    "        print(f\"No validation sample found for node {node_id}.\")\n",
    "    if sample_test:\n",
    "        print(f\"Test sample for node {node_id}:\")\n",
    "        GraphVisualizer.plot_graph(sample_test)\n",
    "    else:\n",
    "        print(f\"No test sample found for node {node_id}.\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have our graph data samples ready to be processed by the trainer and inferencer components. These components will extract representations/embeddings by learning contextual information for the specified task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Inference\n",
    "\n",
    "Tabularized GiGL uses a config-based approach to training and inference. They use the [`GbmlConfig.TrainerConfig`](https://github.com/Snapchat/GiGL/blob/97fdb6e5d41f567900d729e9bb894a8f727dbd0c/proto/snapchat/research/gbml/gbml_config.proto#L176-L191) and [`GbmlConfig.InferencerConfig`](https://github.com/Snapchat/GiGL/blob/97fdb6e5d41f567900d729e9bb894a8f727dbd0c/proto/snapchat/research/gbml/gbml_config.proto#L193-L208) Proto messages.\n",
    "\n",
    "In this case, they both use the [`NodeAnchorBasedLinkPredictionModelingTaskSpec`](https://github.com/Snapchat/GiGL/blob/97fdb6e5d41f567900d729e9bb894a8f727dbd0c/python/gigl/src/common/modeling_task_specs/node_anchor_based_link_prediction_modeling_task_spec.py#L66)\n",
    "\n",
    "You can run the Trainer and Inferencer the same as the other components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy over resource config to GCS so that training and inference can access it.\n",
    "# If we are already using a GCS URI, we will just use it as is.\n",
    "from gigl.common import GcsUri, LocalUri\n",
    "from gigl.src.common.utils.file_loader import FileLoader\n",
    "\n",
    "if isinstance(RESOURCE_CONFIG_PATH, LocalUri):\n",
    "    GCS_RESOURCE_CONFIG_PATH = GcsUri.join(resource_config.temp_assets_regional_bucket_path, JOB_NAME, \"resource_config.yaml\")\n",
    "    loader = FileLoader()\n",
    "    loader.load_file(\n",
    "        file_uri_src=RESOURCE_CONFIG_PATH,\n",
    "        file_uri_dst=GCS_RESOURCE_CONFIG_PATH\n",
    "    )\n",
    "else:\n",
    "    GCS_RESOURCE_CONFIG_PATH = RESOURCE_CONFIG_PATH\n",
    "print(f\"GCS_RESOURCE_CONFIG_PATH: {GCS_RESOURCE_CONFIG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"JOB_NAME: $JOB_NAME\"\n",
    "echo \"FROZEN_TASK_CONFIG_PATH: $FROZEN_TASK_CONFIG_PATH\"\n",
    "echo \"GCS_RESOURCE_CONFIG_PATH: $GCS_RESOURCE_CONFIG_PATH\"\n",
    "echo \"DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG: $DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG\"\n",
    "echo \"DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG: $DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG\"\n",
    "echo \"DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG: $DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG\"\n",
    "\n",
    "python -m gigl.src.training.trainer \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$FROZEN_TASK_CONFIG_PATH \\\n",
    "--resource_config_uri=$GCS_RESOURCE_CONFIG_PATH \\\n",
    "--cpu_docker_uri=$DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG \\\n",
    "--cuda_docker_uri=$DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model gets saved to the path specified in the frozen task config.\n",
    "print(f\"Trained model saved to: {frozen_task_config.shared_config.trained_model_metadata.trained_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"JOB_NAME: $JOB_NAME\"\n",
    "echo \"FROZEN_TASK_CONFIG_PATH: $FROZEN_TASK_CONFIG_PATH\"\n",
    "echo \"GCS_RESOURCE_CONFIG_PATH: $GCS_RESOURCE_CONFIG_PATH\"\n",
    "echo \"DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG: $DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG\"\n",
    "echo \"DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG: $DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG\"\n",
    "echo \"DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG: $DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG\"\n",
    "python -m gigl.src.inference.inferencer \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$FROZEN_TASK_CONFIG_PATH \\\n",
    "--resource_config_uri=$GCS_RESOURCE_CONFIG_PATH \\\n",
    "--cpu_docker_uri=$DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG \\\n",
    "--cuda_docker_uri=$DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG \\\n",
    "--custom_worker_image_uri=$DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at inference results\n",
    "\n",
    "from gigl.src.common.utils.bq import BqUtils\n",
    "\n",
    "bq_emb_out_table = frozen_task_config.shared_config.inference_metadata.node_type_to_inferencer_output_info_map[frozen_task_config.graph_metadata_pb_wrapper.homogeneous_node_type].embeddings_path\n",
    "print(f\"Embeddings should be successfully stored in the following location: {bq_emb_out_table}\")\n",
    "\n",
    "bq_utils = BqUtils(project=resource_config.project)\n",
    "query = f\"SELECT * FROM {bq_emb_out_table} LIMIT 5\"\n",
    "result = list(bq_utils.run_query(query=query, labels={}))\n",
    "\n",
    "print(f\"Query result: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bagl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

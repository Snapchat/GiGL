{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Example \n",
    "\n",
    "This notebook is a walkthrough off training and inferencing on a small toy graph using GiGL\n",
    "\n",
    "\n",
    "## Overview Of Components\n",
    "This notebook shows the process of a simple, human-digestable graph being passed through all the pipeline components in GiGL in preperation for training to help understand how each of the components work.\n",
    "\n",
    "The pipeline consists of the following components:\n",
    "\n",
    "- **Config Populator**: Takes a template config and creates a frozen workflow config that dictates all inputs/outputs and business parameters that is read and used by each subsequent component.\n",
    "    - input: template_config.yaml\n",
    "    - output: frozen_gbml_config.yaml\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- **Data Preprocesser**: Transforms necessary node and edge feature assets as needed as a precursor step in most ML tasks according to user provided data preprocessor config class\n",
    "    - input: frozen_gbml_config.yaml which includes user-defined preprocessor class for custom logic and custom arguments can be passed under dataPreprocessorArgs\n",
    "    - output: PreprocessedMetadata Proto which includes inferred GraphMetadata and preproccessed graph data Tfrecords after applying user defined preprocessing function\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- **Subgraph Sampler**: Samples k-hop subgraphs for each node according to user provided arguments\n",
    "    - input: frozen_gbml_config.yaml, resource_config.yaml\n",
    "    - output: Subgraph Samples (tfrecord format based on predefined schema in protos) are stored in the uri defined in flattenedGraphMetadata field. \n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- **Split Generator**: Splits subgraph sampler outputs into train/test/val sets according to user provided split strategy class.\n",
    "    - input: frozen_gbml_config.yaml which includes instance of SplitStrategy and an instance of Assigner\n",
    "    - output: TFRecord samples\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- **Trainer**: The trainer component reads the output of split generator and trains a model on the training set, stops based on validation set, and evaluates on the test set\n",
    "    - input: frozen_gbml_config.yaml\n",
    "    - output: state_dict stored in trainedModelUri\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- **Inferencer**: Runs inference of a trained model on samples generated by Subgraph Sampler. \n",
    "    - input: frozen_gbml_config.yaml\n",
    "    - output: Embeddings and/or prediction assets\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Graph\n",
    "\n",
    "We use the input graph defined in [examples/toy_visual_example/graph_config.yaml](./graph_config.yaml). \n",
    "You are welcome to change this file to a custom graph off your own choosing.\n",
    "\n",
    "\n",
    "### Visualizing the input graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "from visualize import GraphVisualizer\n",
    "\n",
    "from gigl.src.mocking.toy_asset_mocker import load_toy_graph\n",
    "\n",
    "data: HeteroData = load_toy_graph(graph_config=\"./graph_config.yaml\")\n",
    "# Visualize the graph\n",
    "GraphVisualizer.visualize_graph(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Configs\n",
    "\n",
    "The first thing we will need to do is create the resource and task configs. \n",
    "\n",
    "- **Task Config**: Specifies task-related configurations, guiding the behavior of components according to the needs of\n",
    "  your machine learning task. See [Task Config Guide](../../docs/user_guide/config_guides/task_config_guide.md). For this task, we have already provided a task config: [task_config.yaml](./task_config.yaml)\n",
    "\n",
    "- **Resource Config**: Details the resource allocation and environmental settings across all GiGL components. This\n",
    "  encompasses shared resources for all components, as well as component-specific settings. See [Resource Config Guide](../../docs/user_guide/config_guides/resource_config_guide.md). For this task we provide a resource [resource_config.yaml](./resource_config.yaml). Although, the provided default values in `shared_resource_config.common_compute_config` are for resources you will not have access to unless you are a core contributor.\n",
    "\n",
    "  - **Intructions to configure the resource config to work**:\n",
    "    If you have not already, please follow the [Quick Start Guide](../../docs/user_guide/getting_started/quick_start.md) to setup your cloud environment and setup a default test resource config. You can then copy the relevant `shared_resource_config.common_compute_config` to [resource_config.yaml](./resource_config.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import sys\n",
    "from gigl.common import Uri, UriFactory\n",
    "notebook_dir = os.getcwd()\n",
    "# Add the root gigl dir to the Python path so `example` folder can be imported as a module.\n",
    "sys.path.append(os.path.join(notebook_dir, \"..\", \"..\"))\n",
    "\n",
    "# You are welcome to customize these  to point to your own configuration files.\n",
    "JOB_NAME = \"gigl_test_job\"\n",
    "TEMPLATE_TASK_CONFIG_PATH: Uri = UriFactory.create_uri(f\"{notebook_dir}/template_task_config.yaml\")\n",
    "FROZEN_TASK_CONFIG_POINTER_FILE_PATH: Uri = UriFactory.create_uri(f\"/tmp/GiGL/{JOB_NAME}/frozen_task_config.yaml\")\n",
    "pathlib.Path(FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri).parent.mkdir(parents=True, exist_ok=True)\n",
    "# Ensure you change the resource config path to point to your own resource configuration\n",
    "# i.e. what was exported to $GIGL_TEST_DEFAULT_RESOURCE_CONFIG as part of the quick start guide.\n",
    "RESOURCE_CONFIG_PATH: Uri = UriFactory.create_uri(f\"{notebook_dir}/resource_config.yaml\")\n",
    "\n",
    "# Export variables so we can reference them in cells that execute bash commands below.\n",
    "os.environ[\"JOB_NAME\"] = JOB_NAME\n",
    "os.environ[\"TEMPLATE_TASK_CONFIG_PATH\"] = TEMPLATE_TASK_CONFIG_PATH.uri\n",
    "os.environ[\"FROZEN_TASK_CONFIG_POINTER_FILE_PATH\"] = FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri\n",
    "os.environ[\"RESOURCE_CONFIG_PATH\"] = RESOURCE_CONFIG_PATH.uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on use of mocked assets\n",
    "\n",
    "This step is already done for you. We provide instructions below for posterity, incase the mocked data input [\"graph_config.yaml\"](./graph_config.yaml) is updated.\n",
    "\n",
    "Note: unless you are a core contributor you will not have access to write to public BQ/GCS resources. In this case, can chose to update `MOCK_DATA_GCS_BUCKET` and `MOCK_DATA_BQ_DATASET_NAME` in `python/gigl/src/mocking/lib/constants.py` to upload to your resources you own.\n",
    "\n",
    "We run the following command to upload the relevant mocks to GCS and BQ:\n",
    "```bash\n",
    "python -m gigl.src.mocking.dataset_asset_mocking_suite \\\n",
    "--select mock_toy_graph_homogeneous_node_anchor_based_link_prediction_dataset \\\n",
    "--resource_config_uri=examples/toy_visual_example/resource_config.yaml\n",
    "```\n",
    "\n",
    "Subsequently, we can update the paths in [task_config.yaml](./task_config.yaml)\n",
    "\n",
    "## Validating the configs\n",
    "\n",
    "We provide the ability to validate your resource and task configs. Although the validation is not exhaustive, it does help assert that the more common issues are not present schedule expensive compute is scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.src.validation_check.config_validator import kfp_validation_checks\n",
    "validator = kfp_validation_checks(\n",
    "    job_name=JOB_NAME,\n",
    "    task_config_uri=TEMPLATE_TASK_CONFIG_PATH,\n",
    "    resource_config_uri=RESOURCE_CONFIG_PATH,\n",
    "    start_at=\"config_populator\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Populator\n",
    "\n",
    "Takes in a template `GbmlConfig` and outputs a frozen `GbmlConfig` by populating all job related metadata paths in\n",
    "`sharedConfig`. These are mostly GCS paths which the following components read and write from, and use as an\n",
    "intermediary data communication medium. For example, the field `sharedConfig.trainedModelMetadata` is populated with a\n",
    "GCS URI, which indicates to the Trainer to write the trained model to this path, and to the Inferencer to read the model\n",
    "from this path. See full [Config Populator Guide](../../docs/user_guide/overview/components/config_populator.md)\n",
    "\n",
    "After running the command below we will have created a frozen config and ploaded it to the the `perm_assets_bucket` provided in the `resource config`.\n",
    "The path to that file will be stored in the file @ `FROZEN_TASK_CONFIG_POINTER_FILE_PATH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python -m \\\n",
    "    gigl.src.config_populator.config_populator \\\n",
    "    --job_name=\"$JOB_NAME\" \\\n",
    "    --template_uri=\"$TEMPLATE_TASK_CONFIG_PATH\" \\\n",
    "    --resource_config_uri=\"$RESOURCE_CONFIG_PATH\" \\\n",
    "    --output_file_path_frozen_gbml_config_uri=\"$FROZEN_TASK_CONFIG_POINTER_FILE_PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the frozen task config file path\n",
    "FROZEN_TASK_CONFIG_PATH: Uri\n",
    "with open(FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri, 'r') as file:\n",
    "    FROZEN_TASK_CONFIG_PATH = UriFactory.create_uri(file.read().strip())\n",
    "os.environ[\"FROZEN_TASK_CONFIG_PATH\"] = FROZEN_TASK_CONFIG_PATH.uri\n",
    "print(f\"FROZEN_TASK_CONFIG_PATH: {FROZEN_TASK_CONFIG_PATH}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the diff between template and frozen config.\n",
    "\n",
    "As pointed above, running the config populatortes ob related metadata paths to the template config. Lets see what those paths are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from difflib import unified_diff\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from gigl.src.common.utils.file_loader import FileLoader\n",
    "\n",
    "def sort_yaml_dict_recursively(obj: dict) -> dict:\n",
    "    # We sort the yaml recursively as the GiGL proto serialization code does not guarantee order of original keys.\n",
    "    # This is important for the diff to be stable and not show errors due to key/list order changes.\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: sort_yaml_dict_recursively(obj[k]) for k in sorted(obj)}\n",
    "    elif isinstance(obj, list):\n",
    "        return [sort_yaml_dict_recursively(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def show_colored_unified_diff(f1_lines, f2_lines, f1_name, f2_name):\n",
    "    diff_lines = list(unified_diff(f1_lines, f2_lines, fromfile=f1_name, tofile=f2_name))\n",
    "    html_lines = []\n",
    "    for line in diff_lines:\n",
    "        if line.startswith('+') and not line.startswith('+++'):\n",
    "            color = '#228B22'  # green\n",
    "        elif line.startswith('-') and not line.startswith('---'):\n",
    "            color = '#B22222'  # red\n",
    "        elif line.startswith('@'):\n",
    "            color = '#1E90FF'  # blue\n",
    "        else:\n",
    "            color = \"#000000\"  # black\n",
    "        html_lines.append(f'<pre style=\"margin:0; color:{color}; background-color:white;\">{line.rstrip()}</pre>')\n",
    "    display(HTML(''.join(html_lines)))\n",
    "\n",
    "\n",
    "file_loader = FileLoader()\n",
    "frozen_task_config_file_contents: str\n",
    "template_task_config_file_contents: str\n",
    "\n",
    "with open(file_loader.load_to_temp_file(file_uri_src=FROZEN_TASK_CONFIG_PATH).name, 'r') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "    # sort_keys by default \n",
    "    frozen_task_config_file_contents = yaml.dump(sort_yaml_dict_recursively(data))\n",
    "\n",
    "with open(file_loader.load_to_temp_file(file_uri_src=TEMPLATE_TASK_CONFIG_PATH).name, 'r') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "    template_task_config_file_contents = yaml.dump(sort_yaml_dict_recursively(data))\n",
    "\n",
    "# Example usage\n",
    "show_colored_unified_diff(\n",
    "    template_task_config_file_contents.splitlines(),\n",
    "    frozen_task_config_file_contents.splitlines(),\n",
    "    f1_name='template_task_config.yaml',\n",
    "    f2_name='frozen_task_config.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "from IPython.display import display, HTML\n",
    "from gigl.src.common.utils.file_loader import FileLoader\n",
    "import yaml\n",
    "\n",
    "\n",
    "def sort_dict_recursively(obj: dict) -> dict:\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: sort_dict_recursively(obj[k]) for k in sorted(obj)}\n",
    "    elif isinstance(obj, list):\n",
    "        return [sort_dict_recursively(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def show_colored_unified_diff(f1_lines, f2_lines):\n",
    "    print(f2_lines)\n",
    "    html_lines = []\n",
    "    for f2_line in f2_lines:\n",
    "        if f2_line not in f1_lines:\n",
    "            color = '#228B22'  # green - addition\n",
    "            prefix = '+ '\n",
    "            html_lines.append(f'<pre style=\"margin:0; color:{color}\">{prefix}{f2_line.rstrip()}</pre>')\n",
    "        else:\n",
    "            color = \"#FF06C9\"\n",
    "            prefix = '  '\n",
    "            html_lines.append(f'<pre style=\"margin:0; color:{color}\">{prefix}{f2_line.rstrip()}</pre>')\n",
    "\n",
    "    display(HTML(''.join(html_lines)))\n",
    "\n",
    "file_loader = FileLoader()\n",
    "frozen_ask_config_f = file_loader.load_to_temp_file(\n",
    "    file_uri_src=FROZEN_TASK_CONFIG_PATH,\n",
    ")\n",
    "template_task_config_f = file_loader.load_to_temp_file(\n",
    "    file_uri_src=TEMPLATE_TASK_CONFIG_PATH,\n",
    ")\n",
    "\n",
    "frozen_task_config_file_contents: str\n",
    "template_task_config_file_contents: str\n",
    "\n",
    "with open(frozen_ask_config_f.name, 'r') as f:\n",
    "    data = yaml.safe_load(frozen_ask_config_f.name)\n",
    "    frozen_task_config_file_contents = yaml.dump(sort_dict_recursively(data), sort_keys=False)\n",
    "\n",
    "with open(template_task_config_f.name, 'r') as f:\n",
    "    data = yaml.safe_load(template_task_config_f.name)\n",
    "    template_task_config_file_contents = yaml.dump(sort_dict_recursively(data), sort_keys=False)\n",
    "\n",
    "print(template_task_config_file_contents)\n",
    "\n",
    "# Example usage\n",
    "show_colored_unified_diff(template_task_config_file_contents.splitlines(), frozen_task_config_file_contents.splitlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEMPLATE_TASK_CONFIG_PATH)\n",
    "# template_task_config_file_contents = file_loader.load_to_temp_file(\n",
    "#     file_uri_src=TEMPLATE_TASK_CONFIG_PATH,\n",
    "# )\n",
    "# with open(template_task_config_file_contents.name, 'r') as f:\n",
    "#     template_task_config_file_contents = f.read()\n",
    "#     print(template_task_config_file_contents)\n",
    "\n",
    "\n",
    "frozen_task_config_file_contents = file_loader.load_to_temp_file(\n",
    "    file_uri_src=FROZEN_TASK_CONFIG_PATH,\n",
    ")\n",
    "\n",
    "with open(frozen_task_config_file_contents.name, 'r') as f:\n",
    "    frozen_task_config_file_contents = f.read()\n",
    "    print(frozen_task_config_file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import unified_diff\n",
    "\n",
    "diff = unified_diff(a.splitlines(), b.splitlines(), lineterm='')\n",
    "print('\\n'.join(list(diff)))\n",
    "\n",
    "\n",
    "import yaml\n",
    "\n",
    "yaml_file = \"./frozen_gbml_config.yaml\"\n",
    "\n",
    "def visualize_yaml(file_path):\n",
    "    with open(file_path, 'r') as yaml_file:\n",
    "        yaml_data = yaml.safe_load(yaml_file)\n",
    "\n",
    "    print(yaml.dump(yaml_data))\n",
    "\n",
    "print(\"Frozen GBML Config Yaml:\")\n",
    "visualize_yaml(yaml_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a code snippet to output the difference between the original config file and the frozen config to illustrate what ConfigPopulator adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def compare_yaml(file_path1, file_path2):\n",
    "    with open(file_path1, 'r') as yaml_file1:\n",
    "        yaml_data1 = yaml.safe_load(yaml_file1)\n",
    "\n",
    "    with open(file_path2, 'r') as yaml_file2:\n",
    "        yaml_data2 = yaml.safe_load(yaml_file2)\n",
    "\n",
    "    diff = compare_dicts(yaml_data1, yaml_data2)\n",
    "    print(yaml.dump(diff, default_flow_style=False))\n",
    "\n",
    "def compare_dicts(dict1, dict2):\n",
    "    diff = {}\n",
    "    for key in set(dict2.keys()):\n",
    "        if key not in dict1:\n",
    "            diff[key] = dict2[key]\n",
    "    return diff\n",
    "\n",
    "gbml_config = \"toy_graph/configs/gbml_toy_config.yaml\"\n",
    "frozen_config_local = \"./frozen_gbml_config.yaml\"\n",
    "compare_yaml(gbml_config, frozen_config_local)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Input Graph\n",
    "\n",
    "We have now configured everything required to run the subsequent steps. Before proceeding we can visualize what the input graph looks like to get a better understanding of what happens in each of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import GraphVisualizer\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "graph_visualizer = GraphVisualizer(\"./graph_config.yaml\")\n",
    "graph_visualizer.visualize_graph()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessor\n",
    "\n",
    "The Data Preprocessor uses Tensorflow Transform to achieve data transformation in a distributed fashion. Any custom preprocessing is to be defined in the preprocessor class that is inherited from the DataPreprocessorConfig class.\n",
    "\n",
    "Overall, this class houses all logic for\n",
    "\n",
    "- Preparing datasets for ingestion and transformation (see [`prepare_for_pipeline`](https://github.com/Snapchat/GiGL/blob/10f1a35196f3946ae14c3e8e57d1cb685f01ffb5/python/gigl/src/data_preprocessor/lib/data_preprocessor_config.py#L40) function) So this is where you would house logic to pull data from a custom data source or perform any specific transformations.\n",
    "- Defining transformation imperatives for different node types (see [`get_nodes_preprocessing_spec`](https://github.com/Snapchat/GiGL/blob/10f1a35196f3946ae14c3e8e57d1cb685f01ffb5/python/gigl/src/data_preprocessor/lib/data_preprocessor_config.py#L54) function)\n",
    "- Defining transformation imperatives for different edge types (see [`get_edges_preprocessing_spec`](https://github.com/Snapchat/GiGL/blob/10f1a35196f3946ae14c3e8e57d1cb685f01ffb5/python/gigl/src/data_preprocessor/lib/data_preprocessor_config.py#L60))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon completion, the data preprocesser writes out a PreprocessedMetadata proto as TFRecords to URI specified by the preprocessedMetadataUri field in the sharedConfig section of the frozen config as seen below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frozen Config Datapreprocessor Information:\")\n",
    "\n",
    "print(\"Preprocessed Metadata Uri: \", frozen_config.shared_config.preprocessed_metadata_uri)\n",
    "print(\"Data Preprocessor Config: \", frozen_config.dataset_config.data_preprocessor_config)\n",
    "print(\"Flattened Graph Metadata: \", frozen_config.shared_config.flattened_graph_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This proto contains a map to all information about the graph. Nothing has changed in terms of the structure of the graph from the input graph. Only features or transformations (i.e normalization) are applied to the graph.\n",
    "\n",
    "To run the preprocessor we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$frozen_config_uri\"\n",
    "\n",
    "python -m \\\n",
    "    gigl.src.data_preprocessor.data_preprocessor \\\n",
    "    --job_name toy_graph \\\n",
    "    --task_config_uri \"$1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon completion we would see three generated artifacts in the specified gs uri: `edge`, `node`, and `preprocessed_metadata.yaml`. The metadata contains all the inferred GraphMetadata of the graph. One unformatted sample from a tfrecord and the data that is actually stored is visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize_preprocessor_output import visualize_preprocessed_graph\n",
    "\n",
    "preprocessed_metadata_uri = frozen_config.shared_config.preprocessed_metadata_uri\n",
    "node_df, edge_df = visualize_preprocessed_graph(preprocessed_metadata_uri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above visualized one sample of each of the TFRecord's that Data preprocessor creates. To visualize this output better, we can iterate through these TFRecord's and store them in two dataframes (node_df and edge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraph Sampler\n",
    "\n",
    "The Subgraph Sampler receives node and edge data from Data Preprocessor and generates k-hop localized subgraphs for each node in the graph. The purpose is to store the neighborhood of each node independently, and as a result reducing the memory footprint for down-stream components, as they need not load the entire graph into memory but only batches of these node neighborhoods. \n",
    "To run subgraph sampler we use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd ~/GiGL\n",
    "\n",
    "make compile_jars\n",
    "\n",
    "python -m gigl.src.subgraph_sampler.subgraph_sampler \\\n",
    "  --job_name=\"toy_graph\" \\\n",
    "  --task_config_uri=\"gs://TEMP DEV GBML PLACEHOLDER/toy_graph/config_populator/frozen_gbml_config.yaml\" \\\n",
    "  --resource_config_uri=\"deployment/configs/e2e_cicd_resource_config.yaml\" \\\n",
    "  --main_jar_file_uri=\"$PACKAGE_GCS_PATH\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon completion, there will be two different directories of subgraph samples. One is the main node anchor based link prediction samples and the other is random negative rooted neigborhood samples which are stored in the locations specified in the frozen_config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frozen_config.shared_config.flattened_graph_metadata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The main, unsupervised_node_anchor_based_link_prediction_samples include root nodes khop neighborhood, positive nodes khop neighborhood and positive edges. These samples will be used for training.\n",
    "The random_negative_rooted_neighborhood_samples (which include root nodes khop neighborhood)samples are double purpose: they will be used for inferencer and random negative samples for training.\n",
    "\n",
    "The random negative are used for the model to be able to learn non-existent (negative) edges since it could overfit on just positive samples. This means it would fail to generalize well to unseen data. The negative edges are just an edge chosen at random. At a large scale, this would most probably be a negative edge. \n",
    "\n",
    "Below we visualize the Root Node Neighbourhood of 5, the Root Node Neighbourhood of its pos_edge's destination node (1) and the resulting sample for root node 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize_sgs_output import SGSVisualizer\n",
    "\n",
    "sgs_visualizer = SGSVisualizer(frozen_config_uri)\n",
    "sgs_visualizer.visualize_random_negative_sample(5)\n",
    "sgs_visualizer.visualize_random_negative_sample(1)\n",
    "sgs_visualizer.visualize_node_anchor_prediction_sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Generator\n",
    "\n",
    "The Split Generator reads localized subgraph samples produced by Subgraph Sampler, and executes the user specified split strategy logic to split the data into training, validation and test sets. Several standard configurations of SplitStrategy and corresponding Assigner classes are implemented already at a GiGL platform-level: transductive node classification, inductive node classification, and transductive link prediction split routines. For more information on split strategies in Graph Machine Learning checkout these resources:\n",
    "\n",
    "1. http://web.stanford.edu/class/cs224w/slides/07-theory.pdf\n",
    "2. https://zqfang.github.io/2021-08-12-graph-linkpredict/ (relevant for explaining transductive vs inductive) \n",
    "\n",
    "In this example, we are using the transductive strategy as specified in our frozen_config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frozen_config.dataset_config.split_generator_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For transductive, at training time, it uses training message edges to predict training supervision edges. At validation time, the training message edges and training supervision edges are used to predict the validation edges and then all 3 are used to predict test edges. Below is the command to run split generator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python -m \\\n",
    "    gigl.src.split_generator.split_generator \\\n",
    "    --job_name toy_graph \\\n",
    "    --task_config_uri gs://TEMP DEV GBML PLACEHOLDER/toy_graph/config_populator/frozen_gbml_config.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon completion, there will be 3 folders for train,test, and val. Each of them contains the protos for the positive and negaitve samples. The path for these folders is specified in the following location in the frozen_config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frozen_config.shared_config.dataset_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can visualize the train,test, and val sample for the same root node as above (5) to see the pipeline process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize_sgn_output import SGNVisualizer\n",
    "\n",
    "sgn_vis = SGNVisualizer(frozen_config_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgn_vis.visualize_main_data_output(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have our graph data samples ready to be processed by the trainer and inferencer components. These components will extract representations/embeddings by learning contextual information for the specified task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

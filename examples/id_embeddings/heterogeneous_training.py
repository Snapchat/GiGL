"""
This file contains an example for how to run heterogeneous ID embedding training using live subgraph sampling powered by GraphLearn-for-PyTorch (GLT).
While `run_example_training` is coupled with GiGL orchestration, the `_training_process` and `testing_process` functions are generic
and can be used as references for writing training for pipelines not dependent on GiGL orchestration.

To run this file with GiGL orchestration, set the fields similar to below:

trainerConfig:
  trainerArgs:
    log_every_n_batch: "50"
    ssl_positive_label_percentage: "0.05"
  command: python -m examples.id_embeddings.heterogeneous_training
featureFlags:
  should_run_glt_backend: 'True'

Given a frozen task config with some already populated data preprocessor output, the following training script can be run locally using:
WORLD_SIZE=1 RANK=0 MASTER_ADDR="localhost" MASTER_PORT=20000 python -m examples.id_embeddings.heterogeneous_training --task_config_uri=<frozen_task_config_uri>

A frozen task config with data preprocessor outputs can be generated by running an e2e pipeline with `stop_after=data_preprocessor` and using the
frozen config generated from the `config_populator` component after the run has completed.
"""

from __future__ import annotations

import os

# Suppress TensorFlow logs
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # isort: skip

import argparse
import statistics
import time
from collections.abc import Iterator, Mapping
from dataclasses import dataclass
from typing import Any, Literal, Optional, Sequence, Union
from torch_geometric.data import Data, HeteroData

import torch
import torch.distributed
import torch.multiprocessing as mp
from torch import nn
from torch.distributed.optim import _apply_optimizer_in_backward as apply_optimizer_in_backward
from torch.optim import AdamW
from torchrec.distributed.model_parallel import DistributedModelParallel as DMP
from torchrec.distributed.embedding_types import EmbeddingComputeKernel
from torchrec.optim.keyed import CombinedOptimizer, KeyedOptimizerWrapper
from torchrec.optim.optimizers import in_backward_optimizer_filter
from torchrec.optim.rowwise_adagrad import RowWiseAdagrad

import gigl.distributed.utils
from gigl.common import Uri, UriFactory
from gigl.common.logger import Logger
from gigl.common.utils.torch_training import is_distributed_available_and_initialized
from gigl.distributed import (
    DistABLPLoader,
    DistDataset,
    build_dataset_from_task_config_uri,
)
from gigl.distributed.distributed_neighborloader import DistNeighborLoader
from gigl.distributed.utils import get_available_device
from gigl.nn.models import LightGCN
from gigl.src.common.types.graph_data import EdgeType, NodeType, Relation
from gigl.src.common.types.pb_wrappers.gbml_config import GbmlConfigPbWrapper
from gigl.src.common.utils.model import load_state_dict_from_uri, save_state_dict
from gigl.utils.iterator import InfiniteIterator
from gigl.utils.sampling import parse_fanout

logger = Logger()


@dataclass
class DMPConfig:
    device: torch.device
    world_size: int
    local_world_size: int
    pg: Optional[torch.distributed.ProcessGroup] = None
    compute_device: str = "cuda"  # or "cpu"
    prefer_sharding_types: Optional[Sequence[str]] = ("table_wise", "row_wise")
    compute_kernel: EmbeddingComputeKernel = EmbeddingComputeKernel.FUSED


def wrap_with_dmp(model: nn.Module, cfg: DMPConfig) -> nn.Module:
    """Wraps `model` with TorchRec DMP (shards EBCs, DP for the rest)."""
    dmp_model = DMP(module=model, device=cfg.device)
    return dmp_model


def unwrap_from_dmp(model: nn.Module) -> nn.Module:
    """Return the underlying nn.Module if wrapped by DMP, otherwise the module itself."""
    return getattr(model, "module", model)


def _sync_metric_across_processes(metric: torch.Tensor) -> float:
    """
    Takes the average of a training metric across multiple processes. Note that this function requires DDP to be initialized.
    Args:
        metric (torch.Tensor): The metric, expressed as a torch Tensor, which should be synced across multiple processes
    Returns:
        float: The average of the provided metric across all training processes
    """
    assert is_distributed_available_and_initialized(), "DDP is not initialized"
    # Make a copy of the local loss tensor
    loss_tensor = metric.detach().clone()
    torch.distributed.all_reduce(loss_tensor, op=torch.distributed.ReduceOp.SUM)
    return loss_tensor.item() / torch.distributed.get_world_size()


# ============================================================================
# Evaluation Metrics: Recall@K and NDCG@K (LightGCN paper metrics)
# ============================================================================


def recall_at_k(
    scores: torch.Tensor,
    labels: torch.Tensor,
    k_values: list[int],
) -> dict[int, float]:
    """
    Computes Recall@K for collaborative filtering.

    Recall@K = (# of relevant items in top-K) / (# of relevant items)

    For link prediction with 1 positive per query, this simplifies to:
    Recall@K = 1 if positive is in top-K, else 0

    Args:
        scores: Score tensor of shape [B, 1 + N], where B is batch size, N is number of negatives.
            First column contains positive scores, rest are negative scores.
        labels: Label tensor of shape [B, 1 + N], where 1 indicates positive, 0 indicates negative.
            First column should contain 1 (positive label).
        k_values: List of K values to compute Recall@K for (e.g., [10, 20, 50, 100])

    Returns:
        Dictionary mapping K -> Recall@K value
    """
    batch_size = scores.size(0)
    max_k = max(k_values)

    # Get top-K indices (shape: B x max_k)
    topk_indices = torch.topk(scores, k=max_k, dim=1).indices

    # Gather labels for top-K items (shape: B x max_k)
    topk_labels = torch.gather(labels, dim=1, index=topk_indices)

    # For each K, check if positive appears in top-K
    recalls = {}
    for k in k_values:
        # Check if any of the top-K items is positive (label == 1)
        hits = (topk_labels[:, :k].sum(dim=1) > 0).float()  # (B,)
        recalls[k] = hits.mean().item()

    return recalls


def ndcg_at_k(
    scores: torch.Tensor,
    labels: torch.Tensor,
    k_values: list[int],
) -> dict[int, float]:
    """
    Computes Normalized Discounted Cumulative Gain (NDCG@K) for collaborative filtering.

    DCG@K = sum_{i=1}^{K} (2^{rel_i} - 1) / log2(i + 1)
    NDCG@K = DCG@K / IDCG@K

    For binary relevance (0 or 1), this simplifies to:
    DCG@K = sum_{i=1}^{K} rel_i / log2(i + 1)

    Args:
        scores: Score tensor of shape [B, 1 + N], where B is batch size, N is number of negatives.
            First column contains positive scores, rest are negative scores.
        labels: Label tensor of shape [B, 1 + N], where 1 indicates positive, 0 indicates negative.
            First column should contain 1 (positive label).
        k_values: List of K values to compute NDCG@K for (e.g., [10, 20, 50, 100])

    Returns:
        Dictionary mapping K -> NDCG@K value
    """
    batch_size = scores.size(0)
    max_k = max(k_values)

    # Get top-K indices (shape: B x max_k)
    topk_indices = torch.topk(scores, k=max_k, dim=1).indices

    # Gather labels for top-K items (shape: B x max_k)
    topk_labels = torch.gather(labels, dim=1, index=topk_indices).float()  # (B, max_k)

    # Compute position weights: 1 / log2(i + 1) for i in [1, 2, ..., max_k]
    positions = torch.arange(1, max_k + 1, device=scores.device).float()
    weights = 1.0 / torch.log2(positions + 1.0)  # (max_k,)

    ndcgs = {}
    for k in k_values:
        # DCG@K: sum of (relevance * weight) for top-K
        dcg = (topk_labels[:, :k] * weights[:k]).sum(dim=1)  # (B,)

        # IDCG@K: ideal DCG (assumes we have 1 relevant item, so IDCG@K = 1 / log2(2) = 1.0)
        # Since we have exactly 1 positive per query, the ideal ranking puts it at position 1
        idcg = 1.0 / torch.log2(torch.tensor(2.0, device=scores.device))  # 1.0

        # NDCG@K = DCG@K / IDCG@K
        ndcg = dcg / idcg
        ndcgs[k] = ndcg.mean().item()

    return ndcgs


def compute_metrics_from_scores(
    scores: torch.Tensor,
    labels: torch.Tensor,
    k_values: list[int] = [10, 20, 50, 100],
) -> dict[str, float]:
    """
    Compute all evaluation metrics (Recall@K, NDCG@K) from score and label tensors.

    Args:
        scores: Score tensor of shape [B, 1 + N]
        labels: Label tensor of shape [B, 1 + N]
        k_values: List of K values for evaluation (default: [10, 20, 50, 100] as in LightGCN paper)

    Returns:
        Dictionary with metric names and values, e.g.:
        {
            'recall@10': 0.123,
            'recall@20': 0.234,
            'ndcg@10': 0.456,
            'ndcg@20': 0.567,
        }
    """
    recalls = recall_at_k(scores, labels, k_values)
    ndcgs = ndcg_at_k(scores, labels, k_values)

    metrics = {}
    for k in k_values:
        metrics[f'recall@{k}'] = recalls[k]
        metrics[f'ndcg@{k}'] = ndcgs[k]

    return metrics


def _setup_dataloaders(
    dataset: DistDataset,
    split: Literal["train", "val", "test"],
    supervision_edge_type: EdgeType,
    num_neighbors: list[int],
    sampling_workers_per_process: int,
    main_batch_size: int,
    random_batch_size: int,
    device: torch.device,
    sampling_worker_shared_channel_size: str,
    process_start_gap_seconds: int,
) -> tuple[DistABLPLoader, DistNeighborLoader]:
    """
    Sets up main and random dataloaders for training and testing purposes
    Args:
        dataset (DistDataset): Loaded Distributed Dataset for training and testing
        split (Literal["train", "val", "test"]): The current split which we are loading data for
        supervision_edge_type (EdgeType): The supervision edge type to use for training in format query_node -> relation -> labeled_node
        num_neighbors: list[int]: Fanout for subgraph sampling, where the ith item corresponds to the number of items to sample for the ith hop
        sampling_workers_per_process (int): Number of sampling workers per training/testing process
        main_batch_size (int): Batch size for main dataloader with query and labeled nodes
        random_batch_size (int): Batch size for random negative dataloader
        device (torch.device): Device to put loaded subgraphs on
        sampling_worker_shared_channel_size (str): Shared-memory buffer size (bytes) allocated for the channel during sampling
        process_start_gap_seconds (int): The amount of time to sleep for initializing each dataloader. For large-scale settings, consider setting this
            field to 30-60 seconds to ensure dataloaders don't compete for memory during initialization, causing OOM.
    Returns:
        DistABLPLoader: Dataloader for loading main batch data with query and labeled nodes
        DistNeighborLoader: Dataloader for loading random negative data
    """
    rank = torch.distributed.get_rank()

    logger.info(f"Rank {rank} setting up main loader for split {split}")

    logger.info(dataset.graph)
    if split == "train":
        dsts, srcs, _, _ = dataset.graph[("item", "to_train_gigl_positive", "user")].topo.to_coo()
        main_input_nodes = srcs.unique()
        logger.info(f"source: {srcs.shape}, destination: {dsts.shape}")
        logger.info(main_input_nodes.shape)
        logger.info(f"Rank {rank} train_node_ids: {main_input_nodes}")
        shuffle = True
    elif split == "val":
        main_input_nodes = dataset.val_node_ids
        shuffle = False
    else:
        dsts, srcs, _, _ = dataset.graph[("item", "to_test_gigl_positive", "user")].topo.to_coo()
        main_input_nodes = srcs.unique()
        logger.info(f"source: {srcs.shape}, destination: {dsts.shape}")
        logger.info(main_input_nodes.shape)
        logger.info(f"Rank {rank} test_node_ids: {main_input_nodes}")
        shuffle = False

    query_node_type = supervision_edge_type.src_node_type
    labeled_node_type = supervision_edge_type.dst_node_type

    # assert isinstance(main_input_nodes, Mapping)

    main_loader = DistABLPLoader(
        dataset=dataset,
        num_neighbors=num_neighbors,
        input_nodes=(query_node_type, main_input_nodes),
        supervision_edge_type=supervision_edge_type,
        num_workers=sampling_workers_per_process,
        batch_size=main_batch_size,
        pin_memory_device=device,
        worker_concurrency=sampling_workers_per_process,
        channel_size=sampling_worker_shared_channel_size,
        # Each train_main_loader will wait for `process_start_gap_seconds` * `local_process_rank` seconds before initializing to reduce peak memory usage.
        # This is done so that each process on the current machine which initializes a `main_loader` doesn't compete for memory, causing potential OOM
        process_start_gap_seconds=process_start_gap_seconds,
        shuffle=shuffle,
    )

    logger.info(f"---Rank {rank} finished setting up main loader")

    # We need to wait for all processes to finish initializing the main_loader before creating the random_negative_loader so that its initialization doesn't compete for memory with the main_loader, causing potential OOM.
    torch.distributed.barrier()

    assert isinstance(dataset.node_ids, Mapping)

    random_negative_loader = DistNeighborLoader(
        dataset=dataset,
        num_neighbors=num_neighbors,
        input_nodes=(labeled_node_type, dataset.node_ids[labeled_node_type]),
        num_workers=sampling_workers_per_process,
        batch_size=random_batch_size,
        pin_memory_device=device,
        worker_concurrency=sampling_workers_per_process,
        channel_size=sampling_worker_shared_channel_size,
        process_start_gap_seconds=process_start_gap_seconds,
        shuffle=shuffle,
    )

    logger.info(f"--Rank {rank} finished setting up random negative loader")

    # Wait for all processes to finish initializing the random_loader
    torch.distributed.barrier()

    return main_loader, random_negative_loader


def bpr_loss(
    query_emb: torch.Tensor,  # [M, D]
    pos_emb: torch.Tensor,  # [M, D]
    neg_emb: torch.Tensor,  # [M, D] or [M, K, D]
    l2_lambda: float = 0.0,
    l2_params: Optional[Sequence[torch.Tensor]] = None,
) -> torch.Tensor:
    """Bayesian Personalized Ranking loss with dot-product scores.

    Supports one negative per positive ([M, D]) or K negatives ([M, K, D]).
    """
    # s_pos: [M]
    s_pos = (query_emb * pos_emb).sum(dim=-1)

    if neg_emb.dim() == 2:  # [M, D]
        s_neg = (query_emb * neg_emb).sum(dim=-1)
        loss = -torch.nn.functional.logsigmoid(s_pos - s_neg)
    elif neg_emb.dim() == 3:  # [M, K, D]
        # Broadcast query: [M, 1, D]
        s_neg = (query_emb.unsqueeze(1) * neg_emb).sum(dim=-1)  # [M, K]
        loss = -torch.nn.functional.logsigmoid(s_pos.unsqueeze(1) - s_neg).mean(dim=1)
    else:
        raise ValueError("neg_emb must be [M, D] or [M, K, D]")

    loss = loss.mean()

    if l2_lambda > 0.0 and l2_params:
        l2 = sum(p.pow(2).sum() for p in l2_params)
        loss = loss + l2_lambda * l2

    return loss


def _compute_bpr_batch(
    model: nn.Module,
    main_data: HeteroData,
    random_negative_data: HeteroData,
    supervision_edge_type: EdgeType,
    device: torch.device,
    num_random_negs_per_pos: int = 1,
    use_hard_negs: bool = True,
    l2_lambda: float = 0.0,
    debug_log: bool = False,
) -> tuple[torch.Tensor, dict[str, float]]:
    """Compute a BPR batch using LightGCN embeddings and GLT-batched indices for heterogeneous graphs.

    Strategy: one (or K) random negative(s) per positive. If hard negatives exist in the batch,
    we concatenate them as additional negatives (weighting equally).

    Returns:
        loss: The BPR loss
        debug_info: Dictionary with debug statistics (scores, embedding stats, etc.)
    """
    # logger.info(f"Computing BPR batch")
    # Extract relevant node types from the supervision edge
    query_node_type = supervision_edge_type.src_node_type
    labeled_node_type = supervision_edge_type.dst_node_type

    # logger.info(f"Encoding main data")
    # Encode - LightGCN returns dict[NodeType, Tensor] for heterogeneous graphs
    main_emb = model(data=main_data, device=device)
    rand_emb = model(data=random_negative_data, device=device)

    # Debug: collect statistics
    debug_info = {}

    # logger.info(f"Query indices and positives from the main batch")
    # Query indices and positives from the main batch
    B = int(main_data[query_node_type].batch_size)
    query_idx = torch.arange(B, device=device)  # [B]

    # logger.info(f"Positives from the main batch")
    pos_idx = torch.cat(list(main_data.y_positive.values())).to(device)  # [M]
    # Repeat queries to align with positives
    rep_query_idx = query_idx.repeat_interleave(
        torch.tensor([len(v) for v in main_data.y_positive.values()], device=device)
    )  # [M]

    # logger.info(f"Hard negatives from the main batch")
    # Optional hard negatives from the main batch
    if use_hard_negs and hasattr(main_data, "y_negative"):
        hard_neg_idx = torch.cat(list(main_data.y_negative.values())).to(device)
        hard_neg_emb = main_emb[labeled_node_type][hard_neg_idx]  # [H, D]
    else:
        hard_neg_idx = torch.empty(0, dtype=torch.long, device=device)
        hard_neg_emb = torch.empty(
            0, main_emb[labeled_node_type].size(1), device=device
        )

    # logger.info(f"Random negatives: take the first K*M rows from rand_emb for simplicity")
    # Random negatives: take the first K*M rows from rand_emb for simplicity
    M = rep_query_idx.numel()
    D = main_emb[labeled_node_type].size(1)

    total_needed = M * max(1, num_random_negs_per_pos)
    rand_batch_size = int(random_negative_data[labeled_node_type].batch_size)
    if rand_batch_size < total_needed:
        # Tile if fewer than needed
        tile = (total_needed + rand_batch_size - 1) // rand_batch_size
        rand_pool = rand_emb[labeled_node_type][:rand_batch_size].repeat(tile, 1)[
            :total_needed
        ]
    else:
        rand_pool = rand_emb[labeled_node_type][:total_needed]

    # logger.info(f"Positive and query embeddings")
    if num_random_negs_per_pos == 1:
        rand_neg_emb = rand_pool  # [M, D]
    else:
        rand_neg_emb = rand_pool.view(M, num_random_negs_per_pos, D)  # [M, K, D]

    # logger.info(f"Computing scores for debugging")
    # Positive and query embeddings
    q = main_emb[query_node_type][rep_query_idx]  # [M, D]
    pos = main_emb[labeled_node_type][pos_idx]  # [M, D]

    # If we have hard negatives, merge with random negatives by stacking along K
    if hard_neg_emb.numel() > 0:
        # Align hard negatives count to M.
        if hard_neg_emb.size(0) < M:
            ht = (M + hard_neg_emb.size(0) - 1) // hard_neg_emb.size(0)
            hard_neg_emb = hard_neg_emb.repeat(ht, 1)[:M]
        if rand_neg_emb.dim() == 2:  # [M, D]
            neg = torch.stack([rand_neg_emb, hard_neg_emb], dim=1)  # [M, 2, D]
        else:  # [M, K, D]
            neg = torch.cat(
                [rand_neg_emb, hard_neg_emb.unsqueeze(1)], dim=1
            )  # [M, K+1, D]
    else:
        neg = rand_neg_emb  # [M, D] or [M, K, D]

    # Compute scores for debugging
    s_pos = (q * pos).sum(dim=-1)  # [M]
    if neg.dim() == 2:  # [M, D]
        s_neg = (q * neg).sum(dim=-1)  # [M]
    else:  # [M, K, D]
        s_neg = (q.unsqueeze(1) * neg).sum(dim=-1)  # [M, K]

    # Collect debug info
    if debug_log:
        debug_info["pos_score_mean"] = s_pos.mean().item()
        debug_info["pos_score_std"] = s_pos.std().item()
        debug_info["neg_score_mean"] = s_neg.mean().item()
        debug_info["neg_score_std"] = s_neg.std().item()
        debug_info["query_emb_mean"] = q.mean().item()
        debug_info["query_emb_std"] = q.std().item()
        debug_info["query_emb_norm"] = q.norm(dim=-1).mean().item()
        debug_info["pos_emb_mean"] = pos.mean().item()
        debug_info["pos_emb_std"] = pos.std().item()
        debug_info["pos_emb_norm"] = pos.norm(dim=-1).mean().item()
        debug_info["neg_emb_mean"] = neg.mean().item()
        debug_info["neg_emb_std"] = neg.std().item()
        debug_info["neg_emb_norm"] = neg.norm(dim=-1).mean().item()
        debug_info["num_positives"] = M
        debug_info["num_hard_negs"] = hard_neg_idx.numel()
        # Sample some actual node IDs
        debug_info["sample_query_ids"] = rep_query_idx[:5].tolist()
        debug_info["sample_pos_ids"] = pos_idx[:5].tolist()

    loss = bpr_loss(q, pos, neg, l2_lambda=l2_lambda, l2_params=None)

    if debug_log:
        debug_info["bpr_loss"] = loss.item()

    return loss, debug_info


def _evaluate_with_metrics(
    model: nn.Module,
    main_data_loader: Iterator,
    random_negative_data_loader: Iterator,
    supervision_edge_type: EdgeType,
    device: torch.device,
    num_batches: int,
    num_random_negs_per_pos: int = 1,
    use_hard_negs: bool = True,
    k_values: list[int] = [10, 20, 50, 100],
) -> dict[str, float]:
    """
    Evaluate the model using Recall@K and NDCG@K metrics (as used in LightGCN paper).

    Args:
        model: The trained model
        main_data_loader: Iterator for main data (positives + hard negatives)
        random_negative_data_loader: Iterator for random negatives
        supervision_edge_type: The edge type for supervision
        device: Device to run evaluation on
        num_batches: Number of batches to evaluate on
        num_random_negs_per_pos: Number of random negatives per positive
        use_hard_negs: Whether to include hard negatives
        k_values: List of K values for Recall@K and NDCG@K

    Returns:
        Dictionary with metric names and values averaged across all batches
    """
    model.eval()

    query_node_type = supervision_edge_type.src_node_type
    labeled_node_type = supervision_edge_type.dst_node_type

    # Accumulate metrics across batches
    all_metrics = {f'recall@{k}': [] for k in k_values}
    all_metrics.update({f'ndcg@{k}': [] for k in k_values})

    with torch.no_grad():
        for batch_idx in range(num_batches):
            main_data = next(main_data_loader)
            random_negative_data = next(random_negative_data_loader)

            # Get embeddings
            main_emb = model(data=main_data, device=device)
            rand_emb = model(data=random_negative_data, device=device)

            # Get query indices and positives
            B = int(main_data[query_node_type].batch_size)
            query_idx = torch.arange(B, device=device)

            pos_idx = torch.cat(list(main_data.y_positive.values())).to(device)
            rep_query_idx = query_idx.repeat_interleave(
                torch.tensor([len(v) for v in main_data.y_positive.values()], device=device)
            )

            # Get embeddings
            query_emb = main_emb[query_node_type][rep_query_idx]  # [M, D]
            pos_emb = main_emb[labeled_node_type][pos_idx]  # [M, D]

            # Random negatives
            M = pos_idx.size(0)
            rand_B = int(random_negative_data[labeled_node_type].batch_size)
            rand_neg_idx = torch.randint(0, rand_B, (M * num_random_negs_per_pos,), device=device)
            rand_neg_emb = rand_emb[labeled_node_type][rand_neg_idx].view(M, num_random_negs_per_pos, -1)  # [M, K, D]

            # Hard negatives (if available)
            if use_hard_negs and hasattr(main_data, "y_negative"):
                hard_neg_idx = torch.cat(list(main_data.y_negative.values())).to(device)
                hard_neg_emb = main_emb[labeled_node_type][hard_neg_idx]  # [H, D]
            else:
                hard_neg_emb = torch.empty(0, main_emb[labeled_node_type].size(1), device=device)

            # Compute scores
            # Positive scores: [M, 1]
            pos_scores = (query_emb * pos_emb).sum(dim=-1, keepdim=True)

            # Random negative scores: [M, K]
            rand_neg_scores = torch.bmm(
                query_emb.unsqueeze(1),  # [M, 1, D]
                rand_neg_emb.transpose(1, 2),  # [M, D, K]
            ).squeeze(1)  # [M, K]

            # Hard negative scores: [M, H]
            if hard_neg_emb.size(0) > 0:
                hard_neg_scores = torch.matmul(query_emb, hard_neg_emb.T)  # [M, H]
                # Concatenate all negative scores
                all_neg_scores = torch.cat([rand_neg_scores, hard_neg_scores], dim=1)  # [M, K+H]
            else:
                all_neg_scores = rand_neg_scores  # [M, K]

            # Concatenate positive and negative scores: [M, 1+K+H]
            scores = torch.cat([pos_scores, all_neg_scores], dim=1)

            # Create labels: first column is 1 (positive), rest are 0 (negatives)
            labels = torch.zeros_like(scores)
            labels[:, 0] = 1.0

            # Compute metrics for this batch
            batch_metrics = compute_metrics_from_scores(scores, labels, k_values)

            # Accumulate
            for metric_name, metric_value in batch_metrics.items():
                all_metrics[metric_name].append(metric_value)

    # Average across batches
    avg_metrics = {name: sum(values) / len(values) for name, values in all_metrics.items()}

    return avg_metrics


def _training_process(
    local_rank: int,
    local_world_size: int,
    machine_rank: int,
    machine_world_size: int,
    dataset: DistDataset,
    train_supervision_edge_type: EdgeType,
    test_supervision_edge_type: EdgeType,
    node_type_to_num_nodes: dict[NodeType, int],
    master_ip_address: str,
    master_default_process_group_port: int,
    model_uri: Uri,
    num_neighbors: list[int],
    sampling_workers_per_process: int,
    main_batch_size: int,
    random_batch_size: int,
    embedding_dim: int,
    num_layers: int,
    sampling_worker_shared_channel_size: str,
    process_start_gap_seconds: int,
    log_every_n_batch: int,
    learning_rate: float,
    weight_decay: float,
    num_max_train_batches: int,
    num_val_batches: int,
    val_every_n_batch: int,
    should_skip_training: bool,
    num_random_negs_per_pos: int,
    l2_lambda: float,
) -> None:
    """
    This function is spawned by each machine for training a heterogeneous LightGCN model given some loaded distributed dataset.
    Args:
        local_rank (int): Process number on the current machine
        local_world_size (int): Number of training processes spawned by each machine
        machine_rank (int): Rank of the current machine
        machine_world_size (int): Total number of machines
        dataset (DistDataset): Loaded Distributed Dataset for training
        train_supervision_edge_type (EdgeType): The supervision edge type to use for training (e.g., user -> to_train -> item)
        test_supervision_edge_type (EdgeType): The supervision edge type to use for testing (e.g., user -> to_test -> item)
        node_type_to_num_nodes (dict[NodeType, int]): Map from node types to node counts for LightGCN
        master_ip_address (str): IP Address of the master worker for distributed communication
        master_default_process_group_port (int): Port on the master worker for setting up distributed process group communication
        model_uri (Uri): URI Path to save the model to
        num_neighbors: list[int]: Fanout for subgraph sampling, where the ith item corresponds to the number of items to sample for the ith hop
        sampling_workers_per_process (int): Number of sampling workers per training process
        main_batch_size (int): Batch size for main dataloader with query and labeled nodes
        random_batch_size (int): Batch size for random negative dataloader
        embedding_dim (int): Embedding dimension of the model
        num_layers (int): Number of LightGCN layers
        sampling_worker_shared_channel_size (str): Shared-memory buffer size (bytes) allocated for the channel during sampling
        process_start_gap_seconds (int): The amount of time to sleep for initializing each dataloader. For large-scale settings, consider setting this
            field to 30-60 seconds to ensure dataloaders don't compete for memory during initialization, causing OOM.
        log_every_n_batch (int): The frequency we should log batch information when training
        learning_rate (float): Learning rate for training
        weight_decay (float): Weight decay for training
        num_max_train_batches (int): The maximum number of batches to train for across all training processes
        num_val_batches (int): The number of batches to do validation for across all training processes
        val_every_n_batch: (int): The frequency we should log batch information when validating
        should_skip_training (bool): Whether training should be skipped and we should only run testing. Assumes model has been uploaded to the model_uri.
        num_random_negs_per_pos (int): Number of random negatives per positive
        l2_lambda (float): L2 regularization strength
    """
    world_size = machine_world_size * local_world_size
    rank = machine_rank * local_world_size + local_rank
    logger.info(
        f"---Current training process rank: {rank}, training process world size: {world_size}"
    )

    torch.distributed.init_process_group(
        backend="nccl" if torch.cuda.is_available() else "gloo",
        init_method=f"tcp://{master_ip_address}:{master_default_process_group_port}",
        world_size=world_size,
        rank=rank,
    )

    device = get_available_device(local_process_rank=local_rank)
    if torch.cuda.is_available():
        torch.cuda.set_device(device)
    logger.info(f"Training process rank {rank} is using device {device}")

    # Build LightGCN model
    logger.info(f"---Rank {rank} building LightGCN model")
    logger.info(f"node_type_to_num_nodes: {node_type_to_num_nodes}")
    logger.info(f"embedding_dim: {embedding_dim}")
    logger.info(f"num_layers: {num_layers}")
    logger.info(f"device: {device}")
    base_model = LightGCN(
        node_type_to_num_nodes=node_type_to_num_nodes,
        embedding_dim=embedding_dim,
        num_layers=num_layers,
        device=device,
    )

    logger.info(f"base_model: {base_model}")

    # Apply sparse optimizer to embedding bag collection BEFORE DMP wrapping
    # This is the correct TorchRec pattern - optimizer must be applied before sharding
    logger.info(f"---Rank {rank} applying sparse optimizer to embedding bag collection (BEFORE DMP)")
    sparse_lr = learning_rate
    for name, param in base_model._embedding_bag_collection.named_parameters():
        logger.info(f"  Applying RowWiseAdagrad to {name}")
        apply_optimizer_in_backward(
            optimizer_class=RowWiseAdagrad,
            params=[param],
            optimizer_kwargs={"lr": sparse_lr, "weight_decay": weight_decay},
        )
    logger.info(f"Applied RowWiseAdagrad (lr={sparse_lr}, weight_decay={weight_decay}) to embedding parameters")

    logger.info(f"---Rank {rank} wrapping LightGCN model with DMP")
    model = wrap_with_dmp(
        base_model,
        DMPConfig(
            device=device,
            world_size=world_size,
            local_world_size=local_world_size,
            pg=torch.distributed.group.WORLD,
            compute_device="cuda" if device.type == "cuda" else "cpu",
        ),
    )
    logger.info(f"model: {model}")

    # # Initialize embeddings AFTER DMP wrapping (DMP reinitializes them, so doing it before is useless)
    # logger.info(f"---Rank {rank} initializing embeddings with scaled Xavier uniform (AFTER DMP)")
    # unwrapped_model = unwrap_from_dmp(model)
    # logger.info(f"EmbeddingBagCollection parameters:")
    # init_count = 0
    # EMBEDDING_SCALE = 0.1  # Small scale to avoid saturation
    # for name, param in unwrapped_model._embedding_bag_collection.named_parameters():
    #     logger.info(f"  Found parameter: {name}, shape: {param.shape}, device: {param.device}")
    #     logger.info(f"    BEFORE init - mean={param.mean().item():.6f}, std={param.std().item():.6f}, norm={param.norm().item():.6f}")

    #     # Xavier uniform: U(-sqrt(6/(fan_in + fan_out)), sqrt(6/(fan_in + fan_out)))
    #     # For embeddings: fan_in = 1, fan_out = embedding_dim
    #     # Then scale up to combat LightGCN's aggressive neighbor averaging
    #     torch.nn.init.xavier_uniform_(param)
    #     param.data *= EMBEDDING_SCALE
    #     init_count += 1

    #     logger.info(f"    AFTER init (scaled {EMBEDDING_SCALE}x) - mean={param.mean().item():.6f}, std={param.std().item():.6f}, norm={param.norm().item():.6f}")

    # logger.info(f"Initialized {init_count} embedding parameters after DMP wrapping with {EMBEDDING_SCALE}x scaling")

    # After DMP, create dense optimizer for non-embedding parameters (e.g., LGConv layers)
    logger.info(f"---Rank {rank} creating dense optimizer for non-embedding parameters")
    dense_params = dict(in_backward_optimizer_filter(model.named_parameters()))

    if dense_params:
        logger.info(f"Found {len(dense_params)} dense parameters")
        dense_optimizer = KeyedOptimizerWrapper(
            dense_params,
            lambda params: AdamW(params, lr=learning_rate, weight_decay=weight_decay),
        )
        # Combine fused (sparse) optimizer with dense optimizer
        optimizer = CombinedOptimizer([model.fused_optimizer, dense_optimizer])
        logger.info(f"Created CombinedOptimizer with fused (sparse) and dense optimizers")
    else:
        logger.info("No dense parameters found, using only fused optimizer")
        optimizer = CombinedOptimizer([model.fused_optimizer])

    logger.info(f"optimizer: {optimizer}")
    logger.info(f"num_neighbors: {num_neighbors}")


    if should_skip_training:
        logger.info(f"Rank {rank}: Skipping training and loading model from {model_uri}")
        state_dict = load_state_dict_from_uri(model_uri)
        model.load_state_dict(state_dict)
    else:
        logger.info(f"Rank {rank}: Setting up training dataloaders")
        train_main_loader, train_random_negative_loader = _setup_dataloaders(
            dataset=dataset,
            split="train",
            supervision_edge_type=train_supervision_edge_type,  # Use TRAIN edges for training
            num_neighbors=num_neighbors,
            sampling_workers_per_process=sampling_workers_per_process,
            main_batch_size=main_batch_size,
            random_batch_size=random_batch_size,
            device=device,
            sampling_worker_shared_channel_size=sampling_worker_shared_channel_size,
            process_start_gap_seconds=process_start_gap_seconds,
        )

        logger.info(f"Rank {rank}: Setting up validation dataloaders")
        # val_main_loader, val_random_negative_loader = _setup_dataloaders(
        #     dataset=dataset,
        #     split="val",
        #     supervision_edge_type=train_supervision_edge_type,  # Use TRAIN edges for validation
        #     num_neighbors=num_neighbors,
        #     sampling_workers_per_process=sampling_workers_per_process,
        #     main_batch_size=main_batch_size,
        #     random_batch_size=random_batch_size,
        #     device=device,
        #     sampling_worker_shared_channel_size=sampling_worker_shared_channel_size,
        #     process_start_gap_seconds=process_start_gap_seconds,
        # )

        logger.info(f"Rank {rank}: Starting training loop")
        model.train()
        start_time = time.time()

        train_main_iter = InfiniteIterator(train_main_loader)
        train_random_neg_iter = InfiniteIterator(train_random_negative_loader)

        # val_main_iter = InfiniteIterator(val_main_loader)
        # val_random_neg_iter = InfiniteIterator(val_random_negative_loader)

        batch_losses = []
        val_losses = []

        # Track a sample embedding to see if it changes
        sample_node_type = list(node_type_to_num_nodes.keys())[0]
        sample_node_id = 0

        logger.info(f"Starting training loop")

        for batch_num in range(num_max_train_batches):
            logger.info(f"Training batch {batch_num + 1} of {num_max_train_batches}")
            # Enable debug logging for first batch and every log_every_n_batch
            debug_this_batch = (batch_num == 0) or ((batch_num + 1) % log_every_n_batch == 0)

            optimizer.zero_grad()
            # logger.info(f"Zeroing gradients")
            main_data = next(train_main_iter)
            # logger.info(f"Main data: {main_data}")
            random_negative_data = next(train_random_neg_iter)
            # logger.info(f"Random negative data: {random_negative_data}")
            # Log sample data for debugging
            if debug_this_batch and rank == 0:
                logger.info(f"\n{'='*60}")
                logger.info(f"DEBUG Batch {batch_num + 1}")
                logger.info(f"{'='*60}")
                logger.info(f"Main data node types: {main_data.node_types}")
                logger.info(f"Main data edge types: {main_data.edge_types}")
                for node_type in main_data.node_types:
                    node_store = main_data[node_type]
                    batch_size = getattr(node_store, 'batch_size', 'N/A')
                    n_id_shape = node_store.n_id.shape if hasattr(node_store, 'n_id') else 'N/A'
                    num_nodes = node_store.num_nodes if hasattr(node_store, 'num_nodes') else 'N/A'
                    logger.info(f"  {node_type}: batch_size={batch_size}, n_id shape={n_id_shape}, num_nodes={num_nodes}")
                # if hasattr(main_data, 'y_positive'):
                #     logger.info(f"y_positive keys: {list(main_data.y_positive.keys())}")
                #     for k, v in main_data.y_positive.items():
                #         logger.info(f"  {k}: {len(v)} positives, sample IDs: {v[:5].tolist() if len(v) > 0 else []}")

            loss, debug_info = _compute_bpr_batch(
                model=model,
                main_data=main_data,
                random_negative_data=random_negative_data,
                supervision_edge_type=train_supervision_edge_type,  # Use TRAIN edges during training
                device=device,
                num_random_negs_per_pos=num_random_negs_per_pos,
                use_hard_negs=True,
                l2_lambda=l2_lambda,
                debug_log=debug_this_batch,
            )

            if debug_this_batch and rank == 0:
                logger.info(f"\nBatch {batch_num + 1} DEBUG INFO:")
                for key, value in sorted(debug_info.items()):
                    logger.info(f"  {key}: {value}")

            loss.backward()

            # Check if gradients exist and their magnitudes
            # NOTE: With TorchRec's fused optimizer, embedding gradients are not materialized on .grad
            # (they're applied directly in backward), so we only check non-embedding params here
            if debug_this_batch and rank == 0:
                grad_norms = []
                param_norms = []
                for name, param in model.named_parameters():
                    if param.grad is not None:
                        grad_norms.append(param.grad.norm().item())
                        param_norms.append(param.norm().item())
                if grad_norms:
                    logger.info(f"  Non-embedding gradient norms - mean: {sum(grad_norms)/len(grad_norms):.6f}, "
                               f"max: {max(grad_norms):.6f}, min: {min(grad_norms):.6f}")
                    logger.info(f"  Non-embedding param norms - mean: {sum(param_norms)/len(param_norms):.6f}, "
                               f"max: {max(param_norms):.6f}, min: {min(param_norms):.6f}")
                else:
                    logger.info("  Note: No .grad found on params (expected with TorchRec fused optimizer for embeddings)")

            optimizer.step()

            batch_loss = _sync_metric_across_processes(loss)
            batch_losses.append(batch_loss)

            if (batch_num + 1) % log_every_n_batch == 0:
                avg_loss = statistics.mean(batch_losses[-log_every_n_batch:])
                elapsed = time.time() - start_time
                logger.info(
                    f"Rank {rank} | Batch {batch_num + 1}/{num_max_train_batches} | "
                    f"Train Loss: {avg_loss:.4f} | Elapsed: {elapsed:.2f}s"
                )
                logger.info(f"{'='*60}\n")

            # # Validation
            # if (batch_num + 1) % val_every_n_batch == 0:
            #     model.eval()
            #     with torch.no_grad():
            #         val_batch_losses = []
            #         for _ in range(num_val_batches):
            #             val_main_data = next(val_main_iter)
            #             val_random_negative_data = next(val_random_neg_iter)

            #             val_loss, _ = _compute_bpr_batch(
            #                 model=model,
            #                 main_data=val_main_data,
            #                 random_negative_data=val_random_negative_data,
            #                 supervision_edge_type=supervision_edge_type,
            #                 device=device,
            #                 num_random_negs_per_pos=num_random_negs_per_pos,
            #                 use_hard_negs=True,
            #                 l2_lambda=l2_lambda,
            #                 debug_log=False,
            #             )

            #             val_batch_loss = _sync_metric_across_processes(val_loss)
            #             val_batch_losses.append(val_batch_loss)

            #         avg_val_loss = statistics.mean(val_batch_losses)
            #         val_losses.append(avg_val_loss)
            #         logger.info(
            #             f"Rank {rank} | Batch {batch_num + 1} | Val Loss: {avg_val_loss:.4f}"
            #         )

            #     model.train()

        logger.info(f"Rank {rank}: Training completed. Saving model to {model_uri}")
        # if rank == 0:
        #     save_state_dict(model.state_dict(), model_uri)

    # Final testing with Recall@K and NDCG@K metrics
    logger.info(f"Rank {rank}: Setting up test dataloaders")
    test_main_loader, test_random_negative_loader = _setup_dataloaders(
        dataset=dataset,
        split="test",
        supervision_edge_type=test_supervision_edge_type,  # Use TEST edges for evaluation!
        num_neighbors=num_neighbors,
        sampling_workers_per_process=sampling_workers_per_process,
        main_batch_size=main_batch_size,
        random_batch_size=random_batch_size,
        device=device,
        sampling_worker_shared_channel_size=sampling_worker_shared_channel_size,
        process_start_gap_seconds=process_start_gap_seconds,
    )

    logger.info(f"Rank {rank}: Running test evaluation with Recall@K and NDCG@K metrics")
    logger.info(f"  Evaluating on TEST edges: {test_supervision_edge_type}")

    # K values to evaluate (as in LightGCN paper, Table 2)
    # Paper uses: Recall@20, NDCG@20 for Gowalla
    eval_k_values = [10, 20, 50, 100]

    test_metrics = _evaluate_with_metrics(
        model=model,
        main_data_loader=InfiniteIterator(test_main_loader),
        random_negative_data_loader=InfiniteIterator(test_random_negative_loader),
        supervision_edge_type=test_supervision_edge_type,  # Use TEST edges for metric computation
        device=device,
        num_batches=min(100, num_val_batches),
        num_random_negs_per_pos=num_random_negs_per_pos,
        use_hard_negs=True,
        k_values=eval_k_values,
    )

    # Log test metrics
    if rank == 0:
        logger.info(f"\n{'='*80}")
        logger.info("TEST EVALUATION RESULTS (LightGCN Paper Metrics)")
        logger.info(f"{'='*80}")
        for k in eval_k_values:
            recall = test_metrics[f'recall@{k}']
            ndcg = test_metrics[f'ndcg@{k}']
            logger.info(f"  Recall@{k:3d}: {recall:.4f}  |  NDCG@{k:3d}: {ndcg:.4f}")
        logger.info(f"{'='*80}\n")

    torch.distributed.destroy_process_group()


def _run_example_training(
    task_config_uri: str,
):
    """
    Runs an example heterogeneous training + testing loop using GiGL Orchestration.
    Args:
        task_config_uri (str): Path to YAML-serialized GbmlConfig proto.
    """
    start_time = time.time()
    mp.set_start_method("spawn")
    logger.info(f"Starting sub process method: {mp.get_start_method()}")

    gbml_config_pb_wrapper = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(
        gbml_config_uri=UriFactory.create_uri(task_config_uri)
    )

    # Training Hyperparameters for the training and test processes
    trainer_args = dict[Any, Any](gbml_config_pb_wrapper.trainer_config.trainer_args)

    local_world_size = int(trainer_args.get("local_world_size", "1"))
    if torch.cuda.is_available():
        if local_world_size > torch.cuda.device_count():
            raise ValueError(
                f"Specified a local world_size of {local_world_size} which exceeds the number of devices {torch.cuda.device_count()}"
            )

    # Parse supervision edge types - expecting 2: one for training, one for testing
    supervision_edge_types = gbml_config_pb_wrapper.gbml_config_pb.task_metadata.node_anchor_based_link_prediction_task_metadata.supervision_edge_types

    if len(supervision_edge_types) == 1:
        # Legacy behavior: only one edge type specified (for training)
        # Create test edge type by replacing "to_train" with "to_test"
        supervision_edge_type_pb = supervision_edge_types[0]
        train_supervision_edge_type = EdgeType(
            src_node_type=NodeType(supervision_edge_type_pb.src_node_type),
            relation=Relation(supervision_edge_type_pb.relation),
            dst_node_type=NodeType(supervision_edge_type_pb.dst_node_type),
        )
        test_relation = supervision_edge_type_pb.relation.replace("to_train", "to_test")
        test_supervision_edge_type = EdgeType(
            src_node_type=NodeType(supervision_edge_type_pb.src_node_type),
            relation=Relation(test_relation),
            dst_node_type=NodeType(supervision_edge_type_pb.dst_node_type),
        )
        logger.info("Using single supervision edge type (legacy mode)")
    elif len(supervision_edge_types) == 2:
        # New behavior: two edge types specified (training and testing)
        train_edge_type_pb = supervision_edge_types[0]
        test_edge_type_pb = supervision_edge_types[1]

        train_supervision_edge_type = EdgeType(
            src_node_type=NodeType(train_edge_type_pb.src_node_type),
            relation=Relation(train_edge_type_pb.relation),
            dst_node_type=NodeType(train_edge_type_pb.dst_node_type),
        )
        test_supervision_edge_type = EdgeType(
            src_node_type=NodeType(test_edge_type_pb.src_node_type),
            relation=Relation(test_edge_type_pb.relation),
            dst_node_type=NodeType(test_edge_type_pb.dst_node_type),
        )
        logger.info("Using explicit train and test supervision edge types")
    else:
        raise ValueError(f"Expected 1 or 2 supervision edge types, got {len(supervision_edge_types)}")

    logger.info(f"Train supervision edge type: {train_supervision_edge_type}")
    logger.info(f"Test supervision edge type: {test_supervision_edge_type}")

    # Parses the fanout as a string. For heterogeneous case, fanouts should be specified as a string of a list of integers, such as "[10, 10]".
    fanout = trainer_args.get("num_neighbors", "[10, 10]")
    # for debugging:
    # fanout = "{('user', 'to_train', 'item'): [10, 10, 10, 10], ('user', 'to_test', 'item'): [0, 0, 0, 0], ('item', 'to_train', 'user'): [15, 15, 15, 15], ('item', 'to_test', 'user'): [0, 0, 0, 0]}"
    logger.info(f"fanout: {fanout}")
    num_neighbors = parse_fanout(fanout)
    logger.info(f"num_neighbors: {num_neighbors}")


    # While the ideal value for `sampling_workers_per_process` has been identified to be between `2` and `4`, this may need some tuning depending on the
    # pipeline. We default this value to `4` here for simplicity. A `sampling_workers_per_process` which is too small may not have enough parallelization for
    # sampling, which would slow down training, while a value which is too large may slow down each sampling process due to competing resources, which would also
    # then slow down training.
    sampling_workers_per_process: int = int(
        trainer_args.get("sampling_workers_per_process", "8")
    )

    main_batch_size = int(trainer_args.get("main_batch_size", "2048"))
    random_batch_size = int(trainer_args.get("random_batch_size", "2048"))

    # LightGCN Hyperparameters
    embedding_dim = int(trainer_args.get("embedding_dim", "64"))
    num_layers = int(trainer_args.get("num_layers", "2"))

    # BPR params
    num_random_negs_per_pos = int(trainer_args.get("num_random_negs_per_pos", "1"))
    l2_lambda = float(trainer_args.get("l2_lambda", "0.0"))

    # This value represents the the shared-memory buffer size (bytes) allocated for the channel during sampling, and
    # is the place to store pre-fetched data, so if it is too small then prefetching is limited, causing sampling slowdown. This parameter is a string
    # with `{numeric_value}{storage_size}`, where storage size could be `MB`, `GB`, etc. We default this value to 4GB,
    # but in production may need some tuning.
    sampling_worker_shared_channel_size: str = trainer_args.get(
        "sampling_worker_shared_channel_size", "4GB"
    )

    process_start_gap_seconds = int(trainer_args.get("process_start_gap_seconds", "0"))
    log_every_n_batch = int(trainer_args.get("log_every_n_batch", "25"))
    log_every_n_batch = 25

    learning_rate = float(trainer_args.get("learning_rate", "0.01"))
    weight_decay = float(trainer_args.get("weight_decay", "0.0005"))
    num_max_train_batches = int(trainer_args.get("num_max_train_batches", "1000"))
    num_max_train_batches = 10
    num_val_batches = int(trainer_args.get("num_val_batches", "100"))
    val_every_n_batch = int(trainer_args.get("val_every_n_batch", "50"))

    logger.info(
        f"Got training args local_world_size={local_world_size}, \
        num_neighbors={num_neighbors}, \
        sampling_workers_per_process={sampling_workers_per_process}, \
        main_batch_size={main_batch_size}, \
        random_batch_size={random_batch_size}, \
        embedding_dim={embedding_dim}, \
        num_layers={num_layers}, \
        num_random_negs_per_pos={num_random_negs_per_pos}, \
        l2_lambda={l2_lambda}, \
        sampling_worker_shared_channel_size={sampling_worker_shared_channel_size}, \
        process_start_gap_seconds={process_start_gap_seconds}, \
        log_every_n_batch={log_every_n_batch}, \
        learning_rate={learning_rate}, \
        weight_decay={weight_decay}, \
        num_max_train_batches={num_max_train_batches}, \
        num_val_batches={num_val_batches}, \
        val_every_n_batch={val_every_n_batch}"
    )

    # This `init_process_group` is only called to get the master_ip_address, master port, and rank/world_size fields which help with partitioning, sampling,
    # and distributed training/testing. We can use `gloo` here since these fields we are extracting don't require GPU capabilities provided by `nccl`.
    # Note that this init_process_group uses env:// to setup the connection.
    # In VAI we create one process per node thus these variables are exposed through env i.e. MASTER_PORT , MASTER_ADDR , WORLD_SIZE , RANK that VAI sets up for us.
    # If running locally, these env variables will need to be setup by the user manually.
    torch.distributed.init_process_group(backend="gloo")

    master_ip_address = gigl.distributed.utils.get_internal_ip_from_master_node()
    machine_rank = torch.distributed.get_rank()
    machine_world_size = torch.distributed.get_world_size()
    master_default_process_group_port = (
        gigl.distributed.utils.get_free_ports_from_master_node(num_ports=1)
    )[0]
    # Destroying the process group as one will be re-initialized in the training process using above information
    torch.distributed.destroy_process_group()

    logger.info(f"--- Launching data loading process ---")
    dataset = build_dataset_from_task_config_uri(
        task_config_uri=UriFactory.create_uri(task_config_uri),
        is_inference=False,
    )
    logger.info(f"Dataset: {dataset}")
    logger.info(dir(dataset))
    logger.info(f"Dataset.train_node_ids: {dataset.train_node_ids}")
    logger.info(f"Dataset.val_node_ids: {dataset.val_node_ids}")
    logger.info(f"Dataset.test_node_ids: {dataset.test_node_ids}")

    # Calculate node_type_to_num_nodes from dataset
    node_type_to_num_nodes: dict[NodeType, int] = {}
    for node_type_str, node_ids_tensor in dataset.node_ids.items():
        node_type = NodeType(node_type_str)
        max_id = int(node_ids_tensor.max().item())
        num_nodes = max_id + 1
        node_type_to_num_nodes[node_type] = num_nodes
        logger.info(f"Node type {node_type}: {num_nodes} nodes (max_id={max_id})")

    logger.info(
        f"--- Data loading process finished, took {time.time() - start_time:.3f} seconds"
    )

    model_uri = UriFactory.create_uri(
        gbml_config_pb_wrapper.gbml_config_pb.shared_config.trained_model_metadata.trained_model_uri
    )

    should_skip_training = gbml_config_pb_wrapper.shared_config.should_skip_training

    logger.info("--- Launching training processes ...\n")
    start_time = time.time()
    torch.multiprocessing.spawn(
        _training_process,
        args=(  # Corresponding arguments in `_training_process` function
            local_world_size,  # local_world_size
            machine_rank,  # machine_rank
            machine_world_size,  # machine_world_size
            dataset,  # dataset
            train_supervision_edge_type,  # train_supervision_edge_type (user -> to_train -> item)
            test_supervision_edge_type,  # test_supervision_edge_type (user -> to_test -> item)
            node_type_to_num_nodes,  # node_type_to_num_nodes
            master_ip_address,  # master_ip_address
            master_default_process_group_port,  # master_default_process_group_port
            model_uri,  # model_uri
            num_neighbors,  # num_neighbors
            sampling_workers_per_process,  # sampling_workers_per_process
            main_batch_size,  # main_batch_size
            random_batch_size,  # random_batch_size
            embedding_dim,  # embedding_dim
            num_layers,  # num_layers
            sampling_worker_shared_channel_size,  # sampling_worker_shared_channel_size
            process_start_gap_seconds,  # process_start_gap_seconds
            log_every_n_batch,  # log_every_n_batch
            learning_rate,  # learning_rate
            weight_decay,  # weight_decay
            num_max_train_batches,  # num_max_train_batches
            num_val_batches,  # num_val_batches
            val_every_n_batch,  # val_every_n_batch
            should_skip_training,  # should_skip_training
            num_random_negs_per_pos,  # num_random_negs_per_pos
            l2_lambda,  # l2_lambda
        ),
        nprocs=local_world_size,
        join=True,
    )
    logger.info(f"--- Training finished, took {time.time() - start_time} seconds")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Arguments for distributed model training on VertexAI"
    )
    parser.add_argument("--task_config_uri", type=str, help="Gbml config uri")

    # We use parse_known_args instead of parse_args since we only need job_name and task_config_uri for distributed trainer
    logger.info(f"Starting heterogeneous training")
    args, unused_args = parser.parse_known_args()
    logger.info(f"Args: {args}")
    logger.info(f"Unused arguments: {unused_args}")

    # We only need `task_config_uri` for running trainer
    _run_example_training(
        task_config_uri=args.task_config_uri,
    )

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d9e20f",
   "metadata": {},
   "source": [
    "# In-Memory GiGL - Heterogeneous Graph Example\n",
    "\n",
    "Latest version of this notebook can be found on [github](https://github.com/Snapchat/GiGL/blob/main/examples/tutorial/KDD_2025/heterogeneous_walkthrough.ipynb)\n",
    "\n",
    "\n",
    "This notebook provides a walkthrough of preprocessing components with a small toy graph for GiGL's in-memory setting for training/inference. It will help you understand how each of these components perform in-memory training and inference.\n",
    "\n",
    "\n",
    "## Overview Of Components\n",
    "This notebook demonstrates the process of a simple, human-digestible graph being passed through all the pipeline components in GiGL in preparation for training to help understand how each of the components work.\n",
    "\n",
    "The pipeline consists of the following components:\n",
    "\n",
    "- **Config Populator**: Takes a template config and creates a frozen workflow config that dictates all inputs/outputs and business parameters that are read and used by each subsequent component. The template config contains the graph/task definitions and commands/classes you will use to run data preprocessor, trainer, and inferencer. The frozen template config will populate the template config with additional fields detailing where intermediate assets will be written to, such as the trained model and tfrecord output location, and is inferred based on the job name. \n",
    "    - Inputs: \n",
    "        - `template_config.yaml`\n",
    "        - `resource_config.yaml`\n",
    "    - Output: `frozen_gbml_config.yaml`\n",
    "&nbsp;\n",
    "\n",
    "- **Data Preprocessor**: Transforms necessary node and edge feature assets as needed as a precursor step in most ML tasks according to the user-provided data preprocessor config class.\n",
    "    - Inputs: \n",
    "        - `frozen_gbml_config.yaml`, which includes the user-defined preprocessor class for custom logic, and custom arguments can be passed under dataPreprocessorArgs.\n",
    "        - `resource_config.yaml`\n",
    "    - Output: PreprocessedMetadata Proto, which includes inferred GraphMetadata and preprocessed graph data Tfrecords after applying the user-defined preprocessing function.\n",
    "&nbsp;\n",
    "\n",
    "- **Trainer**: The trainer component reads the output of the data preprocessor and trains a model on the data, loading subgraphs on-the-fly by leveraging GiGL's distributed in-memory subgraph sampling capabilities.\n",
    "    - Inputs: \n",
    "        - `frozen_gbml_config.yaml`\n",
    "        - `resource_config.yaml`\n",
    "    - Output: state_dict stored in trainedModelUri.\n",
    "&nbsp;\n",
    "\n",
    "- **Inferencer**: Runs inference of a trained model, leveraging the same distributed in-memory subgraph sampling capabilities, and writes the embeddings to BigQuery.\n",
    "    - Input: \n",
    "        - `frozen_gbml_config.yaml`\n",
    "        - `resource_config.yaml`\n",
    "    - Output: Embeddings assets.\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0dadb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow logs\n",
    "\n",
    "\n",
    "from gigl.common.utils.jupyter_magics import change_working_dir_to_gigl_root\n",
    "change_working_dir_to_gigl_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f416f",
   "metadata": {},
   "source": [
    "## Visualize the dataset\n",
    "\n",
    "First, let's visualize the toy graph :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7073e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from gigl.common.utils.jupyter_magics import GraphVisualizer\n",
    "from gigl.src.mocking.toy_asset_mocker import load_toy_graph\n",
    "\n",
    "\n",
    "original_graph_heterodata: HeteroData = load_toy_graph(graph_config_path=\"examples/tutorial/KDD_2025/graph_config.yaml\")\n",
    "# Visualize the graph\n",
    "GraphVisualizer.visualize_graph(original_graph_heterodata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e468bdd3",
   "metadata": {},
   "source": [
    "## Resource config setup.\n",
    "\n",
    "If you are attending the GiGL tutorial at KDD'25 then you should run the below cells **in addtion to \"Setting up Configs\" to setup your resource config for a Qwiklabs project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b113079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "project = subprocess.check_output(['gcloud', 'config', 'get-value', 'project']).decode(\"utf-8\").strip()  # e.g. qwiklabs-gcp-02-5a342b5770cb\n",
    "\n",
    "RESOURCE_CONFIG_URI = f\"gs://gigl_perm_assets_{project}/tabularized_resource_config.yaml\"\n",
    "\n",
    "os.environ[\"PROJECT\"] = project\n",
    "os.environ[\"RESOURCE_CONFIG_PATH\"] = RESOURCE_CONFIG_URI\n",
    "os.environ[\"GIGL_TEST_DEFAULT_RESOURCE_CONFIG\"] = RESOURCE_CONFIG_URI\n",
    "\n",
    "print(f\"Project: {project}, Resource config uri: {RESOURCE_CONFIG_URI}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e02b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python  <(curl -s https://raw.githubusercontent.com/Snapchat/GiGL/refs/heads/main/scripts/bootstrap_resource_config.py) \\\n",
    "  --project=\"$PROJECT\" \\\n",
    "  --region=\"us-central1\" \\\n",
    "  --gcp_service_account_email=\"gigl-dev@$PROJECT.iam.gserviceaccount.com\" \\\n",
    "  --docker_artifact_registry_path=\"us-central1-docker.pkg.dev/$PROJECT/gigl-base-images\" \\\n",
    "  --temp_assets_bq_dataset_name=\"gigl_temp_assets\" \\\n",
    "  --embedding_bq_dataset_name=\"gigl_embeddings\" \\\n",
    "  --temp_assets_bucket=\"gs://gigl_temp_assets_$PROJECT\" \\\n",
    "  --perm_assets_bucket=\"gs://gigl_perm_assets_$PROJECT\" \\\n",
    "  --template_resource_config_uri=\"examples/tutorial/KDD_2025/resource_config.yaml\" \\\n",
    "  --output_resource_config_path=\"$RESOURCE_CONFIG_PATH\" \\\n",
    "  --force_shell_config_update=True\n",
    "\n",
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the resource config we just created\n",
    "!gsutil cat $RESOURCE_CONFIG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fdf5ce",
   "metadata": {},
   "source": [
    "### Setting up Configs\n",
    "\n",
    "The first thing we need to do is create the resource and task configs. \n",
    "\n",
    "- **Task Config**: Specifies task-related configurations, guiding the behavior of components according to the needs of your machine learning task. See [Task Config Guide](../../../docs/user_guide/config_guides/task_config_guide.md). For this task, we have already provided a task config: [task_config.yaml](./task_config.yaml).\n",
    "\n",
    "- **Resource Config**: Details the resource allocation and environmental settings across all GiGL components. This encompasses shared resources for all components, as well as component-specific settings. See [Resource Config Guide](../../../docs/user_guide/config_guides/resource_config_guide.md). For this task, we provide a resource [resource_config.yaml](./resource_config.yaml). The provided default values in `shared_resource_config.common_compute_config` will need to be changed.\n",
    "\n",
    "  - **Instructions to configure the resource config to work**:\n",
    "    If you have not already, please follow the [Quick Start Guide](../../../docs/user_guide/getting_started/quick_start.md) to set up your cloud environment and create a default test resource config. You can then copy the relevant `shared_resource_config.common_compute_config` to [resource_config.yaml](./resource_config.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d74f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import getpass\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "\n",
    "from gigl.common import Uri, UriFactory\n",
    "from gigl.common.utils.gcs import GcsUri\n",
    "from gigl.src.common.utils.file_loader import FileLoader\n",
    "from gigl.common.constants import DEFAULT_GIGL_RELEASE_SRC_IMAGE_DATAFLOW_CPU, DEFAULT_GIGL_RELEASE_SRC_IMAGE_CUDA, DEFAULT_GIGL_RELEASE_SRC_IMAGE_CPU\n",
    "from gigl.env.pipelines_config import get_resource_config\n",
    "from gigl.src.common.types.pb_wrappers.gigl_resource_config import GiglResourceConfigWrapper\n",
    "\n",
    "file_loader = FileLoader()\n",
    "\n",
    "notebook_dir = pathlib.Path(\"./examples/tutorial/KDD_2025\").as_posix() # We should be in root dir because of cell # 1\n",
    "\n",
    "# You are welcome to customize these to point to your own configuration files.\n",
    "JOB_NAME = f\"{getpass.getuser()}_gigl_toy_example_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "TEMPLATE_TASK_CONFIG_PATH: Uri = UriFactory.create_uri(f\"{notebook_dir}/task_config.yaml\")\n",
    "FROZEN_TASK_CONFIG_POINTER_FILE_PATH: Uri = UriFactory.create_uri(f\"/tmp/GiGL/{JOB_NAME}/frozen_task_config.yaml\")\n",
    "pathlib.Path(FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "resource_config_loaded_path: Uri = UriFactory.create_uri(os.environ.get(\"GIGL_TEST_DEFAULT_RESOURCE_CONFIG\", f\"{notebook_dir}/resource_config.yaml\"))\n",
    "if not isinstance(resource_config_loaded_path, GcsUri):\n",
    "    # If our resource config is not already on GCS, we should upload the local resource so that we can it can be used in our VAI pipelines\n",
    "    resource_config_wrapper: GiglResourceConfigWrapper = get_resource_config(\n",
    "        resource_config_uri=resource_config_loaded_path\n",
    "    )\n",
    "    RESOURCE_CONFIG_PATH: Uri = UriFactory.create_uri(os.path.join(resource_config_wrapper.temp_assets_bucket_path.uri, f\"{getpass.getuser()}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_example_resource_config.yaml\"))\n",
    "    file_loader.load_file(file_uri_src=resource_config_loaded_path, file_uri_dst=RESOURCE_CONFIG_PATH)\n",
    "else:\n",
    "    RESOURCE_CONFIG_PATH = resource_config_loaded_path\n",
    "\n",
    "print(f\"JOB_NAME: {JOB_NAME}\")\n",
    "print(f\"TEMPLATE_TASK_CONFIG_PATH: {TEMPLATE_TASK_CONFIG_PATH.uri}\")\n",
    "print(f\"FROZEN_TASK_CONFIG_POINTER_FILE_PATH: {FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri}\")\n",
    "print(f\"RESOURCE_CONFIG_PATH: {RESOURCE_CONFIG_PATH.uri}\")\n",
    "\n",
    "os.environ[\"JOB_NAME\"] = JOB_NAME\n",
    "os.environ[\"TEMPLATE_TASK_CONFIG_PATH\"] = TEMPLATE_TASK_CONFIG_PATH.uri\n",
    "os.environ[\"FROZEN_TASK_CONFIG_POINTER_FILE_PATH\"] = FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri\n",
    "os.environ[\"RESOURCE_CONFIG_PATH\"] = RESOURCE_CONFIG_PATH.uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748d994",
   "metadata": {},
   "source": [
    "## Validating the Configs\n",
    "\n",
    "We provide the ability to validate your resource and task configs. Although the validation is not exhaustive, it does help assert that the more common issues are not present before expensive compute is scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9978c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.src.validation_check.config_validator import kfp_validation_checks\n",
    "\n",
    "validator = kfp_validation_checks(\n",
    "    job_name=JOB_NAME,\n",
    "    task_config_uri=TEMPLATE_TASK_CONFIG_PATH,\n",
    "    resource_config_uri=RESOURCE_CONFIG_PATH,\n",
    "    start_at=\"config_populator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a465d071",
   "metadata": {},
   "source": [
    "### Config Populator\n",
    "\n",
    "Takes in a template `GbmlConfig` and outputs a frozen `GbmlConfig` by populating all job-related metadata paths in `sharedConfig`. These are mostly GCS paths that the following components read and write from, and use as an intermediary data communication medium. For example, the field `sharedConfig.trainedModelMetadata.trained_model_uri` is populated with a GCS URI, which indicates to the Trainer to write the trained model to this path, and to the Inferencer to read the model from this path. See the full [Config Populator Guide](../../../docs/user_guide/overview/components/config_populator.md).\n",
    "\n",
    "After running the command below, we will have created a frozen config and uploaded it to the `perm_assets_bucket` provided in the `resource config`. The path to that file will be stored in the URI in `FROZEN_TASK_CONFIG_POINTER_FILE_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebe61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m \\\n",
    "    gigl.src.config_populator.config_populator \\\n",
    "    --job_name=\"$JOB_NAME\" \\\n",
    "    --template_uri=\"$TEMPLATE_TASK_CONFIG_PATH\" \\\n",
    "    --resource_config_uri=\"$RESOURCE_CONFIG_PATH\" \\\n",
    "    --output_file_path_frozen_gbml_config_uri=\"$FROZEN_TASK_CONFIG_POINTER_FILE_PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c151ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The command above will write the frozen task config path to the file specified by `FROZEN_TASK_CONFIG_POINTER_FILE_PATH`.\n",
    "# Lets see where it was generated\n",
    "FROZEN_TASK_CONFIG_PATH: Uri\n",
    "with open(FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri, 'r') as file:\n",
    "    FROZEN_TASK_CONFIG_PATH = UriFactory.create_uri(file.read().strip())\n",
    "print(f\"FROZEN_TASK_CONFIG_PATH: {FROZEN_TASK_CONFIG_PATH}\")\n",
    "os.environ[\"FROZEN_TASK_CONFIG_PATH\"] = FROZEN_TASK_CONFIG_PATH.uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6492ee",
   "metadata": {},
   "source": [
    "## Visualizing the Diff Between Template and Frozen Config\n",
    "\n",
    "We now have a frozen task config, with the path specified by `FROZEN_TASK_CONFIG_PATH`. We visualize the diff between the `frozen_task_config` generated by the `config_populator` and the original `template_task_config`. All the code below is just to do that and has nothing to do with GiGL.\n",
    "\n",
    "Specifically, note that:\n",
    "1. The component added `sharedConfig` to the YAML, which contains all the intermediary and final output paths for each component.\n",
    "2. It also added a `condensedEdgeTypeMap` and a `condensedNodeTypeMap`, which map all provided edge types and node types to `int` to save storage space:\n",
    "   - `EdgeType: Tuple[srcNodeType: str, relation: str, dstNodeType: str)] -> int`, and \n",
    "   - `NodeType: str -> int`\n",
    "   - Note: You may also provide your own condensedMaps; they will be generated for you if not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b066e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.common.utils.jupyter_magics import show_task_config_colored_unified_diff\n",
    "\n",
    "show_task_config_colored_unified_diff(\n",
    "    f1_uri=FROZEN_TASK_CONFIG_PATH,\n",
    "    f2_uri=TEMPLATE_TASK_CONFIG_PATH,\n",
    "    f1_name='frozen_task_config.yaml',\n",
    "    f2_name='template_task_config.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dc1b94",
   "metadata": {},
   "source": [
    "# Data Preprocessor\n",
    "\n",
    "Once we have a `frozen_task_config`, the first step is to preprocess the data. The Data Preprocessor component uses [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started) to achieve data transformation in a distributed fashion. In this component, we enumerate the node identifiers, remapping them from their original node identifiers, which can be an integer or string specified by the user, to instead be between 0, 1, ... N for each node type, where N is the total number of nodes for that node type. We also can perform preprocessing on the data (more on this later). These preprocessed and enumerated nodes are uploaded as TFRecords in GCS by the end of this component. \n",
    "\n",
    "### Input Parameters and Output Paths for Data Preprocessor\n",
    "Let's take a quick look at what the data preprocessor fields look like in our frozen config for our toy example. \n",
    "\n",
    "You'll see that the `datasetConfig` contains two fields: a `datasetPreprocessorConfigClsPath` and a `datasetPreprocessorArgs`. \n",
    "- The `datasetPreprocessorConfigClsPath` tells the data preprocessor component which class to call for specifying node, edge, and additional preprocessing information. We will write an example data preprocessor class together shortly.\n",
    "- The `datasetPreprocessorArgs` specifies arguments that should be expected to be passed into the above class' `__init__` function. This can be useful for specifying any argument for data preprocessor component from the task config, such as BigQuery table paths, specific features to extract, etc. \n",
    "\n",
    "\n",
    "Additionally, the generated frozen config also populates an additional `preprocessedMetadataUri` field whose path is unique to your job identifier. This contains the path to a YAML which contains information about the nodes/edges in the graph and their features. This YAML is populated by the data preprocessor component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6206cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will load the frozen task and resource configs file into an object so we can reference it\n",
    "from gigl.env.pipelines_config import get_resource_config\n",
    "from gigl.src.common.types.pb_wrappers.gbml_config import GbmlConfigPbWrapper\n",
    "from gigl.src.common.types.pb_wrappers.gigl_resource_config import GiglResourceConfigWrapper\n",
    "import textwrap\n",
    "\n",
    "frozen_task_config = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(\n",
    "    gbml_config_uri=FROZEN_TASK_CONFIG_PATH\n",
    ")\n",
    "resource_config: GiglResourceConfigWrapper = get_resource_config(\n",
    "    resource_config_uri=RESOURCE_CONFIG_PATH\n",
    ")\n",
    "\n",
    "print(\"Frozen Config DataPreprocessor Information:\")\n",
    "\n",
    "print(\"- Data Preprocessor Config: Specifies what class to use for datapreprocessing and any arguments that might be passed in at runtime to that class\")\n",
    "print(textwrap.indent(str(frozen_task_config.dataset_config.data_preprocessor_config), '\\t'))\n",
    "print(\"- Preprocessed Metadata Uri: Specifies path to the preprocessed metadata file that will be generated by this component and used by subsequent components to understand and find the data that was preprocessed\")\n",
    "print(textwrap.indent(str(frozen_task_config.shared_config.preprocessed_metadata_uri), '\\t'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a903c426",
   "metadata": {},
   "source": [
    "### Visualizing the graph in BigQuery\n",
    "\n",
    "For our example, the Data Preprocessor will begin with a toy graph that has been uploaded to bigquery. Let's first visualize what this uploaded graph looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.src.common.utils.bq import BqUtils\n",
    "\n",
    "\n",
    "data_preprocessor_args = frozen_task_config.dataset_config.data_preprocessor_config.data_preprocessor_args\n",
    "# Node Tables\n",
    "USER_NODE_TABLE = data_preprocessor_args.get(\"bq_user_node_table_name\")\n",
    "STORY_NODE_TABLE = data_preprocessor_args.get(\"bq_story_node_table_name\")\n",
    "\n",
    "# Edge Tables\n",
    "USER_STORY_EDGE_TABLE = data_preprocessor_args.get(\"bq_user_story_edge_table_name\")\n",
    "STORY_USER_EDGE_TABLE = data_preprocessor_args.get(\"bq_story_user_edge_table_name\")\n",
    "\n",
    "print(f\"BigQuery User Node Table Name: {USER_NODE_TABLE}\")\n",
    "print(f\"BigQuery Story Node Table Name: {STORY_NODE_TABLE}\")\n",
    "print(f\"BigQuery User Story Edge Table Name: {USER_STORY_EDGE_TABLE}\")\n",
    "print(f\"BigQuery Story User Edge Table Name: {STORY_USER_EDGE_TABLE}\")\n",
    "\n",
    "def display_table(table_bq_path):\n",
    "    bq_utils = BqUtils()\n",
    "    visualize_rows_query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM `{table_bq_path}`\n",
    "        LIMIT 2\n",
    "    \"\"\"\n",
    "    result = bq_utils.run_query(query=visualize_rows_query, labels=resource_config.get_resource_labels())\n",
    "    for row in result:\n",
    "        print(dict(row))\n",
    "\n",
    "\n",
    "print(\"\\n--Visualizing User Node Table--\\n\")\n",
    "display_table(USER_NODE_TABLE)\n",
    "\n",
    "print(\"\\n--Visualizing Story Node Table--\\n\")\n",
    "display_table(STORY_NODE_TABLE)\n",
    "\n",
    "print(\"\\n--Visualizing User-Story Edge Table--\\n\")\n",
    "display_table(USER_STORY_EDGE_TABLE)\n",
    "\n",
    "print(\"\\n--Visualizing Story-User Edge Table--\\n\")\n",
    "display_table(USER_STORY_EDGE_TABLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655fc69",
   "metadata": {},
   "source": [
    "Notice that we have also mocked edge features for the toy graph, but we will only be using node features for this demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8168352f",
   "metadata": {},
   "source": [
    "\n",
    "### Data Preprocessor Config\n",
    "\n",
    "Next, we'll dive into how to build a data preprocessor config for the toy example.\n",
    "\n",
    "- This class must inherit from [gigl.src.data_preprocessor.lib.data_preprocessor_config.DataPreprocessorConfig](https://snapchat.github.io/GiGL/docs/api/gigl/src/data_preprocessor/lib/data_preprocessor_config/index.html#gigl.src.data_preprocessor.lib.data_preprocessor_config.DataPreprocessorConfig).\n",
    "\n",
    "In your preprocessor spec, you must implement the following 3 functions as defined by the base class `DataPreprocessorConfig`:\n",
    "  - `prepare_for_pipeline`: Preparing datasets for ingestion and transformation.\n",
    "  - `get_nodes_preprocessing_spec`: Defining transformation imperatives for different node types.\n",
    "  - `get_edges_preprocessing_spec`: Defining transformation imperatives for different edge types.\n",
    "\n",
    "We will highlight how these functions are implemented for the toy graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0791ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant imports for creating a Data Preprocessor Config\n",
    "\n",
    "from gigl.src.common.types import AppliedTaskIdentifier\n",
    "from gigl.src.common.types.graph_data import EdgeType, EdgeUsageType, NodeType, Relation\n",
    "from gigl.src.data_preprocessor.lib.data_preprocessor_config import (\n",
    "    DataPreprocessorConfig,\n",
    "    build_ingestion_feature_spec_fn,\n",
    "    build_passthrough_transform_preprocessing_fn,\n",
    ")\n",
    "from gigl.src.data_preprocessor.lib.ingest.bigquery import (\n",
    "    BigqueryEdgeDataReference,\n",
    "    BigqueryNodeDataReference,\n",
    ")\n",
    "from gigl.src.data_preprocessor.lib.ingest.reference import (\n",
    "    EdgeDataReference,\n",
    "    NodeDataReference,\n",
    ")\n",
    "from gigl.src.data_preprocessor.lib.types import (\n",
    "    EdgeDataPreprocessingSpec,\n",
    "    EdgeOutputIdentifier,\n",
    "    NodeDataPreprocessingSpec,\n",
    "    NodeOutputIdentifier,\n",
    ")\n",
    "\n",
    "class ToyDataPreprocessorConfig(DataPreprocessorConfig):\n",
    "    \"\"\"\n",
    "    Any data preprocessor config needs to inherit from DataPreprocessorConfig and implement the necessary methods:\n",
    "    - prepare_for_pipeline: This method is called at the very start of the pipeline. Can be used to prepare any data,\n",
    "    such as running BigQuery queries, or kicking of dataflow pipelines etc. to generate node/edge feature tables.\n",
    "    - get_nodes_preprocessing_spec: This method returns a dictionary of NodeDataReference to NodeDataPreprocessingSpec\n",
    "        This is used to specify how to preprocess the node data using a TFT preprocessing function.\n",
    "        See TFT documentation for more details: https://www.tensorflow.org/tfx/transform/get_started\n",
    "    - get_edges_preprocessing_spec: This method returns a dictionary of EdgeDataReference to EdgeDataPreprocessingSpec\n",
    "        This is used to specify how to preprocess the edge data using a TFT preprocessing function\n",
    "    \"\"\"\n",
    "\n",
    "    # We use the __init__ function to define node types, edge types, and the node/edge tables that we will be feeding into the data preprocessor.\n",
    "    # The arguments to __init__ are provided through the DataPreprocessorArgs field in the task config\n",
    "    def __init__(self, bq_user_node_table_name: str, bq_story_node_table_name: str, bq_user_story_edge_table_name: str, bq_story_user_edge_table_name: str):\n",
    "        super().__init__()\n",
    "\n",
    "        self._user_table = bq_user_node_table_name\n",
    "        self._story_table = bq_story_node_table_name\n",
    "\n",
    "        self._user_to_story_table = bq_user_story_edge_table_name\n",
    "        self._story_to_user_table = bq_story_user_edge_table_name\n",
    "\n",
    "        # We also specify the node types and edge types for the heterogeneous graph:\n",
    "\n",
    "        self._user_node_type = NodeType(\"user\")\n",
    "        self._story_node_type = NodeType(\"story\")\n",
    "\n",
    "        self._user_to_story_edge_type = EdgeType(\n",
    "            self._user_node_type,\n",
    "            Relation(\"to\"),\n",
    "            self._story_node_type,\n",
    "        )\n",
    "\n",
    "        self._story_to_user_edge_type = EdgeType(\n",
    "            self._story_node_type,\n",
    "            Relation(\"to\"),\n",
    "            self._user_node_type,\n",
    "        )\n",
    "\n",
    "        # These features are taken from our node tables. Note that both the \"user\" and \"story\" node types use the same feature names.\n",
    "        self._node_float_feature_list = [\"f0\", \"f1\"]\n",
    "\n",
    "        # We store a mapping of each node type to their respective table URI.\n",
    "        self._node_tables: dict[NodeType, str] = {\n",
    "            self._user_node_type: self._user_table,\n",
    "            self._story_node_type: self._story_table,\n",
    "        }\n",
    "\n",
    "        # We store a mapping of each edge type to their respective table URI.\n",
    "        self._edge_tables: dict[EdgeType, str] = {\n",
    "            self._user_to_story_edge_type: self._user_to_story_table,\n",
    "            self._story_to_user_edge_type: self._story_to_user_table,\n",
    "        }\n",
    "\n",
    "    def prepare_for_pipeline(\n",
    "        self, applied_task_identifier: AppliedTaskIdentifier\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        This function is called at the very start of the pipeline before enumerator and data preprocessor. \n",
    "        This function does not return anything and can be used to perform any operation needed\n",
    "        before running the pipeline, such as gathering data for node and edge sources\n",
    "\n",
    "        Args: \n",
    "            applied_task_identifier (AppliedTaskIdentifier): A unique identifier for the task being run. This is usually \n",
    "                the job name if orchestrating through GiGL's orchestration logic.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def get_nodes_preprocessing_spec(\n",
    "        self,\n",
    "    ) -> dict[NodeDataReference, NodeDataPreprocessingSpec]:\n",
    "        # We specify where the input data is located using NodeDataReference\n",
    "        # In this case, we are reading from BigQuery, thus make use off BigqueryNodeDataReference\n",
    "\n",
    "        output_dict: dict[NodeDataReference, NodeDataPreprocessingSpec] = {}\n",
    "\n",
    "        # Both of our node table use \"node_id\" for specifying the node identifier.\n",
    "        node_identifier = \"node_id\"\n",
    "\n",
    "        for node_type, table in self._node_tables.items():\n",
    "            node_data_reference = BigqueryNodeDataReference(\n",
    "                reference_uri=table,\n",
    "                node_type=node_type,\n",
    "            )\n",
    "\n",
    "            # This is the column name for identifying the node after the TFTransform\n",
    "            node_output_id = NodeOutputIdentifier(node_identifier)\n",
    "\n",
    "            # The ingestion feature spec function is used to specify the input columns and their types\n",
    "            # that will be read from the NodeDataReference - which in this case is BQ.\n",
    "            feature_spec_fn = build_ingestion_feature_spec_fn(\n",
    "                fixed_int_fields=[node_identifier],\n",
    "                fixed_float_fields=self._node_float_feature_list,\n",
    "            )\n",
    "\n",
    "            # We don't need any special preprocessing for the node features.\n",
    "            # Thus, we can make use of a \"passthrough\" transform preprocessing function that simply passes the input\n",
    "            # features through to the output features.\n",
    "            preprocessing_fn = build_passthrough_transform_preprocessing_fn()\n",
    "\n",
    "            output_dict[node_data_reference] = NodeDataPreprocessingSpec(\n",
    "                feature_spec_fn=feature_spec_fn,\n",
    "                preprocessing_fn=preprocessing_fn,\n",
    "                identifier_output=node_output_id,\n",
    "                features_outputs=self._node_float_feature_list,\n",
    "            )\n",
    "        return output_dict\n",
    "\n",
    "    def get_edges_preprocessing_spec(\n",
    "        self,\n",
    "    ) -> dict[EdgeDataReference, EdgeDataPreprocessingSpec]:\n",
    "        output_dict: dict[EdgeDataReference, EdgeDataPreprocessingSpec] = {}\n",
    "\n",
    "        # Both of our edge table uses \"src\" and \"dst\" for specifying the node ids for each edge.\n",
    "        src_node_identifier = \"src\"\n",
    "        dst_node_identifier = \"dst\"\n",
    "\n",
    "        for edge_type, table in self._edge_tables.items():\n",
    "            edge_ref = BigqueryEdgeDataReference(\n",
    "                reference_uri=table,\n",
    "                edge_type=edge_type,\n",
    "                edge_usage_type=EdgeUsageType.MAIN,\n",
    "            )\n",
    "\n",
    "            feature_spec_fn = build_ingestion_feature_spec_fn(\n",
    "                fixed_int_fields=[\n",
    "                    src_node_identifier,\n",
    "                    dst_node_identifier,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # We don't need any special preprocessing for the edges as there are no edge features to begin with.\n",
    "            # Thus, we can make use of a \"passthrough\" transform preprocessing function that simply passes the input\n",
    "            # features through to the output features.\n",
    "            preprocessing_fn = build_passthrough_transform_preprocessing_fn()\n",
    "            edge_output_id = EdgeOutputIdentifier(\n",
    "                src_node=NodeOutputIdentifier(src_node_identifier),\n",
    "                dst_node=NodeOutputIdentifier(dst_node_identifier),\n",
    "            )\n",
    "\n",
    "            output_dict[edge_ref] = EdgeDataPreprocessingSpec(\n",
    "                identifier_output=edge_output_id,\n",
    "                feature_spec_fn=feature_spec_fn,\n",
    "                preprocessing_fn=preprocessing_fn,\n",
    "            )\n",
    "\n",
    "        return output_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11018373",
   "metadata": {},
   "source": [
    "Let's see what these fields look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data_preprocessor_config = ToyDataPreprocessorConfig(*data_preprocessor_args)\n",
    "toy_data_preprocessor_config.prepare_for_pipeline(JOB_NAME) # Doesn't do anything for the toy example\n",
    "node_preprocessing_spec = toy_data_preprocessor_config.get_nodes_preprocessing_spec()\n",
    "edge_preprocessing_spec = toy_data_preprocessor_config.get_edges_preprocessing_spec()\n",
    "\n",
    "print(\"\\n--Node Preprocessing Spec--\\n\")\n",
    "print(node_preprocessing_spec)\n",
    "print(\"\\n--Edge Preprocessing Spec--\\n\")\n",
    "print(edge_preprocessing_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd815c6e",
   "metadata": {},
   "source": [
    "As mentioned earlier, one of the purposes of the [DataPreprocessorConfig](https://snapchat.github.io/GiGL/docs/api/gigl/src/data_preprocessor/lib/data_preprocessor_config/index.html#gigl.src.data_preprocessor.lib.data_preprocessor_config.DataPreprocessorConfig) class is to provide a way to preprocess the feature data provided. In this example, we are importing the data from BiqQuery.\n",
    "\n",
    "You will note that in the above example, we are not doing anything special (i.e., no feature engineering), just reading from BQ and passing through the features. This is achieved by specifying the `build_passthrough_transform_preprocessing_fn` for both the node and edge features. This default option simply passes the features from the input to the enumerated output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f25c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from gigl.src.data_preprocessor.lib.types import (\n",
    "    TFTensorDict,\n",
    ")\n",
    "import tensorflow as tf\n",
    "\n",
    "example_tftensordict = {\n",
    "    \"age\": tf.constant([25, 32, 47], dtype=tf.float32),\n",
    "    \"income\": tf.constant([50000, 64000, 120000], dtype=tf.float32),\n",
    "    \"city\": tf.constant([\"New York\", \"Los Angeles\", \"New York\"], dtype=tf.string)\n",
    "}\n",
    "\n",
    "# Copy of the passthrough preprocessing function from gigl.src.data_preprocessor.data_preprocessor_config.build_passthrough_transform_preprocessing_fn for reference\n",
    "\n",
    "def build_passthrough_transform_preprocessing_fn() -> (\n",
    "    Callable[[TFTensorDict], TFTensorDict]\n",
    "):\n",
    "    \"\"\"\n",
    "    Produces a callable which acts as a pass-through preprocessing_fn for TFT to use.  In other words, it simply\n",
    "    passes all keys available in the input onwards to the output.\n",
    "\n",
    "    See https://www.tensorflow.org/tfx/tutorials/transform/census#create_a_tftransform_preprocessing_fn/ for details.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    def preprocessing_fn(inputs: TFTensorDict) -> TFTensorDict:\n",
    "        return inputs\n",
    "\n",
    "    return preprocessing_fn\n",
    "\n",
    "preprocessing_fn = build_passthrough_transform_preprocessing_fn()\n",
    "result = preprocessing_fn(example_tftensordict)\n",
    "\n",
    "print('\\n --Before Transformation--\\n')\n",
    "for key, feature in example_tftensordict.items():\n",
    "    print(f\"Feature Name: '{key}': {feature}\")\n",
    "\n",
    "print('\\n --After Transformation--\\n')\n",
    "for key, feature in result.items():\n",
    "    print(f\"Feature Name: '{key}': {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deef395",
   "metadata": {},
   "source": [
    "We could, if we wanted, define our own [preprocessing function](https://www.tensorflow.org/tfx/transform/get_started#preprocessing_function_example), and replace it with `build_passthrough_transform_preprocessing_fn()` defined in the code. This could accomplish any sort of preprocessing task we'd want, such as normalizing numerical features, handling `NULL` values, or creating numerical representations (such as 1-hot encoding) for categorical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_normalization_preprocessing_fn(numerical_features: list[str], categorical_features: list[str]) -> Callable[[TFTensorDict], TFTensorDict]:\n",
    "    \"\"\"\n",
    "    Produces a callable which applies z-normalization to specified numerical features and label encodings to categorical features.\n",
    "\n",
    "    Args:\n",
    "        numerical_features (list[str]): A list of names of the numerical features to apply z-normalization.\n",
    "        categorical_features (list[str]): A list of names of the categorical features to apply label encoding.\n",
    "    Returns:\n",
    "        Callable[[TFTensorDict], TFTensorDict]: A preprocessing function that applies transformations to the specified features.\n",
    "    \"\"\"\n",
    "\n",
    "    def preprocessing_fn(inputs: TFTensorDict) -> TFTensorDict:\n",
    "        outputs = inputs.copy()\n",
    "        for feature_name in numerical_features:\n",
    "            feat = inputs[feature_name]\n",
    "            # outputs[feature_name] = tft.scale_to_z_score(feat) \n",
    "            # This would be used normally as part of a Beam pipeline, but this is not compatible with our eager execution example, \n",
    "            # so we can calculate z-score manually below.\n",
    "            mean, variance = tf.nn.moments(feat, axes=0)\n",
    "            stddev = tf.sqrt(variance)\n",
    "            outputs[feature_name] = (feat - mean) / stddev\n",
    "        for feature_name in categorical_features:\n",
    "            feat = inputs[feature_name]\n",
    "            # Extract unique city names\n",
    "            unique_count, _ = tf.unique(feat)\n",
    "\n",
    "            # Create a lookup table mapping each unique city to a unique integer\n",
    "            lookup_table = tf.lookup.StaticHashTable(\n",
    "                initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "                    keys=unique_count,\n",
    "                    values=tf.range(tf.size(unique_count, out_type=tf.int64), dtype=tf.int64)\n",
    "                ),\n",
    "                default_value=-1  # This is the value returned for any key not in the table\n",
    "            )\n",
    "\n",
    "            # Use the lookup table to convert city names to integers\n",
    "            encoded_cities = lookup_table.lookup(feat)\n",
    "            outputs[feature_name] = tf.reshape(encoded_cities, tf.shape(feat))\n",
    "        return outputs\n",
    "    return preprocessing_fn\n",
    "\n",
    "preprocessing_fn = build_normalization_preprocessing_fn(numerical_features=[\"age\", \"income\"], categorical_features=[\"city\"])\n",
    "result = preprocessing_fn(example_tftensordict)\n",
    "\n",
    "print('\\n --Before Transformation--\\n')\n",
    "for key, feature in example_tftensordict.items():\n",
    "    print(f\"Feature Name: '{key}': {feature}\")\n",
    "    \n",
    "print('\\n --After Transformation--\\n')\n",
    "for key, feature in result.items():\n",
    "    print(f\"Feature Name: '{key}': {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d90ed5",
   "metadata": {},
   "source": [
    "### Running Data Preprocessor and visualizing the Preprocessed Metadata\n",
    "\n",
    "Now that we've built the preprocessor config, we'll now run the data preprocessor component using this config, which has been copied over to [preprocessor_config.py](./preprocessor_config.py) and specified in the task_config in the `datasetConfig.dataPreprocessorConfig.dataPreprocessorConfigClsPath`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514815a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WARN: There is an issue when trying to run dataflow jobs from inside a jupyter kernel; thus we cannot use the line\", \n",
    "below to run the preprocessor as you would normally in a python script. \n",
    "\n",
    "runner.run_data_preprocessor(pipeline_config=pipeline_config)\n",
    "\n",
    "Instead, we will run the preprocessor from the command line.\n",
    "Note: You can actually do this with every component; we just make use of the runner to make it easier to run the components.\n",
    "\"\"\"\n",
    "!python -m gigl.src.data_preprocessor.data_preprocessor \\\n",
    "--job_name=\"$JOB_NAME\" \\\n",
    "--task_config_uri=\"$FROZEN_TASK_CONFIG_PATH\" \\\n",
    "--resource_config_uri=\"$RESOURCE_CONFIG_PATH\" \\\n",
    "--custom_worker_image_uri=\"$DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da898e",
   "metadata": {},
   "source": [
    "Upon completion of job, we will see the preprocessed metadata be populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fecd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_task_config = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(\n",
    "    gbml_config_uri=FROZEN_TASK_CONFIG_PATH\n",
    ")\n",
    "preprocessed_metadata_pb = frozen_task_config.preprocessed_metadata_pb_wrapper.preprocessed_metadata_pb\n",
    "print(f\"\\n--Preprocessed Metadata--\\n{preprocessed_metadata_pb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc20f60",
   "metadata": {},
   "source": [
    "You do not have to worry about these details in code as it is all handled by the data preprocessor component and subsequent data loaders. We can observe the uploaded TFRecord files\n",
    "for each node and edge type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea7934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for condensed_node_type in preprocessed_metadata_pb.condensed_node_type_to_preprocessed_metadata:\n",
    "    node_type = frozen_task_config.graph_metadata.condensed_node_type_map[condensed_node_type]\n",
    "    preprocessed_nodes = preprocessed_metadata_pb.condensed_node_type_to_preprocessed_metadata[condensed_node_type].tfrecord_uri_prefix\n",
    "    print(f\"\\nPreprocessed nodes for node type {node_type} are stored in: \\n\")\n",
    "    print(file_loader.list_children(UriFactory.create_uri(preprocessed_nodes)))\n",
    "\n",
    "for condensed_edge_type in preprocessed_metadata_pb.condensed_edge_type_to_preprocessed_metadata:\n",
    "    edge_type = frozen_task_config.graph_metadata.condensed_edge_type_map[condensed_edge_type]  \n",
    "    preprocessed_edges = preprocessed_metadata_pb.condensed_edge_type_to_preprocessed_metadata[condensed_edge_type].main_edge_info.tfrecord_uri_prefix\n",
    "    print(f\"\\nPreprocessed edges for edge type {edge_type} are stored in: \\n\")\n",
    "    print(file_loader.list_children(UriFactory.create_uri(preprocessed_edges)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0a6b4",
   "metadata": {},
   "source": [
    "There is not a lot of data so we will have just generated one file for each of the preprocessed node and edge types.\n",
    "\n",
    "\n",
    "We can also observe the enumerated node ID tables. Since the node IDs were already integers between 0, 1, ... N for each node type, the values for the `node_id` (original column) and `int_id` (enumerated column) will be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "for condensed_node_type in preprocessed_metadata_pb.condensed_node_type_to_preprocessed_metadata:\n",
    "    node_type = frozen_task_config.graph_metadata.condensed_node_type_map[condensed_node_type]\n",
    "    table_bq_path = preprocessed_metadata_pb.condensed_node_type_to_preprocessed_metadata[condensed_node_type].enumerated_node_ids_bq_table\n",
    "    print(f\"\\n Displaying rows in {table_bq_path} for node type {node_type}: \\n\")\n",
    "    display_table(table_bq_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28e5af9",
   "metadata": {},
   "source": [
    "# Trainer\n",
    "\n",
    "Now that our data is preprocessed, transformed, and uploaded to GCS as TFRecords, we will use the data for distributed training and inference. First, we'll walk through how to build a distributed dataset from some frozen task config, load a sampled subgraph from the graph, and run a forward/backward pass on this subgraph.\n",
    "\n",
    "### Anchor Based Link Prediction\n",
    "\n",
    "For this example, we will be using an Anchor Based Link Prediction (ABLP) training objective. With ABLP, we learn the model parameters by trying to minimize the difference between the model's outputs (embeddings) for each node, as opposed to standard Link Prediction(LP), where we try to predict if a link exists in the graph. We find that ABLP is a superior objective for learning embeddings for all nodes in the graph.\n",
    "\n",
    "![NABLP example](../../../docs/assets//images/ablp.png) \n",
    "\n",
    "In the above image, we are trying to minimize the distance between the anchor nodes and positive nodes embeddings, and maximize the distance between anchor nodes and the negative nodes. By minimizng the difference between embeddings between two nodes, you can later use KNN or similar to get recommendations for a given item (ex. user).\n",
    "\n",
    "With ABLP, you can train in either a supervised or a self-supervised manner. In the supervised approach, you provide your own labels, such as positive and negative edges. In the self-supervised approach, edges in the graph can be selected as positive labels. In either case, you can also use \"random negatives,\" which are random nodes in the graph used as negative labels.\n",
    "\n",
    "In the following cell, we will introduce necesary classes and functions from `gigl.distributed` to run training and inference:\n",
    "\n",
    "### [`DistLinkPredictionDataset`](https://snapchat.github.io/GiGL/docs/api/gigl/distributed/dist_link_prediction_dataset/index.html#gigl.distributed.dist_link_prediction_dataset.DistLinkPredictionDataset) \n",
    "`DistLinkPredictionDataset`s are objects for storing a distributed graph. They contain both data that exists on their own machine, e.g. `node_ids`, which are all nodes on a given machine, and information about which machine in the cluster stores a information about a given node or edge.\n",
    "\n",
    "The information about which machines stores what is called a *partition book* (shortened to `pb` in code).\n",
    "\n",
    "Note that the fields of the dataset may either be a `Foo` or a `dict[NodeType | EdgeType, Foo]`, depending on if the graph is homogeneous or heteregeneous, respectively.\n",
    "\n",
    "Note that for *all* ABLP tasks, we treat the labels as an edge type, so all ABLP datasets are heterogeneous. We provide the [`gigl.types.graph.to_homogeneous`](https://snapchat.github.io/GiGL/docs/api/gigl/types/graph/index.html#gigl.types.graph.to_homogeneous) to convert `{FOO: Tensor([1, 2])} -> Tensor([1, 2])` in a way that satisfies MyPy.\n",
    "\n",
    "\n",
    "### [`build_dataset_from_task_config_uri`](https://snapchat.github.io/GiGL/docs/api/gigl/distributed/index.html#gigl.distributed.build_dataset_from_task_config_uri)\n",
    "\n",
    "`build_dataset_from_task_config_uri` returns an initialized dataset from a provide task config URI. The task config *must* have the `SharedConfig` field populated (e.g. it was produced by `config-populator`).\n",
    "\n",
    "The function does the following:\n",
    "\n",
    "1. Parses some parameteres from the task config, like `edge_dir` - full documentation is in the docstring of the function.\n",
    "2. Spawns a subprocess that:\n",
    "    1. Reads the dataset from GCS, with each machine reading a distinct subset\n",
    "    2. Partitions the data across the cluster\n",
    "    3. (Optionally, if training) builds train, val, test splits\n",
    "    4. Returns the `DistLinkPredictionDataset`\n",
    "\n",
    "NOTE: This function *requires* a `torch.distributed` process group to exist.\n",
    "\n",
    "### [`DistNeighborLoader`](https://snapchat.github.io/GiGL/docs/api/gigl/distributed/index.html#gigl.distributed.DistNeighborLoader)\n",
    "\n",
    "GiGL implementation of a GLT [DistNeighborLoader](https://github.com/alibaba/graphlearn-for-pytorch/blob/26fe3d4e050b081bc51a79dc9547f244f5d314da/graphlearn_torch/python/distributed/dist_neighbor_loader.py#L29) which in turn is a distributed implementation of a PyG [NeighborLoader](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/loader/neighbor_loader.html) (which, in turn, is an implementation of a Torch [DataLoader](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for working with graphs).\n",
    "\n",
    "\n",
    "The neighborloader should function fairly similarly to the GLT implementation, with one key difference: *the input nodes will get sharded across process on a given machine*. e.g. if you have `input_nodes: [0, 1, 2, 3]` and there are two processes on a given machine, then the first process will *use* `input_node: [0, 1]` and the second process will use `input_nodes: [2, 3]`.\n",
    "\n",
    "The *output* `Data` or `HeteroData` object will have the additional fields added:\n",
    "\n",
    "* `num_sampled_nodes` - 1d tensor of the number of sampled nodes, per hop\n",
    "* `num_sampled_edges` - 1d tensor of the number of sampled edges, per hop\n",
    "\n",
    "### [`DistABLPLoader`](https://snapchat.github.io/GiGL/docs/api/gigl/distributed/dist_ablp_neighborloader/index.html#gigl.distributed.dist_ablp_neighborloader.DistABLPLoader)\n",
    "\n",
    "Neighborloader implementation for ABLP tasks. This is very similar to the `DistNeighborLoader` but required the additional `supervision_edge_type` arguement for which edge type in the graph is being trained on.\n",
    "\n",
    "The *output* `HeteroData` object will have the additional fields from `DistNeighborLoader` and:\n",
    "\n",
    "* `y_positive` - dict of `anchor node id` -> `tensor(positive label node ids)`\n",
    "* `y_negative` - optional dict of `anchor node id` -> `tensor(negative label node ids)`\n",
    "\n",
    "### Example output.\n",
    "\n",
    "Let's take the below [graph](https://is.gd/a8DK15), with sampled node `0`, and fanout [2, 2].\n",
    "\n",
    "```\n",
    "0 -> 1 [label=\"Positive example\" color=\"green\"]\n",
    "0 -> 2 [label=\"Negative example\" color=\"red\"]\n",
    "\n",
    "0 -> {3, 4}\n",
    "3 -> {5, 6}\n",
    "4 -> {7, 8}\n",
    "\n",
    "1 -> 9\n",
    "2 -> 10\n",
    "```\n",
    "\n",
    "The returned object from a `DistABLPLoader will have the following fields:\n",
    "\n",
    "- `y_positive`: {0: torch.tensor([1])} - 1 is the only positive label for node 0\n",
    "- `y_negative`: {0: torch.tensor([2])} - 2 is the only negative label for node 0\n",
    "- `num_sampled_nodes`: [\n",
    "\n",
    "    3,  - [0, 1, 2]\n",
    "\n",
    "    4,  - [3, 4, 9, 10]\n",
    "\n",
    "    4,  - [5, 6, 7, 8]\n",
    "    \n",
    "]\n",
    "\n",
    "- `num_sampled_edges`: [\n",
    "\n",
    "    4, - [0 -> 3, 0 -> 4. 1 -> 9, 2 -> 10]\n",
    "\n",
    "    4, - [3 -> 5, 3 -> 6, 4 -> 7, 4 -> 8]\n",
    "\n",
    "]\n",
    "\n",
    "Note that `num_sampled_nodes` always has one more item in it as opposed to `num_sampled_edges`, as it includes the root nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf042984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a simple forward/backward pass of the model\n",
    "\n",
    "# First, we need to load the dataset\n",
    "import torch\n",
    "\n",
    "from gigl.distributed import (\n",
    "    DistLinkPredictionDataset,\n",
    "    build_dataset_from_task_config_uri,\n",
    ")\n",
    "# GiGL is meant to operate in a very large distributed setting, so we need to initialize the process group.\n",
    "torch.distributed.init_process_group(\n",
    "    backend=\"gloo\",  # Use the Gloo backend for CPU training.\n",
    "    init_method=\"tcp://localhost:29500\",\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    ")\n",
    "\n",
    "# `build_dataset_from_task_config_uri` is a utility function\n",
    "# to build a dataset in a distributed manner.\n",
    "# It will:\n",
    "# 1. Read the serialized graph data whose located is specified in the task config.\n",
    "# 2. Load the graph data in a distributed manner.\n",
    "# 3. Partition the graph data into shards for distributed training.\n",
    "# 4. Optional: If training, will generate splits for training.\n",
    "dataset: DistLinkPredictionDataset = build_dataset_from_task_config_uri(\n",
    "        task_config_uri=FROZEN_TASK_CONFIG_PATH,\n",
    "        is_inference=False,\n",
    "        _tfrecord_uri_pattern=\".*tfrecord\", # Our example data uses a different tfrecord pattern.\n",
    ")\n",
    "\n",
    "# And instantiate a dataloader:\n",
    "from gigl.distributed import DistABLPLoader\n",
    "\n",
    "loader = DistABLPLoader(\n",
    "            dataset=dataset,\n",
    "            num_neighbors=[2, 2],  # Example neighbor sampling configuration.\n",
    "            input_nodes=(\"user\", torch.tensor([0])),  # Example input nodes, adjust as needed.\n",
    "            batch_size=1,\n",
    "            supervision_edge_type=(\"user\", \"to\", \"story\"),  # Supervision edge type defined in the graph.\n",
    "            pin_memory_device=torch.device(\n",
    "                \"cpu\"\n",
    "            ),  # Only CPU training for this example.\n",
    "        )\n",
    "data: HeteroData = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at the data we just loaded.\n",
    "print(data)\n",
    "\n",
    "# You might notice a few things about the data that is different from vanilla PyG:\n",
    "# * num_sampled_nodes and num_sampled_edges are present,\n",
    "# * representing the number of nodes and edges sampled per hop.\n",
    "# * y_positive is added, and is a dict of anchor node -> target nodes.\n",
    "\n",
    "GraphVisualizer.visualize_graph(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f22a36",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "The HGTConv architecture is a standard out-of-the-box architecture for modeling heterogeneous graphs with GNNs.  It combines three components:\n",
    "\n",
    "1. Edge-type-based-attention, which defines edge-type-specific projections when computing multi-head attention between adjacent nodes\n",
    "2. Heterogeneous message passing, which uses node and edge-type specific projections before passing messages from source nodes to target nodes\n",
    "3.  Mode-type-specific projection to map the aggregation result back to the target node's type. \n",
    "\n",
    "All these aspects allow very expressive convolutions to be defined which are highly specialized to each node and edge-type and its impact on the objective.  Of course, there are other architectures we could choose which are popular in literature, many of which are available in PyG. Since our goal is not to focus on the specific model here, but rather demonstrate extensibility, we choose this one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a model and do a forward pass\n",
    "# You can interop with any PyG model, but we will use HGTConv for this example.\n",
    "from torch_geometric.nn import HGTConv\n",
    "\n",
    "model = HGTConv(\n",
    "    in_channels=data.num_node_features,\n",
    "    out_channels=16,  # Example output dimension.\n",
    "    metadata=data.metadata(),\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "\n",
    "# Do a forward pass\n",
    "embeddings = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "print(f\"Embeddings: {embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ed2d8",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "GNNs are commonly used to solve link prediction problems by having a GNN model predict node embeddings on a node-level (called the encoder), and training it with a scoring function on an edge-level (called the decoder).  The decoder takes in a pair of node embeddings and outputs a scalar indicating similarity; some common choices are a cosine similarity, a dot product, or an MLP on the concatenated node embeddings.  The encoder and decoder are trained using a constrative loss, which contrasts the similarities of positive samples (edges) to negative samples (non-edges) in the graph.  The positives and negatives can more generally be arbitrary pairs which are custom-tailored to the modeler's interests.\n",
    "\n",
    "Here, we use one common contrastive loss called the margin loss, which is implemented in PyTorch as nn.MarginRankingLoss.  Margin loss is applied to triplets of samples of the form (anchor, positive, negative), and defines a loss function which is sensitive to the \"margin\" of similarity between (anchor, positive) and (anchor, negative) -- namely, the loss is 0 if the model can differentiate the two triplet properly but grows linearly with the gap between the two scores if not.  This encourages the model to learn a strong boundary with large margins between positive and negative edges to minimize loss.   Other loss functions can also be used to train this model, including hinge loss, cross-entropy loss, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d89e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's define a loss function for the link prediction task.\n",
    "\n",
    "def compute_loss(model: torch.nn.Module, data: HeteroData) -> torch.Tensor:\n",
    "    main_out: dict[str, torch.Tensor] = model(data.x_dict, data.edge_index_dict)\n",
    "    # data.y_positive = {\n",
    "    #   0: [1, 2],\n",
    "    #   1: [3, 4, 5],\n",
    "    # }\n",
    "    anchor_nodes = torch.arange(data[\"user\"].batch_size).repeat_interleave(\n",
    "        torch.tensor([len(v) for v in data.y_positive.values()])\n",
    "    )\n",
    "    # anchor_nodes = [0, 0, 1, 1, 1]\n",
    "    target_nodes = torch.cat([v for v in data.y_positive.values()])\n",
    "    # target_nodes = [1, 2, 3, 4, 5]\n",
    "    # Use MarginRankingLoss for link prediction\n",
    "    loss_fn = torch.nn.MarginRankingLoss()\n",
    "    query_embeddings = main_out[\"user\"][anchor_nodes]\n",
    "    target_embeddings = main_out[\"story\"][target_nodes]\n",
    "    loss = loss_fn(\n",
    "        input1=query_embeddings,\n",
    "        input2=target_embeddings,\n",
    "        target=torch.ones_like(query_embeddings, dtype=torch.float32),\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "# Note that in practice you would want to wrap this in a training loop\n",
    "# but for this example doing just one pass is sufficient.\n",
    "# A training loop example can be found in:\n",
    "# examples/tutorial/KDD_2025/heterogeneous_training.py\n",
    "loss = compute_loss(model, data)\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# And we can do a backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ee03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now if we run the loss function again, we should see a different value.\n",
    "loss = compute_loss(model, data)\n",
    "print(f\"Loss after backward pass: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've \"Trained\" the model, let's do inference on it.\n",
    "# Note that inference is very similar to training, but we don't need to do a backward pass.\n",
    "# And we should use the `DistNeighborloader` instead of `DistABLPLoader`\n",
    "from gigl.distributed import DistNeighborLoader\n",
    "inference_loader = DistNeighborLoader(\n",
    "    dataset=dataset,\n",
    "    num_neighbors=[2, 2],  # Example neighbor sampling configuration.\n",
    "    input_nodes=(\"user\", torch.tensor([0, 1, 2, 3])),  # Run inference against some of the nodes. In a custom datasets you would use `dataset.node_ids['user']`.\n",
    "    batch_size=1,\n",
    "    pin_memory_device=torch.device(\"cpu\"),  # Only CPU training for this example.\n",
    ")\n",
    "\n",
    "# GiGL has an \"EmbeddingExporter\" to write out the embeddings to disk or GCS. We export embeddings to a local file in this example. \n",
    "from gigl.common import UriFactory\n",
    "from gigl.common.data.export import EmbeddingExporter\n",
    "\n",
    "# Use a local directory for exporting embeddings.\n",
    "# You can also use a GCS URI if you want to export to GCS.\n",
    "# For example, use \"gs://your-bucket-name/path/to/embeddings\".\n",
    "embedding_dir = UriFactory.create_uri(\"examples/tutorial/KDD_2025/.embeddings\")\n",
    "\n",
    "exporter = EmbeddingExporter(\n",
    "    export_dir=embedding_dir,\n",
    ")\n",
    "with torch.no_grad():\n",
    "    for data in inference_loader:\n",
    "        embeddings = model(data.x_dict, data.edge_index_dict)\n",
    "        exporter.add_embedding(\n",
    "            id_batch=data[\"user\"].batch,\n",
    "            embedding_batch=embeddings[\"user\"],\n",
    "            embedding_type=\"user\"\n",
    "        )\n",
    "    exporter.flush_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d08cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the exported embeddings\n",
    "!ls examples/tutorial/KDD_2025/.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the embeddings from disk and load them into a dataframe.\n",
    "from pathlib import Path\n",
    "import fastavro\n",
    "\n",
    "avro_records = []\n",
    "for file in Path(\"examples/tutorial/KDD_2025/.embeddings\").glob(\"*.avro\"):\n",
    "    with open(file, \"rb\") as f:\n",
    "        reader = fastavro.reader(f)\n",
    "        for record in reader:\n",
    "            avro_records.append(record)\n",
    "print(f\"Loaded {len(avro_records)} records from the avro files.\")\n",
    "print(f\"First record: {avro_records[0]}\")\n",
    "\n",
    "# And load them into a dataframe.\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_records(avro_records)\n",
    "print(f\"Dataframe:\\n{df}\")\n",
    "# GiGL also has gigl.common.data.export.load_embeddings_to_bigquery\n",
    "# Which you can use to load the embeddings into BigQuery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb95fce",
   "metadata": {},
   "source": [
    "## Training and inferencing on Vertex AI\n",
    "GiGL supports launching training and inference loops on [Vertex AI](https://cloud.google.com/vertex-ai?hl=en) in order to handle gigantic graphs in a distributed fashion, with access to accelerators.\n",
    "\n",
    "We have a local pipeline [Runner](https://snapchat.github.io/GiGL/docs/api/gigl/orchestration/local/runner/index.html#gigl.orchestration.local.runner.Runner), which can instantiate Vertex AI jobs, let's set it up!\n",
    "\n",
    "The runner is a convenience wrapper over our components, e.g. [Trainer](https://snapchat.github.io/GiGL/docs/api/gigl/src/training/trainer/index.html#gigl.src.training.trainer.Trainer) and [Inferencer](https://snapchat.github.io/GiGL/docs/api/gigl/src/inference/inferencer/index.html#gigl.src.inference.inferencer.Inferencer).\n",
    "\n",
    "At a high level, these components do the following:\n",
    "\n",
    "1. [Initialize](http://snapchat.github.io/GiGL/docs/api/gigl/src/common/utils/metrics_service_provider/index.html#gigl.src.common.utils.metrics_service_provider.initialize_metrics) metrics so that the component costs/etc can be tracked.\n",
    "2. Clear out the output GCS directories, e.g. what is stored in [`TrainedModelMetadata.trained_model_uri`](https://github.com/Snapchat/GiGL/blob/eaf98ee86006c7b3dff7b598fdd3657a5a993519/proto/snapchat/research/gbml/trained_model_metadata.proto#L7), so that any component re-runs will be \"fresh\". This is done to ensure the idempotency of GiGL.\n",
    "3. Run the user-defined logic on Vertex AI. In this case, the logic is contained in [heterogeneous_training.py](./heterogeneous_training.py) and [heterogeneous_inference.py](./heterogeneous_inference.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.orchestration.local.runner import Runner, PipelineConfig\n",
    "\n",
    "config = PipelineConfig(\n",
    "    applied_task_identifier=JOB_NAME,\n",
    "    task_config_uri=FROZEN_TASK_CONFIG_PATH,\n",
    "    resource_config_uri=RESOURCE_CONFIG_PATH,\n",
    "    custom_cpu_docker_uri=DEFAULT_GIGL_RELEASE_SRC_IMAGE_CPU,\n",
    "    custom_cuda_docker_uri=DEFAULT_GIGL_RELEASE_SRC_IMAGE_CUDA,\n",
    ")\n",
    "\n",
    "runner = Runner()\n",
    "\n",
    "# Now launch a training job.\n",
    "runner.run_trainer(config)\n",
    "\n",
    "# You should see some log like:\n",
    "# View Custom Job:\n",
    "# https://console.cloud.google.com/ai/platform/locations/us-central1/training/<job_id>?project=<project_id>\n",
    "# Which will link to the Vertex AI job that was launched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e0298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can run inference on the model.\n",
    "runner.run_inferencer(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400cae3b",
   "metadata": {},
   "source": [
    "## Vertex AI Pipelines\n",
    "\n",
    "You can also run GiGL pipelines on [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction). Doing so provides a number of advantages, such as:\n",
    "\n",
    "1. Decoupling orchestration from dev machines - when you close your laptop the pipeline keeps running\n",
    "2. Orchestrate new runs from GCP console. You can start/stop runs on the console\n",
    "3. Data isolation - Use dedicated Service Accounts and Projects to access sensitive data\n",
    "\n",
    "Vertex AI Pipelines can run [KFP pipelines](https://www.kubeflow.org/docs/components/pipelines/overview/), which from the KFP website: \n",
    "\n",
    "> Kubeflow Pipelines (KFP) is a platform for building and deploying portable and scalable machine learning (ML) workflows using containers on Kubernetes-based systems.\n",
    "\n",
    "GiGL packages it's components (Data Preprocessor, Trainer, etc) together as a KFP pipeline so that they can all be orchestrated together on Vertex AI.\n",
    "\n",
    "GiGL has a [`KfpOrchestrator`](https://snapchat.github.io/GiGL/docs/api/gigl/orchestration/kubeflow/kfp_orchestrator/index.html#gigl.orchestration.kubeflow.kfp_orchestrator.KfpOrchestrator) client which will let you launch GiGL pipelines on Vertex AI.\n",
    "\n",
    "We need to call two methods on the client to start a new pipeline run:\n",
    "\n",
    "1. [`compile`](https://snapchat.github.io/GiGL/_modules/gigl/orchestration/kubeflow/kfp_orchestrator.html#KfpOrchestrator.compile), which generates a KFP Pipeline spec that Vertex AI can consume to orchestrate the pipeline. Under the hood, we write the pipeline yaml to some local file, but you can configure this with the `dst_compiled_pipeline_path` argument.\n",
    "2. [`run`](https://snapchat.github.io/GiGL/_modules/gigl/orchestration/kubeflow/kfp_orchestrator.html#KfpOrchestrator.run) which uploads the pipeline spec to Vertex AI and starts a new pipeline job. Note that `run` requires an applied_task_identifier (pipeline job name), which must be unique in every Project and Region.\n",
    "\n",
    "Note that `compile` requires docker images to be passed in, this is because the docker images that are used by each component are baked into the pipeline definition. For this tutorial, we will be using the default images, however **if you have made code or local config changes**, you will need to provide your own docker images. GiGL has a handy [`script`](https://github.com/Snapchat/GiGL/blob/main/scripts/build_and_push_docker_image.py) will push the docker images for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also run the entire GiGL pipeline on Vertex AI\n",
    "# With the KfpOrchestrator [1]\n",
    "# Note that we use TEMPLATE_TASK_CONFIG_PATH here so we can have another \"dataset\" e.g. SharedConfig.\n",
    "# [1]: https://snapchat.github.io/GiGL/docs/api/gigl/orchestration/kubeflow/kfp_orchestrator/index.html#gigl.orchestration.kubeflow.kfp_orchestrator.KfpOrchestrator\n",
    "from gigl.orchestration.kubeflow.kfp_orchestrator import KfpOrchestrator\n",
    "\n",
    "orchestrator = KfpOrchestrator()\n",
    "# First, compile the KFP pipeline definition\n",
    "orchestrator.compile(\n",
    "    cuda_container_image=DEFAULT_GIGL_RELEASE_SRC_IMAGE_CUDA,\n",
    "    cpu_container_image=DEFAULT_GIGL_RELEASE_SRC_IMAGE_CPU,\n",
    "    dataflow_container_image=DEFAULT_GIGL_RELEASE_SRC_IMAGE_DATAFLOW_CPU,\n",
    ")\n",
    "# Then, run it.\n",
    "orchestrator.run(\n",
    "    applied_task_identifier=JOB_NAME,\n",
    "    task_config_uri=TEMPLATE_TASK_CONFIG_PATH,\n",
    "    resource_config_uri=RESOURCE_CONFIG_PATH,\n",
    "    stop_after=\"trainer\"\n",
    ")\n",
    "\n",
    "# You'll eventually see a link to the KFP pipeline in the logs.\n",
    "# Something like: \n",
    "# https://console.cloud.google.com/vertex-ai/pipelines/locations/us-central1/runs/<job_id>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cbe60f",
   "metadata": {},
   "source": [
    "## GiGL with custom code and configs\n",
    "Until now - we have been using the default GiGL Docker images, which work fine as our examples are pre-baked into the default GiGL images,\n",
    "Which are defined at https://github.com/Snapchat/GiGL/blob/main/dep_vars.env\n",
    "\n",
    "However, if you make any changes to the code or local configs, you will need to re-build the docker images so your changes are reflected in the jobs.\n",
    "\n",
    "GiGL provides the [`build_and_push_docker_image.py`](https://github.com/Snapchat/GiGL/blob/main/scripts/build_and_push_docker_image.py) script which will push docker images with the code as it is stands in the local repository. This script can also be used programmatically, which we do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31552143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building docker images.\n",
    "# Until now - we have been using the default GiGL docker images\n",
    "\n",
    "# In order for your code changes to be reflected in the docker images, you can build your own docker images.\n",
    "# We have a script that will build the docker images for you.\n",
    "from scripts.build_and_push_docker_image import build_and_push_cpu_image, build_and_push_cuda_image, build_and_push_dataflow_image\n",
    "\n",
    "repository = \"gigl-base-images\"\n",
    "tag = f\"{getpass.getuser()}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "notebook_cpu_image = f\"us-central1-docker.pkg.dev/{resource_config.project}/{repository}/src-cuda:{tag}\"\n",
    "notebook_cuda_image = f\"us-central1-docker.pkg.dev/{resource_config.project}/{repository}/src-cuda:{tag}\"\n",
    "notebook_dataflow_image = f\"us-central1-docker.pkg.dev/{resource_config.project}/{repository}/src-dataflow:{tag}\"\n",
    "build_and_push_cpu_image(notebook_cpu_image)\n",
    "build_and_push_cuda_image(notebook_cuda_image)\n",
    "build_and_push_dataflow_image(notebook_dataflow_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can run a custom pipeline with these new images :)\n",
    "# First, compile the KFP pipeline definition\n",
    "orchestrator.compile(\n",
    "    cuda_container_image=notebook_cuda_image,\n",
    "    cpu_container_image=notebook_cpu_image,\n",
    "    dataflow_container_image=notebook_dataflow_image,\n",
    ")\n",
    "# Then, run it.\n",
    "orchestrator.run(\n",
    "    applied_task_identifier=f\"{getpass.getuser()}_gigl_toy_custom_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    task_config_uri=TEMPLATE_TASK_CONFIG_PATH,\n",
    "    resource_config_uri=RESOURCE_CONFIG_PATH,\n",
    "    stop_after=\"trainer\"\n",
    ")\n",
    "# You'll eventually see a link to the KFP pipeline in the logs.\n",
    "# Something like: \n",
    "# https://console.cloud.google.com/vertex-ai/pipelines/locations/us-central1/runs/<job_id>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bagl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

shared_resource_config:
  # Resource labels are just compute labels that should be attached to all compute resources spun up by GiGL.
  # So a practitioner can have a more fine grained understanding of resource utilization and cost of the resources in their GCP billing.
  # Read more here: https://cloud.google.com/compute/docs/labeling-resources#what-are-labels
  resource_labels:
  # We have a 63 character limit for cost_resource_group_tag.

  # COMPONENT is one of {pre|sgs|spl|tra|inf|pos} standing for:
  #   {Preprocessor | Subgraph Sampler | Split Generator | Trainer | Inference
  #   | Post  Processor} so we can get more accurate cost measurements
  #   of each component. This will be automatically filled in code.
    cost_resource_group_tag: dev_experiments_COMPONENT
    cost_resource_group: gigl_platform
  common_compute_config:
    project: "TODO: Update project name"
    region: "TODO: Update region"
    temp_assets_bucket: "TODO: Update temp_assets_bucket"
    temp_regional_assets_bucket: "TODO: Update temp_regional_assets_bucket"
    perm_assets_bucket: "TODO: Update perm_assets_bucket"
    temp_assets_bq_dataset_name: "TODO: Update temp_assets_bq_dataset_name"
    embedding_bq_dataset_name: "TODO: Update embedding_bq_dataset_name"
    gcp_service_account_email: "TODO: Update gcp_service_account_email"
    dataflow_runner: "DataflowRunner"
preprocessor_config:
  edge_preprocessor_config:
    num_workers: 1
    max_num_workers: 2
    machine_type: "n2-standard-16"
    disk_size_gb: 300
  node_preprocessor_config:
    num_workers: 1
    max_num_workers: 2
    machine_type: "n2-standard-16"
    disk_size_gb: 300
trainer_resource_config:
  vertex_ai_trainer_config:
    machine_type: "n1-highmem-8"
    gpu_type: nvidia-tesla-p100  # set to `ACCELERATOR_TYPE_UNSPECIFIED` for cpu training
    gpu_limit: 1  # set to 0 for cpu training
    num_replicas: 2
inferencer_resource_config:
  vertex_ai_inferencer_config:
    machine_type: "n1-highmem-8"
    gpu_type: nvidia-tesla-p100  # set to `ACCELERATOR_TYPE_UNSPECIFIED` for cpu inference
    gpu_limit: 1  # set to 0 for cpu inference
    num_replicas: 2

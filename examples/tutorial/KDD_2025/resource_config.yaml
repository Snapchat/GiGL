shared_resource_config:
  # Resource labels are just compute labels that should be attached to all compute resources spun up by GiGL.
  # So a practitioner can have a more fine grained understanding of resource utilization and cost of the resources in their GCP billing.
  # Read more here: https://cloud.google.com/compute/docs/labeling-resources#what-are-labels
  resource_labels:
  # We have a 63 character limit for cost_resource_group_tag.

  # COMPONENT is one of {pre|tra|inf|pos} standing for:
  #   {Preprocessor | Trainer | Inference | Post  Processor} so we can get more accurate cost measurements
  #   of each component. This will be automatically filled in code.
    cost_resource_group_tag: dev_experiments_COMPONENT
    cost_resource_group: gigl_platform
  common_compute_config:
    # TODO: Update these fields for the tutorial
    project: "external-snap-ci-github-gigl"
    region: "us-central1"
    temp_assets_bucket: "gs://gigl-cicd-temp"
    temp_regional_assets_bucket: "gs://gigl-cicd-temp"
    perm_assets_bucket: "gs://gigl-cicd-temp"
    temp_assets_bq_dataset_name: "gigl_temp_assets"
    embedding_bq_dataset_name: "gigl_temp_assets"
    gcp_service_account_email: "untrusted-external-github-gigl@external-snap-ci-github-gigl.iam.gserviceaccount.com"
    dataflow_runner: "DataflowRunner"
preprocessor_config:
  edge_preprocessor_config:
    num_workers: 1
    max_num_workers: 2
    machine_type: "n2-standard-16"
    disk_size_gb: 300
  node_preprocessor_config:
    num_workers: 1
    max_num_workers: 2
    machine_type: "n2-standard-16"
    disk_size_gb: 300
trainer_resource_config:
  vertex_ai_trainer_config:
    machine_type: "n1-highmem-8"
    gpu_type: nvidia-tesla-p100  # set to `ACCELERATOR_TYPE_UNSPECIFIED` for cpu training
    gpu_limit: 1  # set to 0 for cpu training
    num_replicas: 2
inferencer_resource_config:
  vertex_ai_inferencer_config:
    machine_type: "n1-highmem-8"
    gpu_type: nvidia-tesla-p100  # set to `ACCELERATOR_TYPE_UNSPECIFIED` for cpu inference
    gpu_limit: 1  # set to 0 for cpu inference
    num_replicas: 2

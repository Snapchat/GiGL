# Template config for an example heterogeneous graph.

# Note that you should run this template config through the config_populator
# component to generate a frozen config, which contains the required fields for where assets will be written. You can do this as
# part of an E2E VAI pipeline or by calling the config populator directly by running:
# python -m \
#    gigl.src.config_populator.config_populator \
#    --job_name="$JOB_NAME" \
#    --template_uri="$TEMPLATE_TASK_CONFIG_PATH" \
#    --resource_config_uri="$RESOURCE_CONFIG_PATH" \
#    --output_file_path_frozen_gbml_config_uri="$FROZEN_TASK_CONFIG_POINTER_FILE_PATH"

# ========
# TaskMetadata:
# Specifies the task we are going to perform on the graph.
taskMetadata:
  nodeAnchorBasedLinkPredictionTaskMetadata:
    # Specifying that we will perform node anchor based link prediction on edge of type: user -> to -> story
    supervisionEdgeTypes:
    - dstNodeType: story
      relation: to
      srcNodeType: user
graphMetadata:
  # We have 2 nodes types in the toy dataset: user and story. We also have 2
  # edge types: user -> to -> story and story -> to -> user
  edgeTypes:
  - dstNodeType: user
    relation: to
    srcNodeType: story
  - dstNodeType: story
    relation: to
    srcNodeType: user
  nodeTypes:
  - user
  - story
# ========
# SharedConfig:
# Specifies some extra metadata about the graph structure management of orchestration.
sharedConfig:
  shouldSkipAutomaticTempAssetCleanup: true # Should we skip cleaning up the temporary assets after the run is complete?
# ========
# DatasetConfig:
# Specifies information about the dataset, such as how to access it and how to process it
datasetConfig:
  dataPreprocessorConfig:
    dataPreprocessorConfigClsPath: examples.tutorial.KDD_2025.preprocessor_config.ToyDataPreprocessorConfig
    # our implementation takes no runtime arguments; if provided these are passed to the constructor off dataPreprocessorConfigClsPath
    # dataPreprocessorArgs:
# ========
# TrainerConfig:
# Specifies the training configuration. This includes the trainer command and the arguments to pass to it.
trainerConfig:
  trainerArgs:
    # Example argument to trainer
    log_every_n_batch: "50"
    local_saved_model: "False"  # Whether to use a local saved model instead of a remote URI.
    ssl_positive_label_percentage: "0.05" # We'll select 5% of edges as positive edges for self-supervised learning
  command: python -m examples.tutorial.KDD_2025.heterogeneous_training
# ========
# InferencerConfig:
# specifies the inference configuration. This includes the inferencer command and the arguments to pass to it
inferencerConfig:
  inferencerArgs:
    # Example argument to inferencer
    log_every_n_batch: "50"
    local_saved_model: "False" # Whether to use a local saved model instead of a remote URI.
  command: python -m examples.tutorial.KDD_2025.heterogeneous_inference
# ========
# FeatureFlags:
# any additional flags which we should specify for the training + inference job. We currently use this to
# specify whether GLT should be used as the backend for in-memory subgraph sampling
featureFlags:
  should_run_glt_backend: 'True'

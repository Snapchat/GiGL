{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d9e20f",
   "metadata": {},
   "source": [
    "# In-Memory GiGL - Heterogeneous Graph Example\n",
    "\n",
    "Latest version of this notebook can be found on [github](https://github.com/Snapchat/GiGL/blob/main/examples/tutorial/KDD_2025/heterogeneous_walkthrough.ipynb)\n",
    "\n",
    "\n",
    "This notebook provides a walkthrough of preprocessing components with a small toy graph for GiGL's in-memory setting for training/inference. It will help you understand how each of these components perform in-memory training and inference.\n",
    "\n",
    "\n",
    "## Overview Of Components\n",
    "This notebook demonstrates the process of a simple, human-digestible graph being passed through all the pipeline components in GiGL in preparation for training to help understand how each of the components work.\n",
    "\n",
    "The pipeline consists of the following components:\n",
    "\n",
    "- **Config Populator**: Takes a template config and creates a frozen workflow config that dictates all inputs/outputs and business parameters that are read and used by each subsequent component. The template config contains the graph/task definitions and commands/classes you will use to run data preprocessor, trainer, and inferencer. The frozen template config will populate the template config with additional fields detailing where intermediate assets will be written to, such as the trained model and tfrecord output location, and is inferred based on the job name. \n",
    "    - Inputs: \n",
    "        - `template_config.yaml`\n",
    "        - `resource_config.yaml`\n",
    "    - Output: `frozen_gbml_config.yaml`\n",
    "&nbsp;\n",
    "\n",
    "- **Data Preprocessor**: Transforms necessary node and edge feature assets as needed as a precursor step in most ML tasks according to the user-provided data preprocessor config class.\n",
    "    - Inputs: \n",
    "        - `frozen_gbml_config.yaml`, which includes the user-defined preprocessor class for custom logic, and custom arguments can be passed under dataPreprocessorArgs.\n",
    "        - `resource_config.yaml`\n",
    "    - Output: PreprocessedMetadata Proto, which includes inferred GraphMetadata and preprocessed graph data Tfrecords after applying the user-defined preprocessing function.\n",
    "&nbsp;\n",
    "\n",
    "- **Trainer**: The trainer component reads the output of the data preprocessor and trains a model on the data, loading subgraphs on-the-fly by leveraging GiGL's distributed in-memory subgraph sampling capabilities.\n",
    "    - Inputs: \n",
    "        - `frozen_gbml_config.yaml`\n",
    "        - `resource_config.yaml`\n",
    "    - Output: state_dict stored in trainedModelUri.\n",
    "&nbsp;\n",
    "\n",
    "- **Inferencer**: Runs inference of a trained model, leveraging the same distributed in-memory subgraph sampling capabilities, and writes the embeddings to BigQuery.\n",
    "    - Input: \n",
    "        - `frozen_gbml_config.yaml`\n",
    "        - `resource_config.yaml`\n",
    "    - Output: Embeddings assets.\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0dadb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow logs\n",
    "\n",
    "\n",
    "from gigl.common.utils.jupyter_magics import change_working_dir_to_gigl_root\n",
    "change_working_dir_to_gigl_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f416f",
   "metadata": {},
   "source": [
    "## Visualize the dataset\n",
    "\n",
    "First, let's visualize the toy graph :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7073e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from gigl.common.utils.jupyter_magics import GraphVisualizer\n",
    "from gigl.src.mocking.toy_asset_mocker import load_toy_graph\n",
    "\n",
    "\n",
    "original_graph_heterodata: HeteroData = load_toy_graph(graph_config_path=\"examples/tutorial/KDD_2025/graph_config.yaml\")\n",
    "# Visualize the graph\n",
    "GraphVisualizer.visualize_graph(original_graph_heterodata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fdf5ce",
   "metadata": {},
   "source": [
    "### Setting up Configs\n",
    "\n",
    "The first thing we need to do is create the resource and task configs. \n",
    "\n",
    "- **Task Config**: Specifies task-related configurations, guiding the behavior of components according to the needs of your machine learning task. See [Task Config Guide](../../../docs/user_guide/config_guides/task_config_guide.md). For this task, we have already provided a task config: [task_config.yaml](./task_config.yaml).\n",
    "\n",
    "- **Resource Config**: Details the resource allocation and environmental settings across all GiGL components. This encompasses shared resources for all components, as well as component-specific settings. See [Resource Config Guide](../../../docs/user_guide/config_guides/resource_config_guide.md). For this task, we provide a resource [resource_config.yaml](./resource_config.yaml). The provided default values in `shared_resource_config.common_compute_config` will need to be changed.\n",
    "\n",
    "  - **Instructions to configure the resource config to work**:\n",
    "    If you have not already, please follow the [Quick Start Guide](../../../docs/user_guide/getting_started/quick_start.md) to set up your cloud environment and create a default test resource config. You can then copy the relevant `shared_resource_config.common_compute_config` to [resource_config.yaml](./resource_config.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d74f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import getpass\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "\n",
    "from gigl.common import Uri, UriFactory\n",
    "from gigl.common.utils.gcs import GcsUtils\n",
    "from gigl.common.constants import DEFAULT_GIGL_RELEASE_SRC_IMAGE_DATAFLOW_CPU, DEFAULT_GIGL_RELEASE_SRC_IMAGE_CUDA, DEFAULT_GIGL_RELEASE_SRC_IMAGE_CPU\n",
    "from gigl.env.pipelines_config import get_resource_config\n",
    "from gigl.src.common.types.pb_wrappers.gigl_resource_config import GiglResourceConfigWrapper\n",
    "\n",
    "DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG = DEFAULT_GIGL_RELEASE_SRC_IMAGE_DATAFLOW_CPU\n",
    "\n",
    "# TODO (mkolodner-sc): Update notebook_dir when we move this notebook out of the `KDD` folder\n",
    "notebook_dir = pathlib.Path(\"./examples/tutorial/KDD_2025\").as_posix() # We should be in root dir because of cell # 1\n",
    "\n",
    "# You are welcome to customize these to point to your own configuration files.\n",
    "JOB_NAME = f\"{getpass.getuser()}_gigl_toy_example_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "TEMPLATE_TASK_CONFIG_PATH: Uri = UriFactory.create_uri(f\"{notebook_dir}/task_config.yaml\")\n",
    "FROZEN_TASK_CONFIG_POINTER_FILE_PATH: Uri = UriFactory.create_uri(f\"/tmp/GiGL/{JOB_NAME}/frozen_task_config.yaml\")\n",
    "pathlib.Path(FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# We should upload the local resource to GCS so that we can it can be used in our VAI pipelines\n",
    "resource_config_local_path: Uri = UriFactory.create_uri(os.environ.get(\"GIGL_TEST_DEFAULT_RESOURCE_CONFIG\", f\"{notebook_dir}/resource_config.yaml\"))\n",
    "resource_config_wrapper: GiglResourceConfigWrapper = get_resource_config(\n",
    "    resource_config_uri=resource_config_local_path\n",
    ")\n",
    "RESOURCE_CONFIG_PATH: Uri = UriFactory.create_uri(os.path.join(resource_config_wrapper.temp_assets_bucket_path.uri, f\"{getpass.getuser()}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_example_resource_config.yaml\"))\n",
    "GcsUtils().upload_files_to_gcs({resource_config_local_path: RESOURCE_CONFIG_PATH})\n",
    "\n",
    "print(f\"JOB_NAME: {JOB_NAME}\")\n",
    "print(f\"TEMPLATE_TASK_CONFIG_PATH: {TEMPLATE_TASK_CONFIG_PATH.uri}\")\n",
    "print(f\"FROZEN_TASK_CONFIG_POINTER_FILE_PATH: {FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri}\")\n",
    "print(f\"RESOURCE_CONFIG_PATH: {RESOURCE_CONFIG_PATH.uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748d994",
   "metadata": {},
   "source": [
    "## Validating the Configs\n",
    "\n",
    "We provide the ability to validate your resource and task configs. Although the validation is not exhaustive, it does help assert that the more common issues are not present before expensive compute is scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9978c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.src.validation_check.config_validator import kfp_validation_checks\n",
    "\n",
    "validator = kfp_validation_checks(\n",
    "    job_name=JOB_NAME,\n",
    "    task_config_uri=TEMPLATE_TASK_CONFIG_PATH,\n",
    "    resource_config_uri=RESOURCE_CONFIG_PATH,\n",
    "    start_at=\"config_populator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a465d071",
   "metadata": {},
   "source": [
    "### Config Populator\n",
    "\n",
    "Takes in a template `GbmlConfig` and outputs a frozen `GbmlConfig` by populating all job-related metadata paths in `sharedConfig`. These are mostly GCS paths that the following components read and write from, and use as an intermediary data communication medium. For example, the field `sharedConfig.trainedModelMetadata.trained_model_uri` is populated with a GCS URI, which indicates to the Trainer to write the trained model to this path, and to the Inferencer to read the model from this path. See the full [Config Populator Guide](../../../docs/user_guide/overview/components/config_populator.md).\n",
    "\n",
    "After running the command below, we will have created a frozen config and uploaded it to the `perm_assets_bucket` provided in the `resource config`. The path to that file will be stored in the URI in `FROZEN_TASK_CONFIG_POINTER_FILE_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebe61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m \\\n",
    "    gigl.src.config_populator.config_populator \\\n",
    "    --job_name=\"$JOB_NAME\" \\\n",
    "    --template_uri=\"$TEMPLATE_TASK_CONFIG_PATH\" \\\n",
    "    --resource_config_uri=\"$RESOURCE_CONFIG_PATH\" \\\n",
    "    --output_file_path_frozen_gbml_config_uri=\"$FROZEN_TASK_CONFIG_POINTER_FILE_PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c151ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The command above will write the frozen task config path to the file specified by `FROZEN_TASK_CONFIG_POINTER_FILE_PATH`.\n",
    "# Lets see where it was generated\n",
    "FROZEN_TASK_CONFIG_PATH: Uri\n",
    "with open(FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri, 'r') as file:\n",
    "    FROZEN_TASK_CONFIG_PATH = UriFactory.create_uri(file.read().strip())\n",
    "print(f\"FROZEN_TASK_CONFIG_PATH: {FROZEN_TASK_CONFIG_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6492ee",
   "metadata": {},
   "source": [
    "## Visualizing the Diff Between Template and Frozen Config\n",
    "\n",
    "We now have a frozen task config, with the path specified by `FROZEN_TASK_CONFIG_PATH`. We visualize the diff between the `frozen_task_config` generated by the `config_populator` and the original `template_task_config`. All the code below is just to do that and has nothing to do with GiGL.\n",
    "\n",
    "Specifically, note that:\n",
    "1. The component added `sharedConfig` to the YAML, which contains all the intermediary and final output paths for each component.\n",
    "2. It also added a `condensedEdgeTypeMap` and a `condensedNodeTypeMap`, which map all provided edge types and node types to `int` to save storage space:\n",
    "   - `EdgeType: Tuple[srcNodeType: str, relation: str, dstNodeType: str)] -> int`, and \n",
    "   - `NodeType: str -> int`\n",
    "   - Note: You may also provide your own condensedMaps; they will be generated for you if not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b066e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.common.utils.jupyter_magics import show_task_config_colored_unified_diff\n",
    "\n",
    "show_task_config_colored_unified_diff(\n",
    "    f1_uri=FROZEN_TASK_CONFIG_PATH,\n",
    "    f2_uri=TEMPLATE_TASK_CONFIG_PATH,\n",
    "    f1_name='frozen_task_config.yaml',\n",
    "    f2_name='template_task_config.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dc1b94",
   "metadata": {},
   "source": [
    "# Data Preprocessor\n",
    "\n",
    "Once we have a `frozen_task_config`, the first step is to preprocess the data. The Data Preprocessor component uses [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started) to achieve data transformation in a distributed fashion. \n",
    "\n",
    "### Input Parameters and Output Paths for Data Preprocessor\n",
    "Let's take a quick look at what the data preprocessor fields look like in our frozen config for our toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6206cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will load the frozen task and resource configs file into an object so we can reference it\n",
    "from gigl.env.pipelines_config import get_resource_config\n",
    "from gigl.src.common.types.pb_wrappers.gbml_config import GbmlConfigPbWrapper\n",
    "from gigl.src.common.types.pb_wrappers.gigl_resource_config import GiglResourceConfigWrapper\n",
    "import textwrap\n",
    "\n",
    "frozen_task_config = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(\n",
    "    gbml_config_uri=FROZEN_TASK_CONFIG_PATH\n",
    ")\n",
    "resource_config: GiglResourceConfigWrapper = get_resource_config(\n",
    "    resource_config_uri=RESOURCE_CONFIG_PATH\n",
    ")\n",
    "\n",
    "print(\"Frozen Config DataPreprocessor Information:\")\n",
    "\n",
    "print(\"- Data Preprocessor Config: Specifies what class to use for datapreprocessing and any arguments that might be passed in at runtime to that class\")\n",
    "print(textwrap.indent(str(frozen_task_config.dataset_config.data_preprocessor_config), '\\t'))\n",
    "print(\"- Preprocessed Metadata Uri: Specifies path to the preprocessed metadata file that will be generated by this component and used by subsequent components to understand and find the data that was preprocessed\")\n",
    "print(textwrap.indent(str(frozen_task_config.shared_config.preprocessed_metadata_uri), '\\t'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a903c426",
   "metadata": {},
   "source": [
    "### Visualizing the graph in BigQuery\n",
    "\n",
    "For our example, the Data Preprocessor will begin with a toy graph that has been uploaded to bigquery. Let's first visualize what this uploaded graph looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.src.common.utils.bq import BqUtils\n",
    "\n",
    "\n",
    "data_preprocessor_args = frozen_task_config.dataset_config.data_preprocessor_config.data_preprocessor_args\n",
    "# Node Tables\n",
    "USER_NODE_TABLE = data_preprocessor_args.get(\"bq_user_node_table_name\")\n",
    "STORY_NODE_TABLE = data_preprocessor_args.get(\"bq_story_node_table_name\")\n",
    "\n",
    "# Edge Tables\n",
    "USER_STORY_EDGE_TABLE = data_preprocessor_args.get(\"bq_user_story_edge_table_name\")\n",
    "STORY_USER_EDGE_TABLE = data_preprocessor_args.get(\"bq_story_user_edge_table_name\")\n",
    "\n",
    "print(f\"BigQuery User Node Table Name: {USER_NODE_TABLE}\")\n",
    "print(f\"BigQuery Story Node Table Name: {STORY_NODE_TABLE}\")\n",
    "print(f\"BigQuery User Story Edge Table Name: {USER_STORY_EDGE_TABLE}\")\n",
    "print(f\"BigQuery Story User Edge Table Name: {STORY_USER_EDGE_TABLE}\")\n",
    "\n",
    "def display_graph_table(table_bq_path):\n",
    "    bq_utils = BqUtils()\n",
    "    visualize_rows_query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM `{table_bq_path}`\n",
    "        LIMIT 2\n",
    "    \"\"\"\n",
    "    result = bq_utils.run_query(query=visualize_rows_query, labels=resource_config.get_resource_labels())\n",
    "    for row in result:\n",
    "        print(dict(row))\n",
    "\n",
    "\n",
    "print(\"\\n--Visualizing User Node Table--\\n\")\n",
    "display_graph_table(USER_NODE_TABLE)\n",
    "\n",
    "print(\"\\n--Visualizing Story Node Table--\\n\")\n",
    "display_graph_table(STORY_NODE_TABLE)\n",
    "\n",
    "print(\"\\n--Visualizing User-Story Edge Table--\\n\")\n",
    "display_graph_table(USER_STORY_EDGE_TABLE)\n",
    "\n",
    "print(\"\\n--Visualizing Story-User Edge Table--\\n\")\n",
    "display_graph_table(USER_STORY_EDGE_TABLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655fc69",
   "metadata": {},
   "source": [
    "Notice that we have also mocked edge features for the toy graph, but we will only be using node features for this demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8168352f",
   "metadata": {},
   "source": [
    "\n",
    "### Data Preprocessor Config\n",
    "\n",
    "Next, we'll dive into how to build a data preprocessor config for the toy example.\n",
    "\n",
    "- Any custom preprocessing is to be defined in the preprocessor class, specified in the task config by `datasetConfig.dataPreprocessorConfig.dataPreprocessorConfigClsPath`.\n",
    "- This class must inherit from {py:class}`gigl.src.data_preprocessor.lib.data_preprocessor_config.DataPreprocessorConfig`.\n",
    "\n",
    "In your preprocessor spec, you must implement the following 3 functions as defined by the base class `DataPreprocessorConfig`:\n",
    "  - `prepare_for_pipeline`: Preparing datasets for ingestion and transformation.\n",
    "  - `get_nodes_preprocessing_spec`: Defining transformation imperatives for different node types.\n",
    "  - `get_edges_preprocessing_spec`: Defining transformation imperatives for different edge types.\n",
    "\n",
    "We will highlight how these functions are implemented for the toy graph. You will note that in this case, we are not doing anything special (i.e., no feature engineering), just reading from BQ and passing through the features. We could, if we wanted, define our own [preprocessing function](https://www.tensorflow.org/tfx/transform/get_started#preprocessing_function_example), and replace it with `build_passthrough_transform_preprocessing_fn()` defined in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0791ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant imports for creating a Data Preprocessor Config\n",
    "\n",
    "from gigl.src.common.types import AppliedTaskIdentifier\n",
    "from gigl.src.common.types.graph_data import EdgeType, EdgeUsageType, NodeType, Relation\n",
    "from gigl.src.data_preprocessor.lib.data_preprocessor_config import (\n",
    "    DataPreprocessorConfig,\n",
    "    build_ingestion_feature_spec_fn,\n",
    "    build_passthrough_transform_preprocessing_fn,\n",
    ")\n",
    "from gigl.src.data_preprocessor.lib.ingest.bigquery import (\n",
    "    BigqueryEdgeDataReference,\n",
    "    BigqueryNodeDataReference,\n",
    ")\n",
    "from gigl.src.data_preprocessor.lib.ingest.reference import (\n",
    "    EdgeDataReference,\n",
    "    NodeDataReference,\n",
    ")\n",
    "from gigl.src.data_preprocessor.lib.types import (\n",
    "    EdgeDataPreprocessingSpec,\n",
    "    EdgeOutputIdentifier,\n",
    "    NodeDataPreprocessingSpec,\n",
    "    NodeOutputIdentifier,\n",
    ")\n",
    "\n",
    "class ToyDataPreprocessorConfig(DataPreprocessorConfig):\n",
    "    \"\"\"\n",
    "    Any data preprocessor config needs to inherit from DataPreprocessorConfig and implement the necessary methods:\n",
    "    - prepare_for_pipeline: This method is called at the very start of the pipeline. Can be used to prepare any data,\n",
    "    such as running BigQuery queries, or kicking of dataflow pipelines etc. to generate node/edge feature tables.\n",
    "    - get_nodes_preprocessing_spec: This method returns a dictionary of NodeDataReference to NodeDataPreprocessingSpec\n",
    "        This is used to specify how to preprocess the node data using a TFT preprocessing function.\n",
    "        See TFT documentation for more details: https://www.tensorflow.org/tfx/transform/get_started\n",
    "    - get_edges_preprocessing_spec: This method returns a dictionary of EdgeDataReference to EdgeDataPreprocessingSpec\n",
    "        This is used to specify how to preprocess the edge data using a TFT preprocessing function\n",
    "    \"\"\"\n",
    "\n",
    "    # We use the __init__ function to define node types, edge types, and the node/edge tables that we will be feeding into the data preprocessor.\n",
    "    # The arguments to __init__ are provided through the DataPreprocessorArgs field in the task config\n",
    "    def __init__(self, bq_user_node_table_name: str, bq_story_node_table_name: str, bq_user_story_edge_table_name: str, bq_story_user_edge_table_name: str):\n",
    "        super().__init__()\n",
    "\n",
    "        self._user_table = bq_user_node_table_name\n",
    "        self._story_table = bq_story_node_table_name\n",
    "\n",
    "        self._user_to_story_table = bq_user_story_edge_table_name\n",
    "        self._story_to_user_table = bq_story_user_edge_table_name\n",
    "\n",
    "        # We also specify the node types and edge types for the heterogeneous graph:\n",
    "\n",
    "        self._user_node_type = NodeType(\"user\")\n",
    "        self._story_node_type = NodeType(\"story\")\n",
    "\n",
    "        self._user_to_story_edge_type = EdgeType(\n",
    "            self._user_node_type,\n",
    "            Relation(\"to\"),\n",
    "            self._story_node_type,\n",
    "        )\n",
    "\n",
    "        self._story_to_user_edge_type = EdgeType(\n",
    "            self._story_node_type,\n",
    "            Relation(\"to\"),\n",
    "            self._user_node_type,\n",
    "        )\n",
    "\n",
    "        # These features are taken from our node tables. Note that both the \"user\" and \"story\" node types use the same feature names.\n",
    "        self._node_float_feature_list = [\"f0\", \"f1\"]\n",
    "\n",
    "        # We store a mapping of each node type to their respective table URI.\n",
    "        self._node_tables: dict[NodeType, str] = {\n",
    "            self._user_node_type: self._user_table,\n",
    "            self._story_node_type: self._story_table,\n",
    "        }\n",
    "\n",
    "        # We store a mapping of each edge type to their respective table URI.\n",
    "        self._edge_tables: dict[EdgeType, str] = {\n",
    "            self._user_to_story_edge_type: self._user_to_story_table,\n",
    "            self._story_to_user_edge_type: self._story_to_user_table,\n",
    "        }\n",
    "\n",
    "    def prepare_for_pipeline(\n",
    "        self, applied_task_identifier: AppliedTaskIdentifier\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        This function is called at the very start of the pipeline before enumerator and data preprocessor. \n",
    "        This function does not return anything and can be used to perform any operation needed\n",
    "        before running the pipeline, such as gathering data for node and edge sources\n",
    "\n",
    "        Args: \n",
    "            applied_task_identifier (AppliedTaskIdentifier): A unique identifier for the task being run. This is usually \n",
    "                the job name if orchestrating through GiGL's orchestration logic.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def get_nodes_preprocessing_spec(\n",
    "        self,\n",
    "    ) -> dict[NodeDataReference, NodeDataPreprocessingSpec]:\n",
    "        # We specify where the input data is located using NodeDataReference\n",
    "        # In this case, we are reading from BigQuery, thus make use off BigqueryNodeDataReference\n",
    "\n",
    "        output_dict: dict[NodeDataReference, NodeDataPreprocessingSpec] = {}\n",
    "\n",
    "        # Both of our node table use \"node_id\" for specifying the node identifier.\n",
    "        node_identifier = \"node_id\"\n",
    "\n",
    "        for node_type, table in self._node_tables.items():\n",
    "            node_data_reference = BigqueryNodeDataReference(\n",
    "                reference_uri=table,\n",
    "                node_type=node_type,\n",
    "            )\n",
    "\n",
    "            # This is the column name for identifying the node after the TFTransform\n",
    "            node_output_id = NodeOutputIdentifier(node_identifier)\n",
    "\n",
    "            # The ingestion feature spec function is used to specify the input columns and their types\n",
    "            # that will be read from the NodeDataReference - which in this case is BQ.\n",
    "            feature_spec_fn = build_ingestion_feature_spec_fn(\n",
    "                fixed_int_fields=[node_identifier],\n",
    "                fixed_float_fields=self._node_float_feature_list,\n",
    "            )\n",
    "\n",
    "            # We don't need any special preprocessing for the node features.\n",
    "            # Thus, we can make use of a \"passthrough\" transform preprocessing function that simply passes the input\n",
    "            # features through to the output features.\n",
    "            preprocessing_fn = build_passthrough_transform_preprocessing_fn()\n",
    "\n",
    "            output_dict[node_data_reference] = NodeDataPreprocessingSpec(\n",
    "                feature_spec_fn=feature_spec_fn,\n",
    "                preprocessing_fn=preprocessing_fn,\n",
    "                identifier_output=node_output_id,\n",
    "                features_outputs=self._node_float_feature_list,\n",
    "            )\n",
    "        return output_dict\n",
    "\n",
    "    def get_edges_preprocessing_spec(\n",
    "        self,\n",
    "    ) -> dict[EdgeDataReference, EdgeDataPreprocessingSpec]:\n",
    "        output_dict: dict[EdgeDataReference, EdgeDataPreprocessingSpec] = {}\n",
    "\n",
    "        # Both of our edge table uses \"src\" and \"dst\" for specifying the node ids for each edge.\n",
    "        src_node_identifier = \"src\"\n",
    "        dst_node_identifier = \"dst\"\n",
    "\n",
    "        for edge_type, table in self._edge_tables.items():\n",
    "            edge_ref = BigqueryEdgeDataReference(\n",
    "                reference_uri=table,\n",
    "                edge_type=edge_type,\n",
    "                edge_usage_type=EdgeUsageType.MAIN,\n",
    "            )\n",
    "\n",
    "            feature_spec_fn = build_ingestion_feature_spec_fn(\n",
    "                fixed_int_fields=[\n",
    "                    src_node_identifier,\n",
    "                    dst_node_identifier,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # We don't need any special preprocessing for the edges as there are no edge features to begin with.\n",
    "            # Thus, we can make use of a \"passthrough\" transform preprocessing function that simply passes the input\n",
    "            # features through to the output features.\n",
    "            preprocessing_fn = build_passthrough_transform_preprocessing_fn()\n",
    "            edge_output_id = EdgeOutputIdentifier(\n",
    "                src_node=NodeOutputIdentifier(src_node_identifier),\n",
    "                dst_node=NodeOutputIdentifier(dst_node_identifier),\n",
    "            )\n",
    "\n",
    "            output_dict[edge_ref] = EdgeDataPreprocessingSpec(\n",
    "                identifier_output=edge_output_id,\n",
    "                feature_spec_fn=feature_spec_fn,\n",
    "                preprocessing_fn=preprocessing_fn,\n",
    "            )\n",
    "\n",
    "        return output_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11018373",
   "metadata": {},
   "source": [
    "Let's see what these fields look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data_preprocessor_config = ToyDataPreprocessorConfig(*data_preprocessor_args)\n",
    "toy_data_preprocessor_config.prepare_for_pipeline(JOB_NAME) # Doesn't do anything for the toy example\n",
    "node_preprocessing_spec = toy_data_preprocessor_config.get_nodes_preprocessing_spec()\n",
    "edge_preprocessing_spec = toy_data_preprocessor_config.get_edges_preprocessing_spec()\n",
    "\n",
    "print(\"\\n--Node Preprocessing Spec--\\n\")\n",
    "print(node_preprocessing_spec)\n",
    "print(\"\\n--Edge Preprocessing Spec--\\n\")\n",
    "print(edge_preprocessing_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d90ed5",
   "metadata": {},
   "source": [
    "### Running Data Preprocessor and visualizing the Preprocessed Metadata\n",
    "\n",
    "Now that we've built the preprocessor config, we'll now run the data preprocessor component using this config, which has been copied over to [preprocessor_config.py](./preprocessor_config.py) and specified in the task_config in the `datasetConfig.dataPreprocessorConfig.dataPreprocessorConfigClsPath`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514815a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WARN: There is an issue when trying to run dataflow jobs from inside a jupyter kernel; thus we cannot use the line\", \n",
    "below to run the preprocessor as you would normally in a python script. \n",
    "\n",
    "runner.run_data_preprocessor(pipeline_config=pipeline_config)\n",
    "\n",
    "Instead, we will run the preprocessor from the command line.\n",
    "Note: You can actually do this with every component; we just make use of the runner to make it easier to run the components.\n",
    "\"\"\"\n",
    "!python -m gigl.src.data_preprocessor.data_preprocessor \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$FROZEN_TASK_CONFIG_PATH \\\n",
    "--resource_config_uri=$RESOURCE_CONFIG_PATH \\\n",
    "--custom_worker_image_uri=$DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fecd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upon completion of job, we will see the preprocessed metadata be populated\n",
    "\n",
    "frozen_task_config = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(\n",
    "    gbml_config_uri=FROZEN_TASK_CONFIG_PATH\n",
    ")\n",
    "preprocessed_metadata_pb = frozen_task_config.preprocessed_metadata_pb_wrapper.preprocessed_metadata_pb\n",
    "print(preprocessed_metadata_pb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc20f60",
   "metadata": {},
   "source": [
    "You do not have to worry about these details in code as it is all handled by the data preprocessor component and subsequent data loaders\n",
    "But, for the sake of understanding, we will investigate the condensed_node_type = 0 and condensed_edge_type = 0\n",
    "If you remember the from the frozen config the mappings were as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea7934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Condensed Node Type Mapping:\")\n",
    "print(textwrap.indent(str(frozen_task_config.graph_metadata.condensed_node_type_map), '\\t'))\n",
    "print(\"Condensed Edge Type Mapping:\")\n",
    "print(textwrap.indent(str(frozen_task_config.graph_metadata.condensed_edge_type_map), '\\t'))\n",
    "\n",
    "preprocessed_nodes = preprocessed_metadata_pb.condensed_node_type_to_preprocessed_metadata[0].tfrecord_uri_prefix\n",
    "preprocessed_edges = preprocessed_metadata_pb.condensed_edge_type_to_preprocessed_metadata[0].main_edge_info.tfrecord_uri_prefix\n",
    "print(f\"Preprocessed Nodes are stored in: {preprocessed_nodes}\")\n",
    "print(f\"Preprocessed Edges are stored in: {preprocessed_edges}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940de7f",
   "metadata": {},
   "source": [
    "There is not a lot of data so we will have likely just generated one file for each of the preprocessed nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a3ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $preprocessed_nodes && gsutil ls $preprocessed_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28e5af9",
   "metadata": {},
   "source": [
    "# Trainer\n",
    "\n",
    "Now that our data is uploaded to GCS as TFRecords, we will use the data for distributed training and inference. First, we'll walk through how to build a distributed dataset from some frozen task config, load a sampled subgraph from the graph, and run a forward/backward pass on this subgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf042984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a simple forward/backward pass of the model\n",
    "\n",
    "# TODO(mkolodner): Swap to the on-the-fly task config from pre-populator.\n",
    "# First, we need to load the dataset\n",
    "import torch\n",
    "\n",
    "from gigl.distributed import (\n",
    "    DistLinkPredictionDataset,\n",
    "    build_dataset_from_task_config_uri,\n",
    ")\n",
    "# GiGL is meant to operate in a very large distributed setting, so we need to initialize the process group.\n",
    "torch.distributed.init_process_group(\n",
    "    backend=\"gloo\",  # Use the Gloo backend for CPU training.\n",
    "    init_method=\"tcp://localhost:29500\",\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    ")\n",
    "\n",
    "# `build_dataset_from_task_config_uri` is a utility function\n",
    "# to build a dataset in a distributed manner.\n",
    "# It will:\n",
    "# 1. Read the serialized graph data whose located is specified in the task config.\n",
    "# 2. Load the graph data in a distributed manner.\n",
    "# 3. Partition the graph data into shards for distributed training.\n",
    "# 4. Optional: If training, will generate splits for training.\n",
    "dataset: DistLinkPredictionDataset = build_dataset_from_task_config_uri(\n",
    "        task_config_uri=FROZEN_TASK_CONFIG_PATH,\n",
    "        is_inference=False,\n",
    "        _tfrecord_uri_pattern=\".*tfrecord\", # Our example data uses a different tfrecord pattern.\n",
    ")\n",
    "\n",
    "# And instantiate a dataloader:\n",
    "from gigl.distributed import DistABLPLoader\n",
    "\n",
    "loader = DistABLPLoader(\n",
    "            dataset=dataset,\n",
    "            num_neighbors=[2, 2],  # Example neighbor sampling configuration.\n",
    "            input_nodes=(\"user\", torch.tensor([0])),  # Example input nodes, adjust as needed.\n",
    "            batch_size=1,\n",
    "            supervision_edge_type=(\"user\", \"to\", \"story\"),  # Supervision edge type defined in the graph.\n",
    "            pin_memory_device=torch.device(\n",
    "                \"cpu\"\n",
    "            ),  # Only CPU training for this example.\n",
    "        )\n",
    "data: HeteroData = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at the data we just loaded.\n",
    "print(data)\n",
    "\n",
    "# You might notice a few things about the data that is different from vanilla PyG:\n",
    "# * num_sampled_nodes and num_sampled_edges are present,\n",
    "# * representing the number of nodes and edges sampled per hop.\n",
    "# * y_positive is added, and is a dict of anchor node -> target nodes.\n",
    "\n",
    "GraphVisualizer.visualize_graph(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a model and do a forward pass\n",
    "# You can interop with any PyG model, but we will use HGTConv for this example.\n",
    "from torch_geometric.nn import HGTConv\n",
    "\n",
    "model = HGTConv(\n",
    "    in_channels=data.num_node_features,\n",
    "    out_channels=16,  # Example output dimension.\n",
    "    metadata=data.metadata(),\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "\n",
    "# Do a forward pass\n",
    "embeddings = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "print(f\"Embeddings: {embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d89e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's define a loss function for the link prediction task.\n",
    "# TODO should we define this in some util file?\n",
    "\n",
    "# Note that we really should wrap this\n",
    "\n",
    "def compute_loss(model: torch.nn.Module, data: HeteroData) -> torch.Tensor:\n",
    "    main_out: dict[str, torch.Tensor] = model(data.x_dict, data.edge_index_dict)\n",
    "    # data.y_positive = {\n",
    "    #   0: [1, 2],\n",
    "    #   1: [3, 4, 5],\n",
    "    # }\n",
    "    anchor_nodes = torch.arange(data[\"user\"].batch_size).repeat_interleave(\n",
    "        torch.tensor([len(v) for v in data.y_positive.values()])\n",
    "    )\n",
    "    # anchor_nodes = [0, 0, 1, 1, 1]\n",
    "    target_nodes = torch.cat([v for v in data.y_positive.values()])\n",
    "    # target_nodes = [1, 2, 3, 4, 5]\n",
    "    # Use MarginRankingLoss for link prediction\n",
    "    loss_fn = torch.nn.MarginRankingLoss()\n",
    "    query_embeddings = main_out[\"user\"][anchor_nodes]\n",
    "    target_embeddings = main_out[\"story\"][target_nodes]\n",
    "    loss = loss_fn(\n",
    "        input1=query_embeddings,\n",
    "        input2=target_embeddings,\n",
    "        target=torch.ones_like(query_embeddings, dtype=torch.float32),\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "# Note that in practice you would want to wrap this in a training loop\n",
    "# but for this example doing just one pass is sufficient.\n",
    "# A training loop example can be found in:\n",
    "# examples/tutorial/KDD_2025/heterogeneous_training.py\n",
    "loss = compute_loss(model, data)\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# And we can do a backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ee03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now if we run the loss function again, we should see a different value.\n",
    "loss = compute_loss(model, data)\n",
    "print(f\"Loss after backward pass: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{dataset.node_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've \"Trained\" the model, let's do inference on it.\n",
    "# Note that inference is very similar to training, but we don't need to do a backward pass.\n",
    "# And we should use the `DistNeighborloader` instead of `DistABLPLoader`\n",
    "from gigl.distributed import DistNeighborLoader\n",
    "inference_loader = DistNeighborLoader(\n",
    "    dataset=dataset,\n",
    "    num_neighbors=[2, 2],  # Example neighbor sampling configuration.\n",
    "    input_nodes=(\"user\", torch.tensor([0, 1, 2, 3])),  # Run inference against some of the nodes. In a custom datasets you would use `dataset.node_ids['user']`.\n",
    "    batch_size=1,\n",
    "    pin_memory_device=torch.device(\"cpu\"),  # Only CPU training for this example.\n",
    ")\n",
    "\n",
    "# GiGL has an \"EmbeddingExporter\" to write out the embeddings to disk or GCS. We export embeddings to a local file in this example. \n",
    "from gigl.common import UriFactory\n",
    "from gigl.common.data.export import EmbeddingExporter\n",
    "\n",
    "# Use a local directory for exporting embeddings.\n",
    "# You can also use a GCS URI if you want to export to GCS.\n",
    "# For example, use \"gs://your-bucket-name/path/to/embeddings\".\n",
    "embedding_dir = UriFactory.create_uri(\"examples/tutorial/KDD_2025/.embeddings\")\n",
    "\n",
    "exporter = EmbeddingExporter(\n",
    "    export_dir=embedding_dir,\n",
    ")\n",
    "with torch.no_grad():\n",
    "    for data in inference_loader:\n",
    "        embeddings = model(data.x_dict, data.edge_index_dict)\n",
    "        exporter.add_embedding(\n",
    "            id_batch=data[\"user\"].batch,\n",
    "            embedding_batch=embeddings[\"user\"],\n",
    "            embedding_type=\"user\"\n",
    "        )\n",
    "    exporter.flush_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d08cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the exported embeddings\n",
    "!ls examples/tutorial/KDD_2025/.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the embeddings from disk and load them into a dataframe.\n",
    "from pathlib import Path\n",
    "import fastavro\n",
    "\n",
    "avro_records = []\n",
    "for file in Path(\"examples/tutorial/KDD_2025/.embeddings\").glob(\"*.avro\"):\n",
    "    with open(file, \"rb\") as f:\n",
    "        reader = fastavro.reader(f)\n",
    "        for record in reader:\n",
    "            avro_records.append(record)\n",
    "print(f\"Loaded {len(avro_records)} records from the avro files.\")\n",
    "print(f\"First record: {avro_records[0]}\")\n",
    "\n",
    "# And load them into a dataframe.\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_records(avro_records)\n",
    "print(f\"Dataframe:\\n{df}\")\n",
    "# GiGL also has gigl.common.data.export.load_embeddings_to_bigquery\n",
    "# Which you can use to load the embeddings into BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0525d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

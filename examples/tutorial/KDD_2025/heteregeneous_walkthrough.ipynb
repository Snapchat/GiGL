{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d9e20f",
   "metadata": {},
   "source": [
    "# Toy Example - In-Memory GiGL\n",
    "\n",
    "Latest version of this notebook can be found on [github](https://github.com/Snapchat/GiGL/blob/main/examples/tutorial/KDD_2025/heterogeneous_walkthrough.ipynb)\n",
    "\n",
    "\n",
    "This notebook provides a walkthrough of preprocessing components with a small toy graph for GiGL's in-memory setting for training/inference. It will help you understand how each of these components perform in-memory training and inference.\n",
    "\n",
    "\n",
    "## Overview Of Components\n",
    "This notebook demonstrates the process of a simple, human-digestible graph being passed through all the pipeline components in GiGL in preparation for training to help understand how each of the components work.\n",
    "\n",
    "The pipeline consists of the following components:\n",
    "\n",
    "- **Config Populator**: Takes a template config and creates a frozen workflow config that dictates all inputs/outputs and business parameters that are read and used by each subsequent component.\n",
    "    - Input: `template_config.yaml`\n",
    "    - Output: `frozen_gbml_config.yaml`\n",
    "&nbsp;\n",
    "\n",
    "- **Data Preprocessor**: Transforms necessary node and edge feature assets as needed as a precursor step in most ML tasks according to the user-provided data preprocessor config class.\n",
    "    - Input: `frozen_gbml_config.yaml`, which includes the user-defined preprocessor class for custom logic, and custom arguments can be passed under dataPreprocessorArgs.\n",
    "    - Output: PreprocessedMetadata Proto, which includes inferred GraphMetadata and preprocessed graph data Tfrecords after applying the user-defined preprocessing function.\n",
    "&nbsp;\n",
    "\n",
    "- **Trainer**: The trainer component reads the output of the data preprocessor and trains a model on the data, loading subgraphs on-the-fly by leveraging GiGL's distributed in-memory capabilities.\n",
    "    - Input: `frozen_gbml_config.yaml`\n",
    "    - Output: state_dict stored in trainedModelUri.\n",
    "&nbsp;\n",
    "\n",
    "- **Inferencer**: Runs inference of a trained model, leveraging the same in-memory subgraph sampling capabilities, and writes the embeddings to BigQuery.\n",
    "    - Input: `frozen_gbml_config.yaml`\n",
    "    - Output: Embeddings assets.\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0dadb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow logs\n",
    "\n",
    "\n",
    "from gigl.common.utils.jupyter_magics import change_working_dir_to_gigl_root\n",
    "change_working_dir_to_gigl_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f416f",
   "metadata": {},
   "source": [
    "## Visualize the dataset\n",
    "\n",
    "First, let's visualize the toy graph :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7073e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from gigl.common.utils.jupyter_magics import GraphVisualizer\n",
    "from gigl.src.mocking.toy_asset_mocker import load_toy_graph\n",
    "\n",
    "\n",
    "original_graph_heterodata: HeteroData = load_toy_graph(graph_config_path=\"examples/tutorial/KDD_2025/graph_config.yaml\")\n",
    "# Visualize the graph\n",
    "GraphVisualizer.visualize_graph(original_graph_heterodata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fdf5ce",
   "metadata": {},
   "source": [
    "### Setting up Configs\n",
    "\n",
    "The first thing we need to do is create the resource and task configs. \n",
    "\n",
    "- **Task Config**: Specifies task-related configurations, guiding the behavior of components according to the needs of your machine learning task. See [Task Config Guide](../../docs/user_guide/config_guides/task_config_guide.md). For this task, we have already provided a task config: [task_config.yaml](./task_config.yaml).\n",
    "\n",
    "- **Resource Config**: Details the resource allocation and environmental settings across all GiGL components. This encompasses shared resources for all components, as well as component-specific settings. See [Resource Config Guide](../../docs/user_guide/config_guides/resource_config_guide.md). For this task, we provide a resource [resource_config.yaml](./resource_config.yaml). The provided default values in `shared_resource_config.common_compute_config` will need to be changed.\n",
    "\n",
    "  - **Instructions to configure the resource config to work**:\n",
    "    If you have not already, please follow the [Quick Start Guide](../../docs/user_guide/getting_started/quick_start.md) to set up your cloud environment and create a default test resource config. You can then copy the relevant `shared_resource_config.common_compute_config` to [resource_config.yaml](./resource_config.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d74f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import getpass\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "\n",
    "from gigl.common import Uri, UriFactory\n",
    "from gigl.common.constants import DEFAULT_GIGL_RELEASE_SRC_IMAGE_DATAFLOW_CPU, DEFAULT_GIGL_RELEASE_SRC_IMAGE_CUDA, DEFAULT_GIGL_RELEASE_SRC_IMAGE_CPU\n",
    "\n",
    "DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG = DEFAULT_GIGL_RELEASE_SRC_IMAGE_DATAFLOW_CPU\n",
    "\n",
    "notebook_dir = pathlib.Path(\"./examples/tutorial/KDD_2025\").as_posix() # We should be in root dir because of cell # 1\n",
    "\n",
    "# You are welcome to customize these  to point to your own configuration files.\n",
    "JOB_NAME = f\"{getpass.getuser()}_gigl_toy_example_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "TEMPLATE_TASK_CONFIG_PATH: Uri = UriFactory.create_uri(f\"{notebook_dir}/task_config.yaml\")\n",
    "FROZEN_TASK_CONFIG_POINTER_FILE_PATH: Uri = UriFactory.create_uri(f\"/tmp/GiGL/{JOB_NAME}/frozen_task_config.yaml\")\n",
    "pathlib.Path(FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri).parent.mkdir(parents=True, exist_ok=True)\n",
    "# Ensure you change the resource config path to point to your own resource configuration\n",
    "# i.e. what was exported to $GIGL_TEST_DEFAULT_RESOURCE_CONFIG as part of the quick start guide.\n",
    "RESOURCE_CONFIG_PATH: Uri = UriFactory.create_uri(os.environ.get(\"GIGL_TEST_DEFAULT_RESOURCE_CONFIG\", f\"{notebook_dir}/resource_config.yaml\"))\n",
    "\n",
    "# Export string format of the uris so we can reference them in cells that execute bash commands below.\n",
    "os.environ[\"TEMPLATE_TASK_CONFIG_PATH\"] = TEMPLATE_TASK_CONFIG_PATH.uri\n",
    "os.environ[\"FROZEN_TASK_CONFIG_POINTER_FILE_PATH\"] = FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri\n",
    "os.environ[\"RESOURCE_CONFIG_PATH\"] = RESOURCE_CONFIG_PATH.uri\n",
    "\n",
    "print(f\"JOB_NAME: {JOB_NAME}\")\n",
    "print(f\"TEMPLATE_TASK_CONFIG_PATH: {TEMPLATE_TASK_CONFIG_PATH.uri}\")\n",
    "print(f\"FROZEN_TASK_CONFIG_POINTER_FILE_PATH: {FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri}\")\n",
    "print(f\"RESOURCE_CONFIG_PATH: {RESOURCE_CONFIG_PATH.uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748d994",
   "metadata": {},
   "source": [
    "## Validating the Configs\n",
    "\n",
    "We provide the ability to validate your resource and task configs. Although the validation is not exhaustive, it does help assert that the more common issues are not present before expensive compute is scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9978c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.src.validation_check.config_validator import kfp_validation_checks\n",
    "\n",
    "validator = kfp_validation_checks(\n",
    "    job_name=JOB_NAME,\n",
    "    task_config_uri=TEMPLATE_TASK_CONFIG_PATH,\n",
    "    resource_config_uri=RESOURCE_CONFIG_PATH,\n",
    "    start_at=\"config_populator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a465d071",
   "metadata": {},
   "source": [
    "### Config Populator\n",
    "\n",
    "Takes in a template `GbmlConfig` and outputs a frozen `GbmlConfig` by populating all job-related metadata paths in `sharedConfig`. These are mostly GCS paths that the following components read and write from, and use as an intermediary data communication medium. For example, the field `sharedConfig.trainedModelMetadata` is populated with a GCS URI, which indicates to the Trainer to write the trained model to this path, and to the Inferencer to read the model from this path. See the full [Config Populator Guide](../../docs/user_guide/overview/components/config_populator.md).\n",
    "\n",
    "After running the command below, we will have created a frozen config and uploaded it to the `perm_assets_bucket` provided in the `resource config`. The path to that file will be stored in the file at `FROZEN_TASK_CONFIG_POINTER_FILE_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebe61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# We suppress the output of this cell to avoid cluttering the notebook with logs. Remove %%%capture if you want to see the output.\n",
    "\n",
    "!python -m \\\n",
    "    gigl.src.config_populator.config_populator \\\n",
    "    --job_name=\"$JOB_NAME\" \\\n",
    "    --template_uri=\"$TEMPLATE_TASK_CONFIG_PATH\" \\\n",
    "    --resource_config_uri=\"$RESOURCE_CONFIG_PATH\" \\\n",
    "    --output_file_path_frozen_gbml_config_uri=\"$FROZEN_TASK_CONFIG_POINTER_FILE_PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c151ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The command above will write the frozen task config path to the file specified by `FROZEN_TASK_CONFIG_POINTER_FILE_PATH`.\n",
    "# Lets see where it was generated\n",
    "FROZEN_TASK_CONFIG_PATH: Uri\n",
    "with open(FROZEN_TASK_CONFIG_POINTER_FILE_PATH.uri, 'r') as file:\n",
    "    FROZEN_TASK_CONFIG_PATH = UriFactory.create_uri(file.read().strip())\n",
    "print(f\"FROZEN_TASK_CONFIG_PATH: {FROZEN_TASK_CONFIG_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dc1b94",
   "metadata": {},
   "source": [
    "# Data Preprocessor\n",
    "\n",
    "Once we have a `frozen_task_config`, the first step is to preprocess the data.\n",
    "\n",
    "The Data Preprocessor component uses [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started) to achieve data transformation in a distributed fashion. \n",
    "\n",
    "- Any custom preprocessing is to be defined in the preprocessor class, specified in the task config by `datasetConfig.dataPreprocessorConfig.dataPreprocessorConfigClsPath`.\n",
    "- This class must inherit from {py:class}`gigl.src.data_preprocessor.lib.data_preprocessor_config.DataPreprocessorConfig`.\n",
    "\n",
    "In your preprocessor spec, you must implement the following 3 functions as defined by the base class `DataPreprocessorConfig`:\n",
    "  - `prepare_for_pipeline`: Preparing datasets for ingestion and transformation.\n",
    "  - `get_nodes_preprocessing_spec`: Defining transformation imperatives for different node types.\n",
    "  - `get_edges_preprocessing_spec`: Defining transformation imperatives for different edge types.\n",
    "\n",
    "Please take a look at [preprocessor_config.py](./preprocessor_config.py) to see how these are defined. You will note that in this case, we are not doing anything special (i.e., no feature engineering), just reading from BQ and passing through the features. We could, if we wanted, define our own [preprocessing function](https://www.tensorflow.org/tfx/transform/get_started#preprocessing_function_example), and replace it with `build_passthrough_transform_preprocessing_fn()` defined in the code.\n",
    "\n",
    "### Input Parameters and Output Paths for Data Preprocessor\n",
    "Let's take a quick look at what these are from our frozen config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7115dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will load the frozen task and resource configs file into an object so we can reference it\n",
    "from gigl.env.pipelines_config import get_resource_config\n",
    "from gigl.src.common.types.pb_wrappers.gbml_config import GbmlConfigPbWrapper\n",
    "from gigl.src.common.types.pb_wrappers.gigl_resource_config import GiglResourceConfigWrapper\n",
    "import textwrap\n",
    "\n",
    "frozen_task_config = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(\n",
    "    gbml_config_uri=FROZEN_TASK_CONFIG_PATH\n",
    ")\n",
    "resource_config: GiglResourceConfigWrapper = get_resource_config(\n",
    "    resource_config_uri=RESOURCE_CONFIG_PATH\n",
    ")\n",
    "\n",
    "print(\"Frozen Config DataPreprocessor Information:\")\n",
    "\n",
    "print(\"- Data Preprocessor Config: Specifies what class to use for datapreprocessing and any arguments that might be passed in at runtime to that class\")\n",
    "print(textwrap.indent(str(frozen_task_config.dataset_config.data_preprocessor_config), '\\t'))\n",
    "print(\"- Preprocessed Metadata Uri: Specifies path to the preprocessed metadata file that will be generated by this component and used by subsequent components to understand and find the data that was preprocessed\")\n",
    "print(textwrap.indent(str(frozen_task_config.shared_config.preprocessed_metadata_uri), '\\t'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d90ed5",
   "metadata": {},
   "source": [
    "### Running Data Preprocessor and visualizing the Preprocessed Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514815a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!python -m gigl.src.data_preprocessor.data_preprocessor \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$FROZEN_TASK_CONFIG_PATH \\\n",
    "--resource_config_uri=$RESOURCE_CONFIG_PATH \\\n",
    "--custom_worker_image_uri=$DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fecd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upon completion of job, we will see the preprocessed metadata be populated\n",
    "\n",
    "frozen_task_config = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(\n",
    "    gbml_config_uri=FROZEN_TASK_CONFIG_PATH\n",
    ")\n",
    "preprocessed_metadata_pb = frozen_task_config.preprocessed_metadata_pb_wrapper.preprocessed_metadata_pb\n",
    "print(preprocessed_metadata_pb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc20f60",
   "metadata": {},
   "source": [
    "You do not have to worry about these details in code as it is all handled by the data preprocessor component and subsequent data loaders\n",
    "But, for the sake of understanding, we will investigate the condensed_node_type = 0 and condensed_edge_type = 0\n",
    "If you remember the from the frozen config the mappings were as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea7934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Condensed Node Type Mapping:\")\n",
    "print(textwrap.indent(str(frozen_task_config.graph_metadata.condensed_node_type_map), '\\t'))\n",
    "print(\"Condensed Edge Type Mapping:\")\n",
    "print(textwrap.indent(str(frozen_task_config.graph_metadata.condensed_edge_type_map), '\\t'))\n",
    "\n",
    "preprocessed_nodes = preprocessed_metadata_pb.condensed_node_type_to_preprocessed_metadata[0].tfrecord_uri_prefix\n",
    "preprocessed_edges = preprocessed_metadata_pb.condensed_edge_type_to_preprocessed_metadata[0].main_edge_info.tfrecord_uri_prefix\n",
    "print(f\"Preprocessed Nodes are stored in: {preprocessed_nodes}\")\n",
    "print(f\"Preprocessed Edges are stored in: {preprocessed_edges}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940de7f",
   "metadata": {},
   "source": [
    "There is not a lot of data so we will have likely just generated one file for each of the preprocessed nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a3ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $preprocessed_nodes && gsutil ls $preprocessed_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf042984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a simple forward/backward pass of the model\n",
    "\n",
    "# TODO(mkolodner): Swap to the on-the-fly task config from pre-populator.\n",
    "task_config_uri = \"examples/tutorial/KDD_2025/toy_graph_task_config.yaml\"\n",
    "# First, we need to load the dataset\n",
    "import torch\n",
    "\n",
    "from gigl.distributed import (\n",
    "    DistLinkPredictionDataset,\n",
    "    build_dataset_from_task_config_uri,\n",
    ")\n",
    "# GiGL is meant to operate in a very large distributed setting, so we need to initialize the process group.\n",
    "torch.distributed.init_process_group(\n",
    "    backend=\"gloo\",  # Use the Gloo backend for CPU training.\n",
    "    init_method=\"tcp://localhost:29500\",\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    ")\n",
    "\n",
    "# `build_dataset_from_task_config_uri` is a utility function\n",
    "# to build a dataset in a distributed manner.\n",
    "# It will:\n",
    "# 1. Read the serialized graph data whose located is specified in the task config.\n",
    "# 2. Load the graph data in a distributed manner.\n",
    "# 3. Partition the graph data into shards for distributed training.\n",
    "# 4. Optional: If training, will generate splits for training.\n",
    "dataset: DistLinkPredictionDataset = build_dataset_from_task_config_uri(\n",
    "        task_config_uri=task_config_uri,\n",
    "        is_inference=False,\n",
    "        _tfrecord_uri_pattern=\".*tfrecord\", # Our example data uses a different tfrecord pattern.\n",
    ")\n",
    "\n",
    "# And instantiate a dataloader:\n",
    "from gigl.distributed import DistABLPLoader\n",
    "\n",
    "loader = DistABLPLoader(\n",
    "            dataset=dataset,\n",
    "            num_neighbors=[2, 2],  # Example neighbor sampling configuration.\n",
    "            input_nodes=(\"user\", torch.tensor([0])),  # Example input nodes, adjust as needed.\n",
    "            batch_size=1,\n",
    "            supervision_edge_type=(\"user\", \"to\", \"story\"),  # Supervision edge type defined in the graph.\n",
    "            pin_memory_device=torch.device(\n",
    "                \"cpu\"\n",
    "            ),  # Only CPU training for this example.\n",
    "        )\n",
    "data: HeteroData = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at the data we just loaded.\n",
    "print(data)\n",
    "\n",
    "# You might notice a few things about the data that is different from vanilla PyG:\n",
    "# * num_sampled_nodes and num_sampled_edges are present,\n",
    "# * representing the number of nodes and edges sampled per hop.\n",
    "# * y_positive is added, and is a dict of anchor node -> target nodes.\n",
    "\n",
    "GraphVisualizer.visualize_graph(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a model and do a forward pass\n",
    "# You can interop with any PyG model, but we will use HGTConv for this example.\n",
    "from torch_geometric.nn import HGTConv\n",
    "\n",
    "model = HGTConv(\n",
    "    in_channels=data.num_node_features,\n",
    "    out_channels=16,  # Example output dimension.\n",
    "    metadata=data.metadata(),\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "\n",
    "# Do a forward pass\n",
    "embeddings = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "print(f\"Embeddings: {embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d89e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's define a loss function for the link prediction task.\n",
    "# TODO should we define this in some util file?\n",
    "\n",
    "# Note that we really should wrap this\n",
    "\n",
    "def compute_loss(model: torch.nn.Module, data: HeteroData) -> torch.Tensor:\n",
    "    main_out: dict[str, torch.Tensor] = model(data.x_dict, data.edge_index_dict)\n",
    "    # data.y_positive = {\n",
    "    #   0: [1, 2],\n",
    "    #   1: [3, 4, 5],\n",
    "    # }\n",
    "    anchor_nodes = torch.arange(data[\"user\"].batch_size).repeat_interleave(\n",
    "        torch.tensor([len(v) for v in data.y_positive.values()])\n",
    "    )\n",
    "    # anchor_nodes = [0, 0, 1, 1, 1]\n",
    "    target_nodes = torch.cat([v for v in data.y_positive.values()])\n",
    "    # target_nodes = [1, 2, 3, 4, 5]\n",
    "    # Use MarginRankingLoss for link prediction\n",
    "    loss_fn = torch.nn.MarginRankingLoss()\n",
    "    query_embeddings = main_out[\"user\"][anchor_nodes]\n",
    "    target_embeddings = main_out[\"story\"][target_nodes]\n",
    "    loss = loss_fn(\n",
    "        input1=query_embeddings,\n",
    "        input2=target_embeddings,\n",
    "        target=torch.ones_like(query_embeddings, dtype=torch.float32),\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "# Note that in practice you would want to wrap this in a training loop\n",
    "# but for this example doing just one pass is sufficient.\n",
    "# A training loop example can be found in:\n",
    "# examples/tutorial/KDD_2025/heterogeneous_training.py\n",
    "loss = compute_loss(model, data)\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# And we can do a backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ee03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now if we run the loss function again, we should see a different value.\n",
    "loss = compute_loss(model, data)\n",
    "print(f\"Loss after backward pass: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{dataset.node_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've \"Trained\" the model, let's do inference on it.\n",
    "# Note that inference is very similar to training, but we don't need to do a backward pass.\n",
    "# And we should use the `DistNeighborloader` instead of `DistABLPLoader`\n",
    "from gigl.distributed import DistNeighborLoader\n",
    "inference_loader = DistNeighborLoader(\n",
    "    dataset=dataset,\n",
    "    num_neighbors=[2, 2],  # Example neighbor sampling configuration.\n",
    "    input_nodes=(\"user\", torch.tensor([0, 1, 2, 3])),  # Run inference against some of the nodes. In a custom datasets you would use `dataset.node_ids['user']`.\n",
    "    batch_size=1,\n",
    "    pin_memory_device=torch.device(\"cpu\"),  # Only CPU training for this example.\n",
    ")\n",
    "\n",
    "# GiGL has an \"EmbeddingExporter\" to write out the embeddings to disk or GCS. We export embeddings to a local file in this example. \n",
    "from gigl.common import UriFactory\n",
    "from gigl.common.data.export import EmbeddingExporter\n",
    "\n",
    "# Use a local directory for exporting embeddings.\n",
    "# You can also use a GCS URI if you want to export to GCS.\n",
    "# For example, use \"gs://your-bucket-name/path/to/embeddings\".\n",
    "embedding_dir = UriFactory.create_uri(\"examples/tutorial/KDD_2025/.embeddings\")\n",
    "\n",
    "exporter = EmbeddingExporter(\n",
    "    export_dir=embedding_dir,\n",
    ")\n",
    "with torch.no_grad():\n",
    "    for data in inference_loader:\n",
    "        embeddings = model(data.x_dict, data.edge_index_dict)\n",
    "        exporter.add_embedding(\n",
    "            id_batch=data[\"user\"].batch,\n",
    "            embedding_batch=embeddings[\"user\"],\n",
    "            embedding_type=\"user\"\n",
    "        )\n",
    "    exporter.flush_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d08cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the exported embeddings\n",
    "!ls examples/tutorial/KDD_2025/.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the embeddings from disk and load them into a dataframe.\n",
    "from pathlib import Path\n",
    "import fastavro\n",
    "\n",
    "avro_records = []\n",
    "for file in Path(\"examples/tutorial/KDD_2025/.embeddings\").glob(\"*.avro\"):\n",
    "    with open(file, \"rb\") as f:\n",
    "        reader = fastavro.reader(f)\n",
    "        for record in reader:\n",
    "            avro_records.append(record)\n",
    "print(f\"Loaded {len(avro_records)} records from the avro files.\")\n",
    "print(f\"First record: {avro_records[0]}\")\n",
    "\n",
    "# And load them into a dataframe.\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_records(avro_records)\n",
    "print(f\"Dataframe:\\n{df}\")\n",
    "# GiGL also has gigl.common.data.export.load_embeddings_to_bigquery\n",
    "# Which you can use to load the embeddings into BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0525d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d9e20f",
   "metadata": {},
   "source": [
    "# Walking through training and generating embeddings for a heterogeneous model using GiGL\n",
    "\n",
    "This notebook will walk you through using GiGL to train a hetereogeneous model.\n",
    "For this example, we will be using some very small \"toy graph\" as our dataset.\n",
    "\n",
    "By the end of this notebook you will have:\n",
    "\n",
    "1. Pre-processed the toy graph using GiGL data preprocessor\n",
    "2. Done a forward and backward pass of the model, using GiGL dataloaders\n",
    "3. Generated embeddings for the model.\n",
    "\n",
    "This file is intended to be a companion to our example [heterogeneous_training.py](./heterogenous_training.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0dadb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow logs\n",
    "\n",
    "\n",
    "from gigl.common.utils.jupyter_magics import change_working_dir_to_gigl_root\n",
    "change_working_dir_to_gigl_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f416f",
   "metadata": {},
   "source": [
    "## Visualize the dataset\n",
    "\n",
    "First, let's visualize the toy graph :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7073e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from gigl.common.utils.jupyter_magics import GraphVisualizer\n",
    "from gigl.src.mocking.toy_asset_mocker import load_toy_graph\n",
    "\n",
    "\n",
    "original_graph_heterodata: HeteroData = load_toy_graph(graph_config_path=\"examples/tutorial/KDD_2025/graph_config.yaml\")\n",
    "# Visualize the graph\n",
    "GraphVisualizer.visualize_graph(original_graph_heterodata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dc1b94",
   "metadata": {},
   "source": [
    "# Preprocessor\n",
    "TODO(mkolodner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf042984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a simple forward/backward pass of the model\n",
    "\n",
    "# TODO(mkolodner): Swap to the on-the-fly task config from pre-populator.\n",
    "task_config_uri = \"examples/tutorial/KDD_2025/toy_graph_task_config.yaml\"\n",
    "# First, we need to load the dataset\n",
    "import torch\n",
    "\n",
    "from gigl.distributed import (\n",
    "    DistLinkPredictionDataset,\n",
    "    build_dataset_from_task_config_uri,\n",
    ")\n",
    "# GiGL is meant to operate in a very large distributed setting, so we need to initialize the process group.\n",
    "torch.distributed.init_process_group(\n",
    "    backend=\"gloo\",  # Use the Gloo backend for CPU training.\n",
    "    init_method=\"tcp://localhost:29500\",\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    ")\n",
    "\n",
    "# `build_dataset_from_task_config_uri` is a utility function\n",
    "# to build a dataset in a distributed manner.\n",
    "# It will:\n",
    "# 1. Read the serialized graph data whose located is specified in the task config.\n",
    "# 2. Load the graph data in a distributed manner.\n",
    "# 3. Partition the graph data into shards for distributed training.\n",
    "# 4. Optional: If training, will generate splits for training.\n",
    "dataset: DistLinkPredictionDataset = build_dataset_from_task_config_uri(\n",
    "        task_config_uri=task_config_uri,\n",
    "        is_inference=False,\n",
    "        _tfrecord_uri_pattern=\".*tfrecord\", # Our example data uses a different tfrecord pattern.\n",
    ")\n",
    "\n",
    "# And instantiate a dataloader:\n",
    "from gigl.distributed import DistABLPLoader\n",
    "\n",
    "loader = DistABLPLoader(\n",
    "            dataset=dataset,\n",
    "            num_neighbors=[2, 2],  # Example neighbor sampling configuration.\n",
    "            input_nodes=(\"user\", torch.tensor([0])),  # Example input nodes, adjust as needed.\n",
    "            batch_size=1,\n",
    "            supervision_edge_type=(\"user\", \"to\", \"story\"),  # Supervision edge type defined in the graph.\n",
    "            pin_memory_device=torch.device(\n",
    "                \"cpu\"\n",
    "            ),  # Only CPU training for this example.\n",
    "        )\n",
    "data: HeteroData = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at the data we just loaded.\n",
    "print(data)\n",
    "\n",
    "# You might notice a few things about the data that is different from vanilla PyG:\n",
    "# * num_sampled_nodes and num_sampled_edges are present,\n",
    "# * representing the number of nodes and edges sampled per hop.\n",
    "# * y_positive is added, and is a dict of anchor node -> target nodes.\n",
    "\n",
    "GraphVisualizer.visualize_graph(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a model and do a forward pass\n",
    "# You can interop with any PyG model, but we will use HGTConv for this example.\n",
    "from torch_geometric.nn import HGTConv\n",
    "\n",
    "model = HGTConv(\n",
    "    in_channels=data.num_node_features,\n",
    "    out_channels=16,  # Example output dimension.\n",
    "    metadata=data.metadata(),\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "\n",
    "# Do a forward pass\n",
    "embeddings = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "print(f\"Embeddings: {embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d89e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's define a loss function for the link prediction task.\n",
    "# TODO should we define this in some util file?\n",
    "\n",
    "# Note that we really should wrap this\n",
    "\n",
    "def compute_loss(model: torch.nn.Module, data: HeteroData) -> torch.Tensor:\n",
    "    main_out: dict[str, torch.Tensor] = model(data.x_dict, data.edge_index_dict)\n",
    "    # data.y_positive = {\n",
    "    #   0: [1, 2],\n",
    "    #   1: [3, 4, 5],\n",
    "    # }\n",
    "    anchor_nodes = torch.arange(data[\"user\"].batch_size).repeat_interleave(\n",
    "        torch.tensor([len(v) for v in data.y_positive.values()])\n",
    "    )\n",
    "    # anchor_nodes = [0, 0, 1, 1, 1]\n",
    "    target_nodes = torch.cat([v for v in data.y_positive.values()])\n",
    "    # target_nodes = [1, 2, 3, 4, 5]\n",
    "    # Use MarginRankingLoss for link prediction\n",
    "    loss_fn = torch.nn.MarginRankingLoss()\n",
    "    query_embeddings = main_out[\"user\"][anchor_nodes]\n",
    "    target_embeddings = main_out[\"story\"][target_nodes]\n",
    "    loss = loss_fn(\n",
    "        input1=query_embeddings,\n",
    "        input2=target_embeddings,\n",
    "        target=torch.ones_like(query_embeddings, dtype=torch.float32),\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "# Note that in practice you would want to wrap this in a training loop\n",
    "# but for this example doing just one pass is sufficient.\n",
    "# A training loop example can be found in:\n",
    "# examples/tutorial/KDD_2025/heterogeneous_training.py\n",
    "loss = compute_loss(model, data)\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# And we can do a backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ee03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now if we run the loss function again, we should see a different value.\n",
    "loss = compute_loss(model, data)\n",
    "print(f\"Loss after backward pass: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bagl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

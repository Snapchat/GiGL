shared_resource_config:
  resource_labels:
  # These are compute labels that we will try to attach to the resources created by GiGL components.
  # More information: https://cloud.google.com/compute/docs/labeling-resources.
  # These can be mostly used to get finer grained cost reporting through GCP billing on individual component
  # and pipeline costs.

  # If COMPONENT is provided in cost_resource_group_tag, it will be automatically be replaced with one of
  # {pre|tra|inf|pos} standing for: {Preprocessor | Trainer | Inference | Post Processor} so we can get more accurate cost measurements of each component.
  # See implementation:
  # `python/gigl/src/common/types/pb_wrappers/gigl_resource_config.py#GiglResourceConfigWrapper.get_resource_labels`

    cost_resource_group_tag: dev_experiments_COMPONENT
    cost_resource_group: gigl_platform
  common_compute_config:
    project: "USER_PROVIDED_PROJECT"
    region: "us-central1"
    # We recommend using the same bucket for temp_assets_bucket and temp_regional_assets_bucket
    # These fields will get combined into one in the future. Note: Usually storage for regional buckets is cheaper,
    # thus that is recommended.
    temp_assets_bucket: "gs://USER_PROVIDED_TEMP_ASSETS_BUCKET"
    temp_regional_assets_bucket: "gs://USER_PROVIDED_TEMP_ASSETS_BUCKET"
    perm_assets_bucket: "gs://USER_PROVIDED_PERM_ASSETS_BUCKET"
    temp_assets_bq_dataset_name: "gigl_temp_assets"
    embedding_bq_dataset_name: "gigl_embeddings"
    gcp_service_account_email: "USER_PROVIDED_SA@USER_PROVIDED_PROJECT.iam.gserviceaccount.com"
    dataflow_runner: "DataflowRunner"
preprocessor_config:
  edge_preprocessor_config:
    num_workers: 1
    max_num_workers: 256
    machine_type: "n2d-highmem-64"
    disk_size_gb: 300
  node_preprocessor_config:
    num_workers: 1
    max_num_workers: 128
    machine_type: "n2d-highmem-64"
    disk_size_gb: 300
trainer_resource_config:
  vertex_ai_trainer_config:
    machine_type: "n1-highmem-8"
    gpu_type: "nvidia-tesla-v100"
    gpu_limit: 1
    num_replicas: 16
inferencer_resource_config:
  vertex_ai_inferencer_config:
    machine_type: "n1-highmem-8"
    gpu_type: "nvidia-tesla-v100"
    gpu_limit: 1
    num_replicas: 16

# ========
# TaskMetadata:
# Specifies the task we are going to perform on the graph.
taskMetadata:
  nodeAnchorBasedLinkPredictionTaskMetadata:
    # Specifying that we will perform node anchor based link prediction on edge of type: paper_or_author -> references -> paper_or_author
    supervisionEdgeTypes:
      - srcNodeType: paper_or_author
        relation: references
        dstNodeType: paper_or_author
# ========
# GraphMetadata:
# Specifies what edge and node types are present in the graph.
# Note all the edge / node types here should be referenced in the preprocessor_config
graphMetadata:
  edgeTypes:
  - dstNodeType: paper_or_author
    relation: references
    srcNodeType: paper_or_author
  nodeTypes:
  - paper_or_author
# ========
# SharedConfig:
# Specifies some extra metadata about the graph structure management of orchestration.
sharedConfig:
  isGraphDirected: True
  shouldSkipAutomaticTempAssetCleanup: true # Should we skip cleaning up the temporary assets after the run is complete?
# ========
# DatasetConfig:
# Specifies information about the dataset. How to access it, how to process it, and how to sample subgraphs from it.
datasetConfig:
  dataPreprocessorConfig:
    dataPreprocessorConfigClsPath: examples.MAG240M.preprocessor_config.Mag240DataPreprocessorConfig
    # our implementation takes no runtime arguments; if provided these are passed to the constructor off dataPreprocessorConfigClsPath
    # dataPreprocessorArgs:
# ========
# TrainerConfig:
# Specifies the training configuration. This includes the command to run training and the arguments to pass to it
# The trainer class is responsible for training the model.
trainerConfig:
  trainerArgs: # The following arguments are passed as arguments to the attached file for homogeneous training
    # Example argument to trainer
    log_every_n_batch: "50" # Batch frequency in which we log training metrics
  # Command which will be run to launch training
  command: python -m examples.link_prediction.homogeneous_training
# ========
# InferencerConfig:
# Specifies the inference configuration. This includes the command to run inference and the arguments to pass to it
# The inferencer class is responsible for running inference on the model.
inferencerConfig:
  inferencerArgs: # The following arguments are passed as arguments to the attached file for homogeneous inference
    # Example argument to inferencer
    log_every_n_batch: "50" # Batch frequency in which we log inference metrics
  # The batch size for inference is specified here as its own argument
  inferenceBatchSize: 512
  command: python -m examples.link_prediction.homogeneous_inference
# ========
# FeatureFlags:
# Additional flags which can be provided to pipelines. We currently use this to specify whether we should use GLT as the backend, enabling in-memory subgraph sampling.
featureFlags:
  should_run_glt_backend: 'True'
# ========

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gigl.common.utils.jupyter_magics import change_working_dir_to_gigl_root\n",
    "change_working_dir_to_gigl_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dbc5ef",
   "metadata": {},
   "source": [
    "## DBLP Distributed Training Example\n",
    "\n",
    "This notebook will walk you through how to use GiGL to train a model on the DBLP dataset in a distributed fashion.\n",
    "At the end of this notebook you will have:\n",
    "\n",
    "1. Preprocessed the DBLP dataset and saved it as TFRecord files to GCS\n",
    "2. Trained a model based on the Graph, across multiple machines using Torch Distributed constructs (DDP)\n",
    "3. Performed inference on the trained model, saving the resulting embeddings to BigQuery\n",
    "\n",
    "If you are more interested in the fine details of individual components, or the GBML protos, please see [toy_example_walkthrough.ipynb](../toy_visual_example/toy_example_walkthrough.ipynb) which provides in-depth explanations of what each component is doing.\n",
    "\n",
    "The latest version of this notebook can be found on [github](https://github.com/Snapchat/GiGL/blob/main/examples/link_prediction/dblp.ipynb)\n",
    "\n",
    "NOTE: This notebook and [cora.ipynb](./cora.ipynb) are very similar, and differ in the following ways:\n",
    "* The `TEMPLATE_TASK_CONFIG_URI`s are using different task specs\n",
    "* The `Examining the trained model` cells use homogenoeus/hetergeneous models and PyG constructs, as appropirate for the dataset.\n",
    "* The `# Looking at inference results` cells are for homogeneous/hetereogenous inference results (e.g. if there are multiple BQ tables for the different node types).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b35af",
   "metadata": {},
   "source": [
    "## Setting up GCP Project and configs\n",
    "Assuming you have a GCP project setup:\n",
    "\n",
    "1. Open up `configs/example_resource_config.yaml` and fill all relevant fields under `common_compute_config`:\n",
    "  - project\n",
    "  - region\n",
    "  - temp_assets_bucket\n",
    "  - temp_regional_assets_bucket\n",
    "  - perm_assets_bucket\n",
    "  - temp_assets_bq_dataset_name\n",
    "  - embedding_bq_dataset_name\n",
    "  - gcp_service_account_email\n",
    "\n",
    "2. Ensure your service account has relevant perms. See our [cloud setup guide](../../docs/user_guide/getting_started/cloud_setup_guide.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "from gigl.common import LocalUri\n",
    "from gigl.env.pipelines_config import get_resource_config\n",
    "from gigl.src.common.types.pb_wrappers.gigl_resource_config import GiglResourceConfigWrapper\n",
    "from gigl.src.common.types.pb_wrappers.gbml_config import GbmlConfigPbWrapper\n",
    "\n",
    "curr_datetime = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Firstly, let's give your job a name and ensure that the resource and task configs exist and can be loaded\n",
    "JOB_NAME = f\"{getpass.getuser()}_gigl_dblp_{curr_datetime}\"\n",
    "TEMPLATE_TASK_CONFIG_URI = LocalUri(\"examples/link_prediction/configs/e2e_het_dblp_sup_task_config.yaml\")\n",
    "# Respect the environment variable for resource config URI\n",
    "# if not, set it to some default value.\n",
    "RESOURCE_CONFIG_URI = LocalUri(os.environ.get(\"GIGL_TEST_DEFAULT_RESOURCE_CONFIG\", \"examples/link_prediction/configs/example_resource_config.yaml\"))\n",
    "print(f\"Using resource config URI: {RESOURCE_CONFIG_URI}\")\n",
    "\n",
    "TEMPLATE_TASK_CONFIG: GbmlConfigPbWrapper = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(gbml_config_uri=TEMPLATE_TASK_CONFIG_URI)\n",
    "RESOURCE_CONFIG: GiglResourceConfigWrapper = get_resource_config(resource_config_uri=RESOURCE_CONFIG_URI)\n",
    "PROJECT = RESOURCE_CONFIG.project\n",
    "\n",
    "\n",
    "print(f\"Succesfully found task config and resource config. Script will help execute job: {JOB_NAME} on project: {PROJECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1892357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets run some basic checks to validate correctness of the task and resource config\n",
    "from gigl.src.validation_check.config_validator import kfp_validation_checks\n",
    "\n",
    "kfp_validation_checks(\n",
    "    job_name=JOB_NAME,\n",
    "    task_config_uri=TEMPLATE_TASK_CONFIG_URI,\n",
    "    resource_config_uri=RESOURCE_CONFIG_URI,\n",
    "    # config_populator is the first step in the pipeline; where we will populat the template task config specified above and generate a frozen config\n",
    "    start_at=\"config_populator\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385941bd",
   "metadata": {},
   "source": [
    "## Compiling Src Docker images\n",
    "\n",
    "You will need to build and push docker images with your custom code so that individual GiGL components can leverage your code.\n",
    "For this experiment we will consider the DLPB specs and code to be \"custom code\", and we will guide you how to build a docker image with the code.\n",
    "\n",
    "We will make use of `scripts/build_and_push_docker_image.py` for this.\n",
    "\n",
    "Make note that this builds `containers/Dockerfile.src` and `containers/Dockerfile.dataflow.src`; which have instructions to `COPY` the `examples` folder - which contains all the source code for DLPB, and it has all the GiGL src code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806de7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from scripts.build_and_push_docker_image import build_and_push_cpu_image, build_and_push_cuda_image, build_and_push_dataflow_image\n",
    "\n",
    "DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG = f\"gcr.io/{PROJECT}/gigl_dataflow_runtime:{curr_datetime}\"\n",
    "DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG = f\"gcr.io/{PROJECT}/gigl_cuda:{curr_datetime}\"\n",
    "DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG = f\"gcr.io/{PROJECT}/gigl_cpu:{curr_datetime}\"\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    executor.submit(\n",
    "        build_and_push_dataflow_image,\n",
    "        image_name=DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG,\n",
    "    )\n",
    "    executor.submit(\n",
    "        build_and_push_cuda_image,\n",
    "        image_name=DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG,\n",
    "    )\n",
    "    executor.submit(\n",
    "        build_and_push_cpu_image,\n",
    "        image_name=DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG,\n",
    "    )\n",
    "\n",
    "print(f\"\"\"We built and pushed the following docker images:\n",
    "- {DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG}\n",
    "- {DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG}\n",
    "- {DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9729a",
   "metadata": {},
   "source": [
    "## We will instantiate local runner to help orchestrate the test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4fdfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.orchestration.local.runner import Runner, PipelineConfig\n",
    "\n",
    "\n",
    "runner = Runner()\n",
    "pipeline_config = PipelineConfig(\n",
    "    applied_task_identifier=JOB_NAME,\n",
    "    task_config_uri=TEMPLATE_TASK_CONFIG_URI,\n",
    "    resource_config_uri=RESOURCE_CONFIG_URI,\n",
    "    custom_cuda_docker_uri=DOCKER_IMAGE_MAIN_CUDA_NAME_WITH_TAG,\n",
    "    custom_cpu_docker_uri=DOCKER_IMAGE_MAIN_CPU_NAME_WITH_TAG,\n",
    "    dataflow_docker_uri=DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e69e2f",
   "metadata": {},
   "source": [
    "## First we will run config populator\n",
    "The config populator takes in a template `GbmlConfig` and outputs a frozen `GbmlConfig` by populating all job related metadata paths in `sharedConfig`. These are mostly GCS paths which the following components read and write from, and use as an intermediary data communication medium. For example, the field `sharedConfig.trainedModelMetadata` is populated with a GCS URI, which indicates to the Trainer to write the trained model to this path, and to the Inferencer to read the model from this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b433f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.src.common.utils.file_loader import FileLoader\n",
    "frozen_config_uri = runner.run_config_populator(pipeline_config=pipeline_config)\n",
    "frozen_config = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(gbml_config_uri=frozen_config_uri)\n",
    "file_loader = FileLoader()\n",
    "\n",
    "print(f\"Config Populator has successfully generated the following frozen config from the template ({TEMPLATE_TASK_CONFIG_URI}) :\")\n",
    "print(frozen_config.gbml_config_pb)\n",
    "\n",
    "pipeline_config.task_config_uri = frozen_config_uri # We need to update the task config uri to the new frozen config uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6fdf33",
   "metadata": {},
   "source": [
    "## Visualizing the Diff Between Template and Frozen Config\n",
    "\n",
    "We now have a frozen task config, with the path specified by `FROZEN_TASK_CONFIG_PATH`. We visualize the diff between the `frozen_task_config` generated by the `config_populator` and the original `template_task_config`. All the code below is just to do that and has nothing to do with GiGL.\n",
    "\n",
    "Specifically, note that:\n",
    "1. The component added `sharedConfig` to the YAML, which contains all the intermediary and final output paths for each component.\n",
    "2. It also added a `condensedEdgeTypeMap` and a `condensedNodeTypeMap`, which map all provided edge types and node types to `int` to save storage space:\n",
    "   - `EdgeType: Tuple[srcNodeType: str, relation: str, dstNodeType: str)] -> int`, and \n",
    "   - `NodeType: str -> int`\n",
    "   - Note: You may also provide your own condensedMaps; they will be generated for you if not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596bfa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigl.common.utils.jupyter_magics import show_task_config_colored_unified_diff\n",
    "\n",
    "show_task_config_colored_unified_diff(\n",
    "    f1_uri=frozen_config_uri,\n",
    "    f2_uri=TEMPLATE_TASK_CONFIG_URI,\n",
    "    f1_name='frozen_task_config.yaml',\n",
    "    f2_name='template_task_config.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa990b5",
   "metadata": {},
   "source": [
    "## Next we run the preprocessor\n",
    "The Data Preprocessor reads node, edge and respective feature data from a data source, and produces preprocessed / transformed versions of all this data, for subsequent components to use.  It uses Tensorflow Transform to achieve data transformation in a distributed fashion, and allows for transformations like categorical encoding, scaling, normalization, casting and more.\n",
    "\n",
    "In this case we are using preprocessing spec defined in `python/gigl/src/mocking/mocking_assets/passthrough_preprocessor_config_for_mocked_assets.py` - take a look for more details.\n",
    "\n",
    "You will note that the preprocessor will create a few BQ jobs to prepare the node and edge tables, subsequently it will kick off TFT (dataflow) jobs to do the actual preprocessing. The preprocessor will: (1) create a preprocessing spec and dump it to path specified in frozen config `sharedConfig.preprocessedMetadataUri`. (2) Respective Dataflow jobs will dump the preprocessed assets as `.tfrecord` files to the paths specified inside the preprocessing spec `preprocessedMetadataUri`\n",
    "\n",
    "The preprocessor will also *enumerate* all node ids, remapping the node ids as integers.\n",
    "See the preprocessor [docs](../../docs/user_guide/overview/components/data_preprocessor.md) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915d7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARN: There is an issue when trying to run dataflow jobs from inside a jupyter kernel; thus we cannot use the line \n",
    "# below to run the preprocessor as you would normally in a python script.\n",
    "# runner.run_data_preprocessor(pipeline_config=pipeline_config) \n",
    "\n",
    "# Instead, we will run the preprocessor from the command line.\n",
    "# Note: You can actually do this with every component; we just make use of the runner to make it easier to run the components.\n",
    "!python -m gigl.src.data_preprocessor.data_preprocessor \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$frozen_config_uri \\\n",
    "--resource_config_uri=$RESOURCE_CONFIG_URI \\\n",
    "--custom_worker_image_uri=$DOCKER_IMAGE_DATAFLOW_RUNTIME_NAME_WITH_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3aaba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocesor outputs.\n",
    "from snapchat.research.gbml.preprocessed_metadata_pb2 import PreprocessedMetadata\n",
    "\n",
    "print(frozen_config.gbml_config_pb.shared_config.trained_model_metadata.trained_model_uri)\n",
    "# Reload frozen config as config populator has modified the file.\n",
    "frozen_config = GbmlConfigPbWrapper.get_gbml_config_pb_wrapper_from_uri(gbml_config_uri=frozen_config_uri)\n",
    "\n",
    "# Let's see what the preprocessor has outputted\n",
    "print(frozen_config.preprocessed_metadata_pb_wrapper.preprocessed_metadata_pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f896167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature keys can make the message a bit hard to read, so let's filter them out.\n",
    "filtered_preprocessed_metadata = PreprocessedMetadata()\n",
    "filtered_preprocessed_metadata.CopyFrom(frozen_config.preprocessed_metadata_pb_wrapper.preprocessed_metadata_pb)\n",
    "for node_type in filtered_preprocessed_metadata.condensed_node_type_to_preprocessed_metadata:\n",
    "    filtered_preprocessed_metadata.condensed_node_type_to_preprocessed_metadata[node_type].ClearField(\"feature_keys\")\n",
    "for edge_type in filtered_preprocessed_metadata.condensed_edge_type_to_preprocessed_metadata:\n",
    "    filtered_preprocessed_metadata.condensed_edge_type_to_preprocessed_metadata[edge_type].main_edge_info.ClearField(\"feature_keys\")\n",
    "print(\"More readable preprocessed metadata:\")\n",
    "print(filtered_preprocessed_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf141f3a",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "The Trainer component reads the pre-processed graphs stored as TFRecords on GCS (whose paths are specified in the frozen config), and trains a GNN model on the training set, early stops on the performance of the validation set, and finally evaluates on the test set. The training logic is implemented with PyTorch Distributed Data Parallel (DDP) Training, which enables distributed training on multiple GPU cards across multiple worker nodes.\n",
    "\n",
    "The trainer reads the graph data which are stored as TFRecords on GCS, whose *locations* are stored at `GbmlConfig.SharedConfig.preprocessed_metadata_uri`.\n",
    "\n",
    "Once the model is trained, the model weights will be saved to the URI located at `GbmlConfig.SharedConfig.TrainedModelMetadata.trained_model_uri`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.run_trainer(pipeline_config=pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54158f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Examining the trained model\n",
    "# The trained model will be saved at: `GbmlConfig.SharedConfig.TrainedModelMetadata.trained_model_uri`\n",
    "print(frozen_config.gbml_config_pb.shared_config.trained_model_metadata.trained_model_uri)\n",
    "\n",
    "# You can load the model locally and play around with it:\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from examples.link_prediction.models import init_example_gigl_heterogeneous_model\n",
    "from gigl.common import UriFactory\n",
    "from gigl.src.common.utils.model import load_state_dict_from_uri\n",
    "from gigl.src.common.types.graph_data import EdgeType, NodeType\n",
    "\n",
    "\n",
    "edge_types = frozen_config.task_metadata_pb_wrapper.get_supervision_edge_types()\n",
    "print(f\"Supervision edge types: {edge_types}\")\n",
    "edge_type = edge_types[0]\n",
    "query_node_type = edge_type.src_node_type\n",
    "labeled_node_type = edge_type.dst_node_type\n",
    "\n",
    "graph_metadata = frozen_config.graph_metadata_pb_wrapper\n",
    "\n",
    "# Build dicts of a specific node type and edge type to feature dimension.\n",
    "# This dimension is the *input* dimension of the model.\n",
    "node_feature_dims: dict[NodeType, int] = {\n",
    "        graph_metadata.condensed_node_type_to_node_type_map[\n",
    "            condensed_node_type\n",
    "        ]: node_feature_dim\n",
    "        for condensed_node_type, node_feature_dim in frozen_config.preprocessed_metadata_pb_wrapper.condensed_node_type_to_feature_dim_map.items()\n",
    "}\n",
    "edge_feature_dims: dict[EdgeType, int] = {\n",
    "        graph_metadata.condensed_edge_type_to_edge_type_map[\n",
    "            condensed_edge_type\n",
    "        ]: edge_feature_dim\n",
    "        for condensed_edge_type, edge_feature_dim in frozen_config.preprocessed_metadata_pb_wrapper.condensed_edge_type_to_feature_dim_map.items()\n",
    "}\n",
    "model = init_example_gigl_heterogeneous_model(\n",
    "    node_type_to_feature_dim=node_feature_dims,\n",
    "    edge_type_to_feature_dim=edge_feature_dims,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    state_dict=load_state_dict_from_uri(UriFactory.create_uri(frozen_config.gbml_config_pb.shared_config.trained_model_metadata.trained_model_uri))\n",
    ")\n",
    "print(model)\n",
    "\n",
    "# Create some random data to test the model.\n",
    "example_data = HeteroData()\n",
    "example_data[query_node_type].x = torch.rand((10, node_feature_dims[node_type]))\n",
    "example_data[labeled_node_type].x = torch.rand((10, node_feature_dims[labeled_node_type]))\n",
    "example_data[edge_type].edge_index = torch.randint(0, 10, (2, 20))\n",
    "embeddings = model(example_data, device=torch.device(\"cpu\"), output_node_types=[edge_type.src_node_type, edge_type.dst_node_type])\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcceaac9",
   "metadata": {},
   "source": [
    "## Inference\n",
    "The Inferencer component is responsible for running inference of a trained model on samples generated on the fly during live subgraph sampling.  At a high level, it works by applying a trained model in an embarrassingly parallel and distributed fashion across these samples, and persisting the output embeddings and/or predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda84831",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.run_inferencer(\n",
    "    pipeline_config=pipeline_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cb217c",
   "metadata": {},
   "source": [
    "## Post Processor\n",
    "\n",
    "The inferencer outputs embeddings for *enumerated* node ids, e.g. the node ids that the preprocessor outputs. The preprocessor stores a mappings between unenumerated node ids and enumnerated node ids in `PreprocessedMetadata.condensed_node_type_to_preprocessed_metadata[node_type].enumerated_node_ids_bq_table` [1]\n",
    "\n",
    "The postprocessor unemuerates the embeddings and stores them in `PreprocessedMetadata.condensed_node_type_to_preprocessed_metadata[node_type].enumerated_node_ids_bq_table`\n",
    "\n",
    "[1]: https://github.com/Snapchat/GiGL/blob/924522caef86fd07d07e39c770d87376c7fdedbe/proto/snapchat/research/gbml/preprocessed_metadata.proto#L19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41fa868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARN: There is an issue when trying to run dataflow jobs from inside a jupyter kernel; thus we cannot use the line \n",
    "# below to run the postprocessor as you would normally in a python script.\n",
    "\n",
    "# Instead, we will run the preprocessor from the command line.\n",
    "# Note: You can actually do this with every component; we just make use of the runner to make it easier to run the components.\n",
    "!python -m gigl.src.post_process.post_processor \\\n",
    "--job_name=$JOB_NAME \\\n",
    "--task_config_uri=$frozen_config_uri \\\n",
    "--resource_config_uri=$RESOURCE_CONFIG_URI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29d52b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at inference results\n",
    "# Note we need to do this *after* we run the post-processor.\n",
    "# As in the live-subgraph sampling world, the post processor unenumerates the node ids per the mapping in\n",
    "# `PreprocessedMetadata.condensed_node_type_to_preprocessed_metadata[node_type].enumerated_node_ids_bq_table`\n",
    "# and stores the resulting embeddings in the BQ table specified in the frozen config.\n",
    "\n",
    "from gigl.src.common.utils.bq import BqUtils\n",
    "\n",
    "bq_utils = BqUtils(project=PROJECT)\n",
    "for node_type in graph_metadata.node_types:\n",
    "    print(f\"Node type: {node_type}\")\n",
    "    bq_emb_out_table = frozen_config.shared_config.inference_metadata.node_type_to_inferencer_output_info_map[node_type].embeddings_path\n",
    "    if bq_emb_out_table:\n",
    "        print(f\"Embeddings for {node_type} should be successfully stored in the following location: {bq_emb_out_table}\")\n",
    "        query = f\"SELECT * FROM {bq_emb_out_table} LIMIT 5\"\n",
    "        result = list(bq_utils.run_query(query=query, labels={}))\n",
    "        print(f\"Query result: {result}\")\n",
    "    else:\n",
    "        print(f\"No embeddings for {node_type} found in the frozen config. This is expected if the node type was not used in the training or inference.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bagl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# @package _global_
# MAG240M experiment: Large-scale training with in-batch negative sampling

defaults:
  - override /dataset: mag240_papers   # Use MAG240M papers dataset

model:
  node_embedding_dim: 128              # 128-dim embeddings
  embedding_similarity_type: COSINE    # Use cosine similarity for better normalization
  src_operator: IDENTITY               # No transformation on source embeddings
  dst_operator: IDENTITY               # No transformation on destination embeddings

training:
  max_steps: 50000                     # Extended training for large dataset
  early_stopping:
    patience: 3                        # Shorter patience for large-scale training
  dataloader:
    num_workers: 16                    # More workers for large dataset loading
  sampling:
    positive_edge_batch_size: 8192     # Large batch size for efficiency
    num_inbatch_negatives_per_edge: 1024  # Many in-batch negatives for diversity
    num_random_negatives_per_edge: 0   # No random negatives (memory efficient)
    negative_corruption_side: DST      # Corrupt destination nodes
  optimizer:
    sparse:
      lr: 0.1                          # High learning rate for sparse embeddings
      weight_decay: 0.001              # Light regularization for sparse
    dense:
      lr: 0.001                        # Lower learning rate for dense layers
      weight_decay: 0.001              # Standard regularization for dense
  distributed:
    num_processes_per_machine: 4       # Distributed training with 4 processes
    storage_reservation_percentage: 0.10  # Lower storage buffer
  checkpointing:
    save_every: 750                    # Save checkpoints more frequently
    should_save_async: true            # Async saving for efficiency
  logging:
    log_every: 1                       # Log every step for monitoring

validation:
  num_batches: 10                      # Limited batches for faster validation
  step_frequency: 250                  # Validate every 250 steps
  dataloader:
    num_workers: 3                     # Fewer workers for validation
  sampling:
    positive_edge_batch_size: 8192     # Large validation batch size
    num_inbatch_negatives_per_edge: 1024  # Match training negatives
    num_random_negatives_per_edge: 0   # No random negatives in validation
    negative_corruption_side: DST      # Match training corruption side
  hit_rates_at_k:                      # Evaluation metrics
    - 1                                # Hit@1 (most strict)
    - 10                               # Hit@10 (moderate)
    - 100                              # Hit@100 (most lenient)

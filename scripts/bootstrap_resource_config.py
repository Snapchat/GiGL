import argparse
import datetime
import getpass
import os
import pathlib
import subprocess
import tempfile
from dataclasses import dataclass
from typing import Optional

import yaml

from gigl.common import HttpUri, LocalUri, UriFactory
from gigl.src.common.utils.file_loader import FileLoader

GIGL_ROOT_DIR = pathlib.Path(__file__).resolve().parent.parent
LOCAL_DEV_TEMPLATE_RES_CONF = LocalUri(
    GIGL_ROOT_DIR / "deployment" / "configs" / "unittest_resource_config.yaml"
)
FALLBACK_TEMPLATE_RES_CONF = HttpUri(
    uri="https://raw.githubusercontent.com/Snapchat/GiGL/refs/heads/main/deployment/configs/unittest_resource_config.yaml"
)


@dataclass
class Param:
    default: Optional[str]
    description: str
    long_description: str = ""
    required: bool = True


class SupportedParams:
    def __init__(self):
        try:
            project = subprocess.check_output(
                ["gcloud", "config", "get-value", "project"], text=True
            ).strip()
        except subprocess.CalledProcessError as e:
            print(
                "Error retrieving active project name; is your gcloud SDK configured correctly?",
                e,
            )
            raise
        self.defaults: dict[str, Param] = {
            "project": Param(
                default=project,
                description="GCP Project name that you are planning on using",
            ),
            "region": Param(
                default="us-central1",
                description="The GCP region where you created your resources",
            ),
            "gcp_service_account_email": Param(
                default=None, description="The GCP Service account"
            ),
            "docker_artifact_registry_path": Param(
                default=None,
                description="The Docker Artifact Registry path where your source code images will be stored i.e. `us-central1-docker.pkg.dev/YOUR_PROJECT_NAME/YOUR_REPO_NAME`",
            ),
            "temp_assets_bq_dataset_name": Param(
                default=None,
                description="`temp_assets_bq_dataset_name` - Dataset name used for temporary assets",
            ),
            "embedding_bq_dataset_name": Param(
                default=None,
                description="`embedding_bq_dataset_name` - Dataset used of output embeddings",
            ),
            "temp_assets_bucket": Param(
                default=None,
                description="`temp_assets_bucket` - GCS Bucket for storing temporary assets i.e. `gs://YOUR_BUCKET_NAME`",
            ),
            "perm_assets_bucket": Param(
                default=None,
                description="`perm_assets_bucket` - GCS Bucket for storing permanent assets i.e. `gs://YOUR_BUCKET_NAME`",
            ),
        }


def infer_shell_file() -> str:
    """Infers the user's default shell configuration file."""
    shell = os.environ.get("SHELL", "")
    shell_config_map = {
        "zsh": "~/.zshrc",
        "bash": "~/.bashrc",
    }

    for key, config_file in shell_config_map.items():
        if key in shell:
            print(f"Detected shell: {key}. Using config file: {config_file}")
            return config_file

    print(
        "Could not infer the default shell. Please specify the shell configuration file manually."
    )
    return input(
        "Enter the path to your shell configuration file (e.g., ~/.bashrc): "
    ).strip()


def update_shell_config(
    shell_config_path: str,
    gigl_test_default_resource_config: str,
    gigl_project: str,
    gigl_docker_artifact_registry_path: str,
):
    """Updates the shell configuration file with the environment variables in an idempotent way."""
    shell_config_path = os.path.expanduser(shell_config_path)
    start_marker = "# ====== GiGL ENV Config - Begin ====="
    end_marker = "# ====== GiGL ENV Config - End ====="
    export_lines = [
        start_marker + "\n",
        "# This section is auto-generated by GiGL/scripts/bootstrap_resource_config.py.\n",
        f'export GIGL_TEST_DEFAULT_RESOURCE_CONFIG="{gigl_test_default_resource_config}"\n',
        f'export GIGL_PROJECT="{gigl_project}"\n',
        f'export GIGL_DOCKER_ARTIFACT_REGISTRY="{gigl_docker_artifact_registry_path}"\n',
        end_marker + "\n",
    ]

    # Read the existing shell config file
    if not os.path.exists(shell_config_path):
        raise FileNotFoundError(
            f"Shell config file '{shell_config_path}' does not exist."
        )
    shell_config_lines: list[str]
    with open(shell_config_path, "r") as shell_config:
        shell_config_lines = shell_config.readlines()

    inside_block = False
    updated_shell_config_lines = []
    for line in shell_config_lines:
        if line.strip().startswith(start_marker):
            inside_block = True
        elif line.strip().startswith(end_marker):
            inside_block = False
            continue
        if not inside_block:
            updated_shell_config_lines.append(line)

    # Add the new GiGL config block
    updated_shell_config_lines.extend(export_lines)

    # Write back to the shell config file
    with open(shell_config_path, "w") as shell_config:
        shell_config.writelines(updated_shell_config_lines)

    print(f"Updated {shell_config_path} with:\n{export_lines}.")


def assert_gcp_project_exists(project_id: str):
    command = f"gcloud projects describe {project_id}"
    result = subprocess.run(command, shell=True, capture_output=True, text=True)

    print(result.stdout)
    if result.returncode != 0:
        print(f"Command `{command}` failed with error: {result.stderr}")
        raise ValueError(
            f"Project '{project_id}' does not exist or you do not have access to it."
        )


def assert_bq_dataset_exists(dataset_name: str, project: str):
    command = f"bq show --project_id {project} {dataset_name}"
    result = subprocess.run(command, shell=True, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"Command `{command}` failed with error: {result.stderr}")
        raise ValueError(
            f"BigQuery dataset '{dataset_name}' does not exist in project '{project}' or you do not have access to it."
        )
    print(f"Confirmed BigQuery dataset '{dataset_name}' exists.")


def assert_gcs_bucket_exists(bucket_name: str):
    assert bucket_name.startswith("gs://"), "Bucket name must start with 'gs://'"
    command = f"gsutil ls {bucket_name}"
    result = subprocess.run(command, shell=True, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"Command `{command}` failed with error: {result.stderr}")
        raise ValueError(
            f"GCS bucket '{bucket_name}' does not exist or you do not have access to it."
        )
    print(f"Confirmed GCS bucket '{bucket_name}' exists.")


if __name__ == "__main__":
    print("Welcome to the GiGL Cloud Environment Configuration Script!")
    print("This script will help you set up your cloud environment for GiGL.")
    print(
        "Before running this script, please ensure you have followed the GiGL Cloud Setup Guide:"
    )
    print(
        "https://snapchat.github.io/GiGL/docs/user_guide/getting_started/cloud_setup_guide"
    )
    print("======================================================")

    resource_config_path: str
    file_loader = FileLoader()
    if file_loader.does_uri_exist(uri=LOCAL_DEV_TEMPLATE_RES_CONF):
        print(
            f"Using local development template resource config: {LOCAL_DEV_TEMPLATE_RES_CONF}"
        )
        resource_config_path = LOCAL_DEV_TEMPLATE_RES_CONF.uri
    else:
        print(f"Using fallback template resource config: {FALLBACK_TEMPLATE_RES_CONF}")
        tmp_file = file_loader.load_to_temp_file(
            file_uri_src=FALLBACK_TEMPLATE_RES_CONF
        )
        print(f"Downloaded fallback template resource config to {tmp_file.name}")
        resource_config_path = tmp_file.name

    supported_params = SupportedParams()
    parser = argparse.ArgumentParser(
        description="Bootstrap GiGL resource config. All parameters can be provided as CLI args or interactively."
    )
    for key, param in supported_params.defaults.items():
        help_text = (
            f"{param.description} (default: {param.default})"
            if param.default
            else param.description
        )
        parser.add_argument(f"--{key}", type=str, help=help_text)
    args = parser.parse_args()

    values: dict[str, str] = {}
    for key, param in supported_params.defaults.items():
        # Check if value is provided in command line arguments
        if getattr(args, key):
            values[key] = getattr(args, key)
            continue
        # If not, prompt for input
        else:
            input_question: str
            long_description_clause = (
                f"\n{param.long_description}" if param.long_description else ""
            )
            if param.default is None:
                required_clause = "(required)" if param.required else "(optional)"
                input_question = f"-> {param.description}{long_description_clause} {required_clause}: "
            else:
                input_question = f"-> {param.description}{long_description_clause}\nDefaults to: [{param.default}]: "

            values[key] = input(input_question).strip() or param.default  # type: ignore
            if not values[key] and param.required:
                raise ValueError(
                    f"Missing required value for {key}. Please provide a value."
                )

    # Validate existence of resources
    assert_gcp_project_exists(values["project"])
    assert values["region"], "Region cannot be empty"
    assert values["gcp_service_account_email"], "GCP Service account cannot be empty"
    assert values[
        "docker_artifact_registry_path"
    ], "Docker Artifact Registry path cannot be empty"
    assert_bq_dataset_exists(
        dataset_name=values["temp_assets_bq_dataset_name"], project=values["project"]
    )
    assert_bq_dataset_exists(
        dataset_name=values["embedding_bq_dataset_name"], project=values["project"]
    )
    assert_gcs_bucket_exists(bucket_name=values["temp_assets_bucket"])
    assert_gcs_bucket_exists(bucket_name=values["perm_assets_bucket"])

    curr_datetime = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    curr_username = getpass.getuser()
    default_resource_config_dest_path = f"{values['perm_assets_bucket']}/{curr_username}/{curr_datetime}/gigl_test_default_resource_config.yaml"

    destination_file_path = (
        input(
            f"Output path for resource config (default: {default_resource_config_dest_path}); can be GCS or Local path: "
        ).strip()
        or default_resource_config_dest_path
    )
    assert destination_file_path and destination_file_path.startswith(
        "gs://"
    ), "Destination file path must be a GCS path starting with 'gs://'"

    print("=======================================================")
    print(f"Will now create the resource config file @ {destination_file_path}.")
    print("Using the following values:")
    update_fields_dict = {
        "project": values["project"],
        "region": values["region"],
        "gcp_service_account_email": values["gcp_service_account_email"],
        "temp_assets_bucket": values["temp_assets_bucket"],
        "temp_regional_assets_bucket": values["temp_assets_bucket"],
        "perm_assets_bucket": values["perm_assets_bucket"],
        "temp_assets_bq_dataset_name": values["temp_assets_bq_dataset_name"],
        "embedding_bq_dataset_name": values["embedding_bq_dataset_name"],
    }
    for key, value in update_fields_dict.items():
        print(f"  {key}: {value}")

    with open(resource_config_path, "r") as file:
        config = yaml.safe_load(file)

    # Update the YAML content
    common_compute_config: dict = config.get("shared_resource_config").get(
        "common_compute_config"
    )
    common_compute_config.update(update_fields_dict)

    tmp_file = tempfile.NamedTemporaryFile(delete=False)
    with open(tmp_file.name, "w") as file:
        yaml.safe_dump(config, file)

    file_loader = FileLoader(project=values["project"])
    file_uri_src = UriFactory.create_uri(uri=tmp_file.name)
    file_uri_dest = UriFactory.create_uri(uri=destination_file_path)
    file_loader.load_file(file_uri_src=file_uri_src, file_uri_dst=file_uri_dest)

    print(f"Updated YAML file saved at '{destination_file_path}'")

    # Update the user's shell configuration
    should_update_shell_config = (
        input(
            "Do you want to update your shell configuration file so you can use this new resource config for tests? [y/n] (Default: y): "
        )
        .strip()
        .lower()
        or "y"
    )
    if should_update_shell_config == "y":
        shell_config_path: str = infer_shell_file()
        update_shell_config(
            shell_config_path=shell_config_path,
            gigl_test_default_resource_config=destination_file_path,
            gigl_project=values["project"],
            gigl_docker_artifact_registry_path=values["docker_artifact_registry_path"],
        )

        print(
            f"Please restart your shell or run `source {shell_config_path}` to apply the changes."
        )
    else:
        print(
            "Skipping shell configuration update. Please remember to set the environment variables manually "
            + "if you want `make unit_test | integration_test` commands to work correctly."
        )
